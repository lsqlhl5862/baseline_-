{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from datetime import datetime \n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from tqdm.autonotebook import *\n",
    "import pdfplumber\n",
    "tqdm.pandas()\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "def extract_pdf_content(pdf_path):\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index_page in np.arange(0, len(pdf.pages), 1):\n",
    "            # 读取多页\n",
    "            page = pdf.pages[index_page]   # 第n页的信息\n",
    "            text = page.extract_text()\n",
    "            text_list.append(text)\n",
    "            table = page.extract_tables()\n",
    "            for t in table:\n",
    "                table_list.append(t)\n",
    "    return text_list, table_list\n",
    "\n",
    "# a='train_output'\n",
    "# b=\"train\"+r\"_\\d\"\n",
    "# print(re.match(b,a))\n",
    "def csv_2_df(file_path):\n",
    "    file_list=file_path.parents[0].glob(\"*.csv\")\n",
    "    result=None\n",
    "    for item in file_list:\n",
    "        if re.match(file_path.stem.split(\"_\")[0]+r\"_\\d\",item.stem) is not None:\n",
    "            print(item.name)\n",
    "            result=pd.read_csv(item) if result is None else pd.concat([result,pd.read_csv(item)])\n",
    "    return result\n",
    "\n",
    "test_path = pl.Path('datasets/train_1.csv')\n",
    "\n",
    "test_df = csv_2_df(test_path)\n",
    "test_df.to_csv(\"datasets/train.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF解析原始数据 \n",
    "## 加载数据并采用pdfplumber抽取PDF中的文字和表格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id       认购日期        理财产品名称     产品发行方名称    理财类型  认购金额(万元)  \\\n0          1 2019-03-27         汇聚金1号  中融国际信托有限公司      信托   10000.0   \n1          1 2019-03-27  招商银行步步生金8699        招商银行  银行理财产品     200.0   \n\n       产品起息日      产品到息日  产品期限  资金来源    实际购买公司名称 实际购买公司和上市公司关系 买卖方是否有关联关系  \\\n0 2019-03-27 2019-09-23  180天  自有资金  恒生电子股份有限公司          公司本身          否   \n1 2019-03-27        NaT   NaN  自有资金  恒生电子股份有限公司          公司本身          否   \n\n        公告日期  \n0 2019-04-25  \n1 2019-04-25  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>认购日期</th>\n      <th>理财产品名称</th>\n      <th>产品发行方名称</th>\n      <th>理财类型</th>\n      <th>认购金额(万元)</th>\n      <th>产品起息日</th>\n      <th>产品到息日</th>\n      <th>产品期限</th>\n      <th>资金来源</th>\n      <th>实际购买公司名称</th>\n      <th>实际购买公司和上市公司关系</th>\n      <th>买卖方是否有关联关系</th>\n      <th>公告日期</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2019-03-27</td>\n      <td>汇聚金1号</td>\n      <td>中融国际信托有限公司</td>\n      <td>信托</td>\n      <td>10000.0</td>\n      <td>2019-03-27</td>\n      <td>2019-09-23</td>\n      <td>180天</td>\n      <td>自有资金</td>\n      <td>恒生电子股份有限公司</td>\n      <td>公司本身</td>\n      <td>否</td>\n      <td>2019-04-25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2019-03-27</td>\n      <td>招商银行步步生金8699</td>\n      <td>招商银行</td>\n      <td>银行理财产品</td>\n      <td>200.0</td>\n      <td>2019-03-27</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>自有资金</td>\n      <td>恒生电子股份有限公司</td>\n      <td>公司本身</td>\n      <td>否</td>\n      <td>2019-04-25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id                  file_path  \\\n0          1  datasets/train_data/1.PDF   \n1          2  datasets/train_data/2.PDF   \n\n                                                text  \\\n0  ['                                            ...   \n1  ['                                            ...   \n\n                                               tabel  \n0  [[['', None, None, '', None, None, '', None, N...  \n1  [[['', None, None, '', None, None, '', None, N...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>file_path</th>\n      <th>text</th>\n      <th>tabel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>datasets/train_data/1.PDF</td>\n      <td>['                                            ...</td>\n      <td>[[['', None, None, '', None, None, '', None, N...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>datasets/train_data/2.PDF</td>\n      <td>['                                            ...</td>\n      <td>[[['', None, None, '', None, None, '', None, N...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id                     file_path  \\\n0      11188  datasets/test_data/11188.PDF   \n1      11189  datasets/test_data/11189.PDF   \n\n                                                text tabel  \n0  ['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...    []  \n1  ['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...    []  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>file_path</th>\n      <th>text</th>\n      <th>tabel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11188</td>\n      <td>datasets/test_data/11188.PDF</td>\n      <td>['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11189</td>\n      <td>datasets/test_data/11189.PDF</td>\n      <td>['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# 数据准备(train_output文件中格式有点问题，需要提前用excel或者wps打开然后另存为excel文件)\n",
    "train_outputs = pd.read_excel('datasets/train_output.xlsx')\n",
    "\n",
    "# 获取pdf中文字和表格\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index_page in np.arange(0, len(pdf.pages), 1):\n",
    "            # 读取多页\n",
    "            page = pdf.pages[index_page]   # 第n页的信息\n",
    "            text = page.extract_text()\n",
    "            text_list.append(text)\n",
    "            table = page.extract_tables()\n",
    "            for t in table:\n",
    "                table_list.append(t)\n",
    "    return text_list, table_list\n",
    "\n",
    "def get_dir_file(path):\n",
    "    '''\n",
    "    输入文件夹位置，输出整理好的dataframe\n",
    "    '''\n",
    "    path_list = os.listdir(path)\n",
    "    id_list = []\n",
    "    file_path_list = []\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    for i in tqdm(path_list):\n",
    "        if '.PDF' in i:\n",
    "            file_path = path + i\n",
    "            id_list.append(int(i.split('.')[0]))\n",
    "            file_path_list.append(file_path)\n",
    "            try:\n",
    "                text_temp, table_temp = extract_pdf_content(file_path)\n",
    "            except Exception:\n",
    "                print('此pdf无法读取')\n",
    "                text_temp, table_temp = [], []\n",
    "            text_list.append(text_temp)\n",
    "            table_list.append(table_temp)\n",
    "            \n",
    "    df = pd.DataFrame()\n",
    "    df['sample_id'] = id_list\n",
    "    df['file_path'] = file_path_list\n",
    "    df['text'] = text_list\n",
    "    df['tabel'] = table_list\n",
    "    df = df.sort_values('sample_id')\n",
    "    return df\n",
    "\n",
    "# 文件处理太慢，可持续化保存文件\n",
    "train_path = 'datasets/train.csv'\n",
    "if os.path.exists(train_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "else:\n",
    "    train_df = get_dir_file('datasets/train_data/')\n",
    "    train_df.to_csv(train_path,index=False)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "test_path =  'datasets/test.csv'\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "else:\n",
    "    test_df = get_dir_file('datasets/test_data/')\n",
    "    test_df.to_csv(test_path,index=False)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_outputs.head(2)\n",
    "train_df.head(2)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集验证集\n",
    "train_df = train_df.sample(frac=1, random_state=1017)\n",
    "val_df = train_df[:1800]\n",
    "train_df = train_df[1800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "## 抽取整体数据（一个sampleid内此字段内容都相同）\n",
    "## 公告时间，实际购买公司"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.抽取公告时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1800/1800 [00:01<00:00, 1247.40it/s]\nSeries([], Name: text, dtype: object)\n5162    [None]\nName: text, dtype: object\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5883333333333334"
     },
     "metadata": {},
     "execution_count": 65
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1800/1800 [00:00<00:00, 2038.50it/s]\n100%|██████████| 8660/8660 [00:04<00:00, 2115.81it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1800, 2)"
     },
     "metadata": {},
     "execution_count": 65
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1797, 2)"
     },
     "metadata": {},
     "execution_count": 65
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "344\n847\n953\n1096\n"
    }
   ],
   "source": [
    "# 首先针对任务抽取时间（每个时间跟每个id是一一对应的）\n",
    "# 要不是取第一个时间，要不就是取最后一个时间（或者时间加一）这里可以建立一个模型预测\n",
    "# base这里面直接取最后一个时间作为发布日期\n",
    "\n",
    "##wjc字典里添加了十\n",
    "CN_NUM = {\n",
    "    u'月十日':'月10日',u'十月':'10月', u'十日': '0日',u'二十':'二',u'三十':'三', u'十': 1, u'○': 0, u'O':'0', u'Ο':'0',\n",
    "    u'〇': 0, u'一': 1, u'二': 2, u'三': 3,\n",
    "    u'四': 4, u'五': 5, u'六': 6, u'七': 7,\n",
    "    u'八': 8, u'九': 9, u'零': 0, u'壹': 1,\n",
    "    u'贰': 2, u'叁': 3, u'肆': 4, u'伍': 5,\n",
    "    u'陆': 6, u'柒': 7, u'捌': 8, u'玖': 9,\n",
    "    u'貮': 2, u'两': 2,  \n",
    "}\n",
    "######### u'○': 0, u'O':'0', u'Ο':'0',##########可删除这些字典，改用添加内容效果一致，这里暂且保留字典\n",
    "\n",
    "\n",
    "def get_put_time_from_text(row):\n",
    "    row = row.replace(' ', '').replace('\\\\n', '')\n",
    "\n",
    "    ############添加#################改善年份中0有多个不识别的问题########\n",
    "    start=1\n",
    "    for word in '一二三四五六七八九':   \n",
    "        row=re.sub('二.一'+word,'201'+str(start),row)\n",
    "        start=start+1\n",
    "    ########添加############慢#######################\n",
    "\n",
    "    for key in CN_NUM:\n",
    "        row = row.replace(key, str(CN_NUM[key]))   \n",
    "\n",
    "    r = row.replace(\"年\", \"-\").replace(\"月\", \"-\").replace(\"日\", \" \").replace(\"/\", \"-\").strip()\n",
    "    regex = \"(\\d{4}-\\d{1,2}-\\d{1,2})\"\n",
    "    r = re.findall(regex, r)\n",
    "    if len(r)==0:\n",
    "        return np.nan\n",
    "    time_str = r[-1]\n",
    "    first = time_str.split('-')[0]\n",
    "    second = time_str.split('-')[1]\n",
    "    last = time_str.split('-')[-1]\n",
    "    second = str.zfill(second, 2)\n",
    "    last = str.zfill(last, 2)\n",
    "    r = '-'.join([first, second, last])\n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_result = pd.DataFrame()\n",
    "val_result['sample_id'] = val_df['sample_id']\n",
    "val_result['predict_time'] = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "test_gg = train_outputs.groupby('sample_id').apply(lambda row:list(row['公告日期'])[0]).reset_index()\n",
    "test_gg.columns = ['sample_id', 'time']\n",
    "val_result = pd.merge(val_result, test_gg, on='sample_id', how='left')\n",
    "##val_result['time'].head(10)\n",
    "###val_result['predict_time'].head(10)\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "testid=6375\n",
    "print(train_df[train_df.sample_id==testid]['text'].astype(str))\n",
    "print(val_df[val_df.sample_id==testid]['text'].astype(str))\n",
    "\n",
    "\n",
    "# 对于每一行，通过列名name访问对应的元素\n",
    "for index,row in val_result.iterrows():\n",
    "    #type(row['predict_time'])\n",
    "    #print(row['sample_id'],row['predict_time']) # 输出每一行\n",
    "    try:\n",
    "        val_result.at[index,'predict_time1']=datetime.strptime(row['predict_time'], \"%Y-%m-%d\").strftime('%Y/%m/%d')\n",
    "        #val_result.at[index,'predict_time1']=time.strptime(row['predict_time'], \"%Y-%m-%d\")\n",
    "    except:\n",
    "        #val_result.at[index,'predict_time1']='1900/01/01'\n",
    "        continue\n",
    "        # print(row['sample_id'])\n",
    "        # print(val_df[val_df.sample_id==row['sample_id']]['text'].astype(str))\n",
    "        \n",
    "for index,row in val_result.iterrows():\n",
    "    val_result.at[index,'time1']=val_result.at[index,'time'].strftime(\"%Y-%m-%d %H:%M:%S\")[0:10]\n",
    "    val_result.at[index,'time2']=val_result.at[index,'time1'].replace('-','')\n",
    "    try:\n",
    "        val_result.at[index,'predict_time2']=val_result.at[index,'predict_time'].replace('-','')\n",
    "    except:\n",
    "        continue\n",
    "    #######粗计算时间具体误差################\n",
    "    val_result.at[index,'差值']=(int)(val_result.at[index,'predict_time2'])-(int)(val_result.at[index,'time2'])\n",
    "\n",
    "fail_val_result=val_result[val_result['差值']!=0]\n",
    "#######抽取错误列放入fail的dataframe########################\n",
    "\n",
    "\n",
    "# val_result['predict_time_inf']=pd.to_datetime(val_result['predict_time'],format='%Y/%m/%d')\n",
    "# val_result['predict_time_inf'].head(10)\n",
    "\n",
    "#val_result['predict_time1'] = datetime.strptime(val_result['predict_time'], \"%Y-%m-%d\").strftime('%Y/%m/%d')\n",
    "#val_result['predict_time_inf']=pd.to_datetime(val_result['predict_time1'],format='%Y/%m/%d')\n",
    "#val_result['日期差值']=val_result['predict_time_inf']-val_result['time']\n",
    "#val_result.head(10)\n",
    "\n",
    "# 判断验证集的准确率\n",
    "np.sum(val_result['predict_time'].astype(str) == val_result['time'].astype(str))/len(val_result)\n",
    "\n",
    "# val_time = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "# test_time = test_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "\n",
    "val_time=pd.DataFrame()\n",
    "val_time[\"公告日期\"]=val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "# val_gm = val_df.progress_apply(lambda row:my_get_gm(row['text']), axis=1)\n",
    "val_time['sample_id'] = val_df['sample_id'].astype(str)\n",
    "#test_gm = test_df.progress_apply(lambda row:get_gm(row['text']), axis=1)\n",
    "\n",
    "test_time=pd.DataFrame()\n",
    "test_time[\"公告日期\"]=test_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "# val_gm = val_df.progress_apply(lambda row:my_get_gm(row['text']), axis=1)\n",
    "test_time['sample_id'] = test_df['sample_id'].astype(str)\n",
    "\n",
    "val_time.shape\n",
    "val_time.dropna().shape\n",
    "def fix_time(df):\n",
    "    df=df.dropna()\n",
    "    index_list=[]\n",
    "    df=df.reset_index(drop=True)\n",
    "    index=-1\n",
    "    for sample_id,date in df[[\"sample_id\",\"公告日期\"]].values:\n",
    "        index+=1\n",
    "        try:\n",
    "            tmp=datetime.strptime(date,\"%Y-%m-%d\")\n",
    "        except:\n",
    "            print(index)\n",
    "            index_list.append(index)\n",
    "    return df.drop(index_list).reset_index(drop=True)\n",
    "\n",
    "val_time=fix_time(val_time)\n",
    "test_time=fix_time(test_time)\n",
    "#test_gm = test_df.progress_apply(lambda row:get_gm(row['text']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.抽取实际购买公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1800/1800 [00:03<00:00, 502.72it/s]\n100%|██████████| 8660/8660 [00:12<00:00, 708.81it/s]\n100%|██████████| 1800/1800 [00:03<00:00, 578.41it/s]\nSeries([], Name: text, dtype: object)\n2929    [None, None, None, None, None]\nName: text, dtype: object\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7972222222222223"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "# 抽取购买公司\n",
    "# 前几句话出现\n",
    "# 将其按照\\\\n 和空格切割\n",
    "def get_gm(row):\n",
    "    row=row.replace('[','').replace(']','').replace('\\'','')    ######粗datawash,去除部分典型符号###########\n",
    "    #head_row=re.split('资金',row)[0]\n",
    "    #head_row=re.split('使用',head_row)[0]\n",
    "    #tag_head_row=headrow.replace('[\\\\\\\\n ]','$|$')\n",
    "    result = re.split('[\\\\\\\\n ]',row)\n",
    "    for i in result:\n",
    "        if '公司' in i:\n",
    "            i=i.replace('（','(').replace('）',')') ##########修改中文括号#################\n",
    "            regex=\"(^.*公司)\"##################此四行为添加的##############################\n",
    "            i=re.findall(regex, i)###########取此行最后一个“公司”前的字符#######\n",
    "            i=i[0]###############格式转换，list取出str####################################\n",
    "            if i=='公司':\n",
    "                continue   ###########跳过字段为公司的答案进入下一个循环#####################\n",
    "            return i\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################粗改进######################################\n",
    "def my_get_gm(row):\n",
    "    row=row.replace('[','').replace(']','').replace('\\'','')    ######粗datawash,去除部分典型符号###########\n",
    "    re_row = re.split('[\\\\\\\\n ]',row)\n",
    "    \n",
    "    ##################取第一次出现公司的行开始计算###################\n",
    "    for i in re_row:\n",
    "        if '公司' not in i:\n",
    "            re_row.remove(i)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    head_row=\"\"\n",
    "    #################list替换为文本##########################\n",
    "    for i in re_row:\n",
    "        head_row=head_row+i+'$|$'\n",
    "    \n",
    "    \n",
    "\n",
    "    # print(type(head_row.find(\"公告编号\")))\n",
    "    # head_row=head_row[head_row.find(\"公告编号\"):-1]\n",
    "\n",
    "    # head_row=head_row.split(\"^.*公告编号：\")[0]\n",
    "    # head_row_a=head_row.split(\"资金\")[0]\n",
    "    # head_row=head_row_a.split(\"使用\")[0]\n",
    "    # head_row=head_row_a.split(\"委托\")[0]\n",
    "    # head_row=head_row_a.split(\"董事\")[0]\n",
    "    head_row=head_row.split(\"理财\")[0].split(\"资金\")[0].split(\"使用\")[0].split(\"董事\")[0]\n",
    "    # print(head_row_a)\n",
    "    tag_head_row=head_row.replace(' ','').replace('子公司','$|$').replace('公司','公司$|$').replace('关于','$|$').replace('-','$|$')\n",
    "    # spl_head_row = re.split('\\$\\|\\$',tag_head_row)\n",
    "    spl_head_row = tag_head_row.split('$|$')\n",
    "    spl_head_row.reverse()\n",
    "    result=spl_head_row\n",
    "    for i in result:\n",
    "        if '公司' in i:\n",
    "            i=i.replace('（','(').replace('）',')') ##########修改中文括号#################\n",
    "            regex=\"(^.*公司)\"##################此四行为添加的##############################\n",
    "            i=re.findall(regex, i)###########取此行最后一个“公司”前的字符#######\n",
    "            i=i[0]###############格式转换，list取出str####################################\n",
    "            if i=='公司' or len(i)<=4 or '”' in i or '“' in i or '简称' in i:\n",
    "                continue   ###########跳过字段为公司的答案进入下一个循环#####################\n",
    "            return i\n",
    "#################到此为止####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################二次改进,文本内容查找追加，成功率暂时比不追加要低一点######################################\n",
    "def my_get_gm2(row):\n",
    "    row=row.replace('[','').replace(']','').replace('\\'','')    ######粗datawash,去除部分典型符号###########\n",
    "    re_row = re.split('[\\\\\\\\n ]',row)\n",
    "    \n",
    "    ###################取第一次出现公司的行开始计算###################\n",
    "    for i in re_row:\n",
    "        if '公司' not in i:\n",
    "            re_row.remove(i)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    head_row=\"\"\n",
    "    #################list替换为文本##########################\n",
    "    for i in re_row:\n",
    "        head_row=head_row+i+'$|$'\n",
    "    \n",
    "    #################去除标题#############################\n",
    "    text_row1=head_row[head_row.find(\"公告\"):-1]\n",
    "    text_row2=head_row[head_row.find(\"意见\"):-1]\n",
    "    regex = \"(子公司.*公司.*购买)\"\n",
    "    text1 = re.findall(regex, text_row1)\n",
    "    text2 = re.findall(regex, text_row2)\n",
    "    text=text1+text2\n",
    "    for i in text:\n",
    "        if '$|$' in i:\n",
    "            text.remove(i)\n",
    "    my_list=[]\n",
    "    for i in text:\n",
    "        spl_i=i.replace(\"购买\",\"$|$\").replace(\"子公司\",\"$|$\").replace(\"公司\",\"公司$|$\")\n",
    "        spl_i=spl_i.split(\"$|$\")\n",
    "        spl_i.reverse()\n",
    "        for j in spl_i:\n",
    "            if '公司' in j:\n",
    "                j=j.replace('（','(').replace('）',')')\n",
    "                regex=\"(^.*公司)\"\n",
    "                j=re.findall(regex, j)\n",
    "                j=j[0]\n",
    "                if j=='公司' or len(j)<=4 or len(j)>30 or '”' in j or '“' in j or '简称' in j:\n",
    "                    continue\n",
    "                my_list.append(j)\n",
    "    #print(text1)\n",
    "\n",
    "    \n",
    "    \n",
    "    head_row=head_row.split(\"理财\")[0].split(\"资金\")[0].split(\"使用\")[0].split(\"董事\")[0]\n",
    "    # print(head_row_a)\n",
    "    tag_head_row=head_row.replace(' ','').replace('子公司','$|$').replace('公司','公司$|$').replace('关于','$|$').replace('-','$|$')\n",
    "    # spl_head_row = re.split('\\$\\|\\$',tag_head_row)\n",
    "    spl_head_row = tag_head_row.split('$|$')\n",
    "    spl_head_row.reverse()\n",
    "    result=spl_head_row\n",
    "    for i in result:\n",
    "        if '公司' in i:\n",
    "            i=i.replace('（','(').replace('）',')') ##########修改中文括号#################\n",
    "            regex=\"(^.*公司)\"##################此四行为添加的##############################\n",
    "            i=re.findall(regex, i)###########取此行最后一个“公司”前的字符#######\n",
    "            i=i[0]###############格式转换，list取出str####################################\n",
    "            if i=='公司' or len(i)<=4 or '”' in i or '“' in i or '简称' in i:\n",
    "                continue   ###########跳过字段为公司的答案进入下一个循环#####################\n",
    "            my_list.insert(0,i)\n",
    "            i=my_list[-1]\n",
    "            return i\n",
    "#################到此为止####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###val_gm = val_df.progress_apply(lambda row:get_gm(row['text']), axis=1)\n",
    "val_gm=pd.DataFrame()\n",
    "val_gm[\"实际购买公司名称\"]=val_df.progress_apply(lambda row:my_get_gm(row['text']), axis=1)\n",
    "# val_gm = val_df.progress_apply(lambda row:my_get_gm(row['text']), axis=1)\n",
    "val_gm['sample_id'] = val_df['sample_id']\n",
    "#test_gm = test_df.progress_apply(lambda row:get_gm(row['text']), axis=1)\n",
    "val_gm=val_gm.dropna()\n",
    "\n",
    "\n",
    "test_gm=pd.DataFrame()\n",
    "test_gm[\"实际购买公司名称\"] = test_df.progress_apply(lambda row:my_get_gm(row['text']), axis=1)\n",
    "test_gm['sample_id']=test_df['sample_id']\n",
    "test_gm=test_gm.dropna()\n",
    "\n",
    "my_val_result = pd.DataFrame()\n",
    "my_val_result['sample_id'] = val_df['sample_id']\n",
    "my_val_result['predict_gs'] = val_df.progress_apply(lambda row: my_get_gm(row['text']), axis=1)\n",
    "my_test_gs = train_outputs.groupby('sample_id').apply(lambda row:list(row['实际购买公司名称'])[0]).reset_index()\n",
    "my_test_gs.columns = ['sample_id', 'gs']\n",
    "my_val_result = pd.merge(my_val_result, my_test_gs, on='sample_id', how='left')\n",
    "my_val_result['是否相等']=my_val_result['predict_gs']==my_val_result['gs']\n",
    "fail_my_val_result=my_val_result[my_val_result['是否相等']!=True]\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "testid=3597\n",
    "print(train_df[train_df.sample_id==testid]['text'].astype(str))\n",
    "print(val_df[val_df.sample_id==testid]['text'].astype(str))\n",
    "\n",
    "# 判断验证集的准确率\n",
    "np.sum(my_val_result['predict_gs'].astype(str) == my_val_result['gs'].astype(str))/len(my_val_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章段落初步分段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                              text_1 label_1\n0                 中银保本理财-人民币按期开放理财产品    理财产品\n1                 中银保本理财-人民币按期开放理财产品    理财产品\n2                        与利率挂钩的结构性产品    理财产品\n3             广发银行“薪加薪”16号XJXCKJ2578    理财产品\n4       兴业银行“金雪球-优悦”保本开放式人民币理财产品(2M)    理财产品\n...                              ...     ...\n212362                  上海浦兴投资发展有限公司      其它\n212363                  上海浦兴投资发展有限公司      其它\n212364                  上海浦兴投资发展有限公司      其它\n212365                  上海浦兴投资发展有限公司      其它\n212366             上海市浦东新区建设(集团)有限公司      其它\n\n[212367 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_1</th>\n      <th>label_1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>中银保本理财-人民币按期开放理财产品</td>\n      <td>理财产品</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>中银保本理财-人民币按期开放理财产品</td>\n      <td>理财产品</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>与利率挂钩的结构性产品</td>\n      <td>理财产品</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>广发银行“薪加薪”16号XJXCKJ2578</td>\n      <td>理财产品</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>兴业银行“金雪球-优悦”保本开放式人民币理财产品(2M)</td>\n      <td>理财产品</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>212362</th>\n      <td>上海浦兴投资发展有限公司</td>\n      <td>其它</td>\n    </tr>\n    <tr>\n      <th>212363</th>\n      <td>上海浦兴投资发展有限公司</td>\n      <td>其它</td>\n    </tr>\n    <tr>\n      <th>212364</th>\n      <td>上海浦兴投资发展有限公司</td>\n      <td>其它</td>\n    </tr>\n    <tr>\n      <th>212365</th>\n      <td>上海浦兴投资发展有限公司</td>\n      <td>其它</td>\n    </tr>\n    <tr>\n      <th>212366</th>\n      <td>上海市浦东新区建设(集团)有限公司</td>\n      <td>其它</td>\n    </tr>\n  </tbody>\n</table>\n<p>212367 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\lsqlh\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 1.903 seconds.\nPrefix dict has been built successfully.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tgrocery.Grocery at 0x193be8cdc10>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "# 最后一部分字段采用预测好的部分，跟提取的text做交互采用双输入lstm在dense层做交互预测最后几个字段\n",
    "\n",
    "# train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "# result_matrix\n",
    "from tgrocery import Grocery\n",
    "train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "\n",
    "train_lstm_input = train_lstm_input.fillna('否')\n",
    "\n",
    "# label_1理财类型-10  label_2资金来源-3 label_3实际购买公司和上市公司关系-3 label_4买卖方是否有关联关系-2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_1 = LabelEncoder()\n",
    "# label_2 = LabelEncoder()\n",
    "# label_3 = LabelEncoder()\n",
    "# label_4 = LabelEncoder()\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "tmp=pd.DataFrame()\n",
    "train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) \n",
    "\n",
    "# train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) + '_' + train_lstm_input['产品发行方名称'].astype(str)\n",
    "\n",
    "# train_data['text_2'] = train_lstm_input['text'].astype(str)\n",
    "\n",
    "# train_lstm_input[\"文本类别\"]=\"理财产品\"\n",
    "\n",
    "train_data['label_1'] = \"理财产品\"\n",
    "\n",
    "\n",
    "train_data2=train_lstm_input[train_lstm_input[\"产品发行方名称\"]!=\"否\"].reset_index(drop=True)\n",
    "\n",
    "# train_data2[\"文本类别\"]=\"发行方\"\n",
    "\n",
    "tmp['text_1']=train_data2[\"产品发行方名称\"].astype(str)\n",
    "\n",
    "# tmp['text_2']= train_data2[\"text\"].astype(str)\n",
    "\n",
    "tmp['label_1']=\"发行方\"\n",
    "\n",
    "train_data = pd.concat([train_data,tmp]).reset_index(drop=True)\n",
    "\n",
    "train_data2=train_lstm_input[train_lstm_input[\"实际购买公司名称\"]!=\"否\"].reset_index(drop=True)\n",
    "\n",
    "# # train_data2[\"文本类别\"]=\"发行方\"\n",
    "\n",
    "tmp['text_1']=train_data2[\"实际购买公司名称\"].astype(str)\n",
    "\n",
    "# tmp['text_2']= train_data2[\"text\"].astype(str)\n",
    "\n",
    "tmp['label_1']=\"购买公司\"\n",
    "\n",
    "train_data = pd.concat([train_data,tmp]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "other_columns_list=[\"认购金额(万元)\",\"认购日期\",\"资金来源\",\"实际购买公司和上市公司关系\",\"实际购买公司名称\"]\n",
    "\n",
    "for item in other_columns_list:\n",
    "\n",
    "    train_lstm_input[item]=train_lstm_input[item].astype(str)\n",
    "\n",
    "    train_data2=train_lstm_input[train_lstm_input[item]!=\"否\"].reset_index(drop=True)\n",
    "\n",
    "    # train_data2[\"文本类别\"]=item\n",
    "\n",
    "    tmp['text_1']=train_data2[item].astype(str)\n",
    "\n",
    "    # tmp['text_2']= train_data2[\"text\"].astype(str)\n",
    "\n",
    "    tmp['label_1']=\"其它\"\n",
    "\n",
    "    \n",
    "    train_data = pd.concat([train_data,tmp]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_data['label_2'] = label_2.fit_transform(train_lstm_input['资金来源'])\n",
    "# train_data['label_3'] = label_3.fit_transform(train_lstm_input['实际购买公司和上市公司关系'])\n",
    "# train_data['label_4'] = label_4.fit_transform(train_lstm_input['买卖方是否有关联关系'])\n",
    "train_data\n",
    "\n",
    "train_src=[]\n",
    "for text,label in train_data[[\"text_1\",\"label_1\"]].values:\n",
    "    train_src.append([label,text])\n",
    "\n",
    "\n",
    "grocery=Grocery(\"productOrcounter\")\n",
    "\n",
    "\n",
    "grocery.train(train_src)\n",
    "\n",
    "grocery.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.清洗提取出来的tabel数据，主要是清洗掉有问题的列 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data=pl.Path(r\"datasets\\1_traintocsv_result.txt\")\n",
    "with open(result_data,encoding=\"utf8\") as f:\n",
    "    val_csv_df=eval(f.read())\n",
    "\n",
    "val_csv_df=pd.DataFrame(val_csv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "list"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "type(val_csv_df[\"tables\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 9017/9017 [00:19<00:00, 461.90it/s]\n"
    }
   ],
   "source": [
    "\n",
    "def str2tables(tabel):\n",
    "    table=tabel\n",
    "    table_result=[]\n",
    "    product_df_list=[]\n",
    "\n",
    "    for item in table:\n",
    "        if(len(item)>0):\n",
    "            # print(item[0])\n",
    "            table_result.append(pd.Series(item[0]))\n",
    "            \n",
    "            \n",
    "            product_df_list.append(pd.DataFrame(item[1:]))\n",
    "            # print(pd.DataFrame(item[1:]))\n",
    "            # print(\"--------\")\n",
    "        else:\n",
    "            continue\n",
    "    return table_result,product_df_list\n",
    "    \n",
    "def get_column_table(val_df):\n",
    "    tmp_table_column={}\n",
    "    tmp_table_column[\"sample_id\"]=[]\n",
    "    tmp_table_column[\"columns\"]=[]\n",
    "    tmp_table_column[\"product_df\"]=[]\n",
    "    for sample_id,tabel in tqdm(val_df[[\"sample_id\",\"tables\"]].values):\n",
    "        table_result,product_df_list=str2tables(tabel)\n",
    "        index=-1\n",
    "        # print(table_result)\n",
    "        for item in table_result:\n",
    "            index+=1\n",
    "            tmp_table_column[\"sample_id\"].append(sample_id)\n",
    "            tmp_table_column[\"columns\"].append(item)\n",
    "            tmp_table_column[\"product_df\"].append(product_df_list[index])\n",
    "    \n",
    "    return pd.DataFrame(tmp_table_column)\n",
    "\n",
    "val_table_column=get_column_table(val_csv_df)\n",
    "# test_table_column=get_column_table(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     sample_id  \\\n0        12850   \n1        13596   \n2        13596   \n3        13555   \n4        13555   \n...        ...   \n1444     12861   \n1445     12875   \n1446     12875   \n1447     12691   \n1448     12691   \n\n                                                                                                  columns  \\\n0     0        受托人\n1       关联关系\n2       投资期限\n3       产品类型\n4    购买金额（元）\n5       购入日期\n6       终止日期\n7    ...   \n1     0            序号\n1          投资主体\n2           受托方\n3          关联关系\n4          产品名称\n5          产品类型\n...   \n2     0            序号\n1          投资主体\n2           受托方\n3          关联关系\n4          产品名称\n5          产品类型\n...   \n3     0         受托方\n1        产品名称\n2        关联关系\n3    认购金额（万元）\n4        起止期限\n5          状态\n6    利息收入（万元...   \n4     0         受托方\n1        产品名称\n2        关联关系\n3    认购金额（万元）\n4        起止期限\n5          状态\n6    利息收入（万元...   \n...                                                                                                   ...   \n1444  0          序号\n1         委托方\n2         受托方\n3    委托理财产品名称\n4        产品类型\n5    委托金额（万元）\n6       期限（天...   \n1445  0         序号\n1       公告编号\n2        委托方\n3        受托方\n4        起息日\n5        到期日\n6     金额（万元）\n7    ...   \n1446  0          序号\n1         委托方\n2         受托方\n3    委托理财产品名称\n4        产品类型\n5    委托金额（万元）\n6       期限（天...   \n1447  0        受托人\n1       产品名称\n2       产品类型\n3         金额\n4      期限（天）\n5      理财起始日\n6      理财终止日\n7    ...   \n1448  0         序号\n1        受托人\n2       产品名称\n3       产品类型\n4         金额\n5      期限(天)\n6      理财起始日\n7    ...   \n\n                                                                                               product_df  \n0                 0  1    2       3              4          5           6       7  \\\n0  浦发银行昆明关上支行  无 ...  \n1        0            1         2  3                 4        5     6             7  \\\n0  1  广州龙之杰科技有限...  \n2          0                  1                       2  3  \\\n0    1        广州龙之杰科技有限公司           中国工商...  \n3                       0      1  2          3                    4    5        6\n0  中国光大银行股份有限公司惠州分行 ...  \n4                 0        1  2          3                     4    5       6\n0  中国民生银行深圳分行  保本型理财产品  ...  \n...                                                                                                   ...  \n1444     0     1     2              3        4       5   6             7  \\\n0  1  必康新沂  江苏银行  聚宝财富2017...  \n1445     0         1     2         3             4             5       6     7  \\\n0  1  2016-156  必康新沂...  \n1446     0     1         2           3        4       5  6             7  \\\n0  1  必康新沂  交通银行徐州分行  蕴通财富...  \n1447                  0                          1        2     3   4          5  \\\n0  中信银行股份有限公司宁波分行 ...  \n1448       0                   1                                          2  \\\n0    1          宁波银行股份有...  \n\n[1449 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>columns</th>\n      <th>product_df</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12850</td>\n      <td>0        受托人\n1       关联关系\n2       投资期限\n3       产品类型\n4    购买金额（元）\n5       购入日期\n6       终止日期\n7    ...</td>\n      <td>0  1    2       3              4          5           6       7  \\\n0  浦发银行昆明关上支行  无 ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13596</td>\n      <td>0            序号\n1          投资主体\n2           受托方\n3          关联关系\n4          产品名称\n5          产品类型\n...</td>\n      <td>0            1         2  3                 4        5     6             7  \\\n0  1  广州龙之杰科技有限...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13596</td>\n      <td>0            序号\n1          投资主体\n2           受托方\n3          关联关系\n4          产品名称\n5          产品类型\n...</td>\n      <td>0                  1                       2  3  \\\n0    1        广州龙之杰科技有限公司           中国工商...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13555</td>\n      <td>0         受托方\n1        产品名称\n2        关联关系\n3    认购金额（万元）\n4        起止期限\n5          状态\n6    利息收入（万元...</td>\n      <td>0      1  2          3                    4    5        6\n0  中国光大银行股份有限公司惠州分行 ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13555</td>\n      <td>0         受托方\n1        产品名称\n2        关联关系\n3    认购金额（万元）\n4        起止期限\n5          状态\n6    利息收入（万元...</td>\n      <td>0        1  2          3                     4    5       6\n0  中国民生银行深圳分行  保本型理财产品  ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1444</th>\n      <td>12861</td>\n      <td>0          序号\n1         委托方\n2         受托方\n3    委托理财产品名称\n4        产品类型\n5    委托金额（万元）\n6       期限（天...</td>\n      <td>0     1     2              3        4       5   6             7  \\\n0  1  必康新沂  江苏银行  聚宝财富2017...</td>\n    </tr>\n    <tr>\n      <th>1445</th>\n      <td>12875</td>\n      <td>0         序号\n1       公告编号\n2        委托方\n3        受托方\n4        起息日\n5        到期日\n6     金额（万元）\n7    ...</td>\n      <td>0         1     2         3             4             5       6     7  \\\n0  1  2016-156  必康新沂...</td>\n    </tr>\n    <tr>\n      <th>1446</th>\n      <td>12875</td>\n      <td>0          序号\n1         委托方\n2         受托方\n3    委托理财产品名称\n4        产品类型\n5    委托金额（万元）\n6       期限（天...</td>\n      <td>0     1         2           3        4       5  6             7  \\\n0  1  必康新沂  交通银行徐州分行  蕴通财富...</td>\n    </tr>\n    <tr>\n      <th>1447</th>\n      <td>12691</td>\n      <td>0        受托人\n1       产品名称\n2       产品类型\n3         金额\n4      期限（天）\n5      理财起始日\n6      理财终止日\n7    ...</td>\n      <td>0                          1        2     3   4          5  \\\n0  中信银行股份有限公司宁波分行 ...</td>\n    </tr>\n    <tr>\n      <th>1448</th>\n      <td>12691</td>\n      <td>0         序号\n1        受托人\n2       产品名称\n3       产品类型\n4         金额\n5      期限(天)\n6      理财起始日\n7    ...</td>\n      <td>0                   1                                          2  \\\n0    1          宁波银行股份有...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1449 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "val_table_column"
   ]
  },
  {
   "source": [
    "# 入表函数，生成答案的matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "1%|          | 71/7797 [01:30<2:43:19,  1.27s/it]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-955053e3b684>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtemp_single\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m \u001b[0mval_result_matrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_result_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_table_column\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;31m# test_result_matrix=get_result_matrix(test_table_column)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-116-955053e3b684>\u001b[0m in \u001b[0;36mget_result_matrix\u001b[1;34m(result_matrix, time_list, sample_id)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[0msum_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" and \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeFinder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mtime_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m             \u001b[1;31m# print(time_all)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_all\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Work\\数据挖掘\\baseline_青青草原我的家\\src\\time_extractor.py\u001b[0m in \u001b[0;36mfind_time\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch_item\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                     \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1900\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                         \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_date\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[0;32m    567\u001b[0m     format string.\"\"\"\n\u001b[1;32m--> 568\u001b[1;33m     \u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[0;32m    350\u001b[0m                          (data_string, format))\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from src.time_extractor import TimeFinder\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def judge_type(columns):\n",
    "    type_index=[]#1:产品名,2:金额,3:发行方,4:期限,5:实际购买公司,6:认购日期,7:公告日期,8:到息日期,9:非购买金额,10:金额单位\n",
    "    columns=columns.map(lambda x:x.replace(\"（\",\"(\").replace(\"）\",\")\").replace(\" \",\"\"))\n",
    "    product_name_pos_words=[\"产品名称\",\"产品名册\",\"产品名\",\"理财产品\",\"项目名\",\"回购名\",\"回购品\",\"标的名\",\"金融产\",\"投资项\"]#\"存款种类\",\"基金类型\"#不能为空\n",
    "    # product_name_neg_words=[\"编号\",\"代码\"]\n",
    "    amt_pos_words=[\"存款金\",\"认购金\",\"投资金\",\"投入金\",\"受托金\",\"理财金\",\"金额\",\"（元\",\"(元\",\"(万元\",\"（万元\",\"(亿元\",\"（亿元\",\"人民币\",\"投资规\",\"认购规\",\"存款规\",\"投入规\",\"理财规\",\"本金\"]\n",
    "    counter_name_pos_words=[\"受托方\",\"银行机\",\"机构名\",\"合作方名\",\"合作银\",\"合作机\",\"受托人\",\"发行主\",\"签约方\",\"协议方\",\"受托机\",\"受托银\",\"认购银\",\"签约银\",\"签约机\",\"协议机\",\"发生主\",\"存放银\",\"存款银\",\"存款机\",\"存放机\",\"购买银\",\"购买机\",\"管理人\",\"管理银\",\"管理机\",\"银行名\",\"发行机\",\"发行主\",\"发行人\",\"对手方\",\"开户银\",\"开户行\",\"开户机\"]#可以为空\n",
    "    time_length_pos_words=[\"期限\",\"(天)\",\"持有时间\"]\n",
    "    amt_neg_words=[\"实际收回\",\"收回\",\"赎回\",\"实际获得\",\"实际损益\",\"收益情况\",\"投资盈亏\",\"投资收益\",\"理财盈亏\",\"理财收益\",\"盈亏\",\"收益（元\",\"收益(元\",\"收益(万元\",\"收益（万元\",\"到期收益\",\"到期收\",\"是否到\",\"是否已\",\"目前状\",\"到期情\",\"到息情\"]\n",
    "\n",
    "    purchaser_pos_words=[\"委托人\",\"委托公\",\"购买公\",\"委托方\",\"购买单\"]\n",
    "    subscription_dt_pos_words=[\"认购日期\",\"认购时间\"]\n",
    "    announcement_dt_pos_words=[\"公告日期\",\"公告时间\"]\n",
    "    coupon_dt_pos_words=[\"到期日\",\"到息日\",\"终止日\",\"到账日\"]\n",
    "    curr_words=[\"金额单位\"]\n",
    "\n",
    "    for words in product_name_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        # columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==0))\n",
    "        if True in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==0):\n",
    "        for words in [\"种类\",\"类型\",\"类别\"]:\n",
    "            judge_flag=[]\n",
    "            columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "            # columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==0))\n",
    "            if True  in judge_flag:\n",
    "                type_index.append(judge_flag.index(True))\n",
    "                break\n",
    "        if(len(type_index)==0):\n",
    "            type_index.append(-1)\n",
    "        \n",
    "\n",
    "    for words in amt_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==1):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in counter_name_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==2):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in time_length_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==3):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in purchaser_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==4):\n",
    "        type_index.append(-1)\n",
    "\n",
    "    for words in subscription_dt_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==5):\n",
    "        type_index.append(-1)\n",
    "\n",
    "    for words in announcement_dt_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==6):\n",
    "        type_index.append(-1)\n",
    "\n",
    "    for words in coupon_dt_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==7):\n",
    "        type_index.append(-1)\n",
    "\n",
    "\n",
    "    for words in amt_neg_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==8):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in curr_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(x==\"单位\"))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==9):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    \n",
    "    return type_index\n",
    "def get_result_matrix(result_matrix,time_list,sample_id=None):\n",
    "\n",
    "    temp_single={}\n",
    "    temp_single['认购日期'] = []\n",
    "    temp_single['产品起息日'] = []\n",
    "    temp_single['产品到息日'] = []\n",
    "    temp_single['产品期限'] = []\n",
    "    temp_single['认购金额(万元)'] = []\n",
    "    temp_single['产品发行方名称'] = []\n",
    "    temp_single['理财产品名称'] = []\n",
    "    temp_single['sample_id'] = []\n",
    "    temp_single[\"实际购买公司名称\"]=  []\n",
    "    temp_single[\"公告日期\"]=[]\n",
    "\n",
    "    if(sample_id is not None):\n",
    "        result_matrix=result_matrix[result_matrix[\"sample_id\"]==sample_id]\n",
    "    for sample_id,columns,product_df in tqdm(result_matrix[[\"sample_id\",\"columns\",\"product_df\"]].values):\n",
    "        \n",
    "        type_index=judge_type(columns)\n",
    "        # product_df\n",
    "        # columns\n",
    "        # type_index\n",
    "        for index in product_df.index:\n",
    "            tmp_df=product_df.loc[index].fillna(\"\")\n",
    "            if(len(tmp_df.shape) ==2 ):\n",
    "                tmp_df=tmp_df.reset_index(drop=True).loc[0]\n",
    "            product_name=\"\"#理财产品名称\n",
    "            amt=\"\"#认购金额(万元)\n",
    "            counter_name=\"\"#产品发行方\n",
    "            pur_dt=\"\"#6认购日期\n",
    "            val_dt=\"\"#起息日期\n",
    "            coupon_dt=\"\"#8到息日期\n",
    "            time_limit=\"\"#期限\n",
    "            purchaser_name=\"\"#5实际购买公司\n",
    "            announcement_dt=\"\"#7公告日期\n",
    "            curr=\"\"#9金额单位\n",
    "\n",
    "\n",
    "            #产品名\n",
    "            if(type_index[0]!=-1):\n",
    "                if (str(grocery.predict(tmp_df.loc[type_index[0]])) == \"理财产品\"):\n",
    "                    product_name=tmp_df.loc[type_index[0]]\n",
    "            else:\n",
    "                candidate_list={}\n",
    "                candidate_list[\"理财产品\"]=[]\n",
    "                candidate_list[\"发行方\"]=[]\n",
    "                candidate_list[\"其它\"]=[]\n",
    "                candidate_list[\"购买公司\"]=[]\n",
    "                for each_word in tmp_df:\n",
    "                    if not (is_number(each_word)):\n",
    "                        candidate_list[str(grocery.predict(each_word))].append(each_word)\n",
    "                if(len(candidate_list[\"理财产品\"])!=0):\n",
    "                    product_name=candidate_list[\"理财产品\"][0]\n",
    "            #金额\n",
    "            if(type_index[1]!=-1):\n",
    "                amt=tmp_df.loc[type_index[1]].replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\")\n",
    "                type_amt=0\n",
    "                if(\"万\" in amt or \"万\" in columns.loc[type_index[1]] or (type_index[9]!=-1 and  \"万\" in columns.loc[type_index[9]])):\n",
    "                    type_amt=1\n",
    "                if(\"亿\" in amt or \"亿\" in columns.loc[type_index[1]] or (type_index[9]!=-1 and  \"亿\" in columns.loc[type_index[9]])):\n",
    "                    type_amt=2\n",
    "                amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "                amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "                if(is_number(amt)):\n",
    "                    amt=float(amt)\n",
    "                    if(type_amt==0 and amt/10000 >float(50)):\n",
    "                        amt/=10000\n",
    "                    if(type_amt==2):\n",
    "                        amt*=10000\n",
    "                # print(amt)\n",
    "            else:\n",
    "                candidate_list=[]\n",
    "                value_list=list(tmp_df)\n",
    "                for item in value_list:\n",
    "                    tmp=str(item).replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\").replace(\"亿\",\"\").replace(\"万\",\"\")\n",
    "                    tmp=re.sub(\"[^0-9.]*额[^0-9.]*\",\"\",tmp)\n",
    "                    tmp=re.sub(\"[^0-9.]*币[^0-9.]*\",\"\",tmp)\n",
    "                    if(is_number(tmp)):\n",
    "                        if(type_index[8]!=-1 and re.search(tmp,tmp_df.loc[type_index[8]]) is None):\n",
    "                            candidate_list.append(float(tmp))\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                if len(candidate_list)>0:\n",
    "                    real_tmp=sorted(candidate_list,reverse=True)[0]\n",
    "\n",
    "                    for item in value_list:\n",
    "                        tmp=str(item).replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\").replace(\"亿\",\"\").replace(\"万\",\"\")\n",
    "                        tmp=re.sub(\"[^0-9.]*额[^0-9.]*\",\"\",tmp)\n",
    "                        tmp=re.sub(\"[^0-9.]*币[^0-9.]*\",\"\",tmp)\n",
    "                        if(is_number(tmp) and float(tmp)==real_tmp):\n",
    "                            amt=item\n",
    "                            type_amt=0\n",
    "                        else:\n",
    "                            continue\n",
    "                        if(\"万\" in amt or (type_index[9]!=-1 and  \"万\" in columns.loc[type_index[9]])):\n",
    "                            type_amt=1\n",
    "                        if(\"亿\" in amt or (type_index[9]!=-1 and  \"亿\" in columns.loc[type_index[9]])):\n",
    "                            type_amt=2\n",
    "                        amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "                        if(is_number(amt)):\n",
    "                            amt=float(amt)\n",
    "                            # print(amt)\n",
    "                            if(type_amt==0 and amt/10000 >float(50)):\n",
    "                                amt/=10000\n",
    "                            if(type_amt==2):\n",
    "                                amt*=10000\n",
    "                        if(amt !=\"\" or amt!=np.nan):\n",
    "                            break\n",
    "            #发行方\n",
    "            if(type_index[2]!=-1):\n",
    "                if (str(grocery.predict(tmp_df.loc[type_index[2]])) == \"发行方\"):\n",
    "                    counter_name=tmp_df.loc[type_index[2]]\n",
    "            else:\n",
    "                candidate_list={}\n",
    "                candidate_list[\"理财产品\"]=[]\n",
    "                candidate_list[\"发行方\"]=[]\n",
    "                candidate_list[\"其它\"]=[]\n",
    "                candidate_list[\"购买公司\"]=[]\n",
    "                for each_word in tmp_df:\n",
    "                    if not (is_number(each_word)):\n",
    "                        each_word=each_word.replace(\"^\",\"\").replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                        candidate_list[str(grocery.predict(each_word))].append(each_word)\n",
    "                # print(candidate_list)\n",
    "                if(len(candidate_list[\"发行方\"])!=0):\n",
    "                    counter_name=candidate_list[\"发行方\"][0]\n",
    "            \n",
    "            #期限\n",
    "            if(type_index[3]!=-1):\n",
    "                text=str(tmp_df.loc[type_index[3]])\n",
    "                # print(text)\n",
    "                a=re.search(\"\\d+?[天]+?\",text)\n",
    "                if (a is None):\n",
    "                    a=re.search(\"\\d+?[个]+[月]+?\",text)\n",
    "                if a is None:\n",
    "                    a=re.search(\"[^\\d]\\d[年]+?\",text)\n",
    "                    if(a is not None):\n",
    "                        a=re.search(\"\\d[年]+?\",a.group())\n",
    "                if a is None:\n",
    "                    a=re.search(\"^\\d[年]+?\",text)\n",
    "                if a is not None:\n",
    "                    time_limit=a.group().replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\")\n",
    "                else:\n",
    "                    if(is_number(text) and type(text) is not float and str(text)!=\"nan\"):\n",
    "                        time_limit=str(text)+\"天\"\n",
    "                        # print(time_limit)\n",
    "                    else:\n",
    "                        time_limit=\"\"\n",
    "\n",
    "            #实际购买公司名称\n",
    "            if(type_index[4]!=-1):\n",
    "                if (str(grocery.predict(tmp_df.loc[type_index[4]])) == \"购买公司\"):\n",
    "                    purchaser_name=tmp_df.loc[type_index[4]]\n",
    "                else:\n",
    "                    candidate_list={}\n",
    "                    candidate_list[\"理财产品\"]=[]\n",
    "                    candidate_list[\"发行方\"]=[]\n",
    "                    candidate_list[\"其它\"]=[]\n",
    "                    candidate_list[\"购买公司\"]=[]\n",
    "                for each_word in tmp_df:\n",
    "                    if not (is_number(each_word)):\n",
    "                        each_word=each_word.replace(\"^\",\"\").replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                        candidate_list[str(grocery.predict(each_word))].append(each_word)\n",
    "                # print(candidate_list)\n",
    "                if(len(candidate_list[\"购买公司\"])!=0):\n",
    "                    counter_name=candidate_list[\"购买公司\"][0]\n",
    "            \n",
    "            #到息日期\n",
    "            if(type_index[7]!=-1):\n",
    "                t = TimeFinder()\n",
    "                time_all=t.find_time(tmp_df.loc[type_index[7]])\n",
    "                if( time_all is not None and len(time_all)==1):\n",
    "                    coupon_dt=time_all[0]\n",
    "                    tmp_df.loc[type_index[7]]=\"\"\n",
    "            \n",
    "            #公告日期\n",
    "            if(type_index[6]!=-1):\n",
    "                t = TimeFinder()\n",
    "                time_all=t.find_time(tmp_df.loc[type_index[6]])\n",
    "                if(time_all is not None and len(time_all)==1):\n",
    "                    announcement_dt=time_all[0]\n",
    "                    tmp_df.loc[type_index[6]]=\"\"\n",
    "\n",
    "            #认购日期\n",
    "            if(type_index[5]!=-1):\n",
    "                t = TimeFinder()\n",
    "                time_all=t.find_time(tmp_df.loc[type_index[2]])\n",
    "                if(time_all is not None and len(time_all)==1):\n",
    "                    pur_dt=time_all[0]\n",
    "                    tmp_df.loc[type_index[5]]=\"\"\n",
    "            \n",
    "            #三个日期\n",
    "            # tmp_df\n",
    "            value_list=[]\n",
    "            noshow=tmp_df.map(lambda x:value_list.append(str(x)))\n",
    "            sum_value=(\" and \").join(i for i in value_list)\n",
    "            t = TimeFinder()\n",
    "            time_all = t.find_time(sum_value)\n",
    "            # print(time_all)\n",
    "            if(time_all is not None):\n",
    "                time_all=sorted(list(set(time_all)),reverse=True)\n",
    "                # print(len(time_all))\n",
    "                # print(sum_value)\n",
    "                # print(product_name)\n",
    "                if(len(time_all)==1):\n",
    "                    pur_dt=time_all[0]\n",
    "                    val_dt=pur_dt\n",
    "                elif(len(time_all)==2):\n",
    "                    # time_all=sorted(list(set(time_all)),reverse=True)\n",
    "                    if(re.search(\"随时\",sum_value) is not None or re.search(\"工作日\",sum_value)):\n",
    "                        time_limit=\"\"\n",
    "                        pur_dt=time_all[1]\n",
    "                        val_dt=time_all[0]\n",
    "                        coupon_dt = \"\"\n",
    "                    else:\n",
    "                        if(pur_dt !=\"\"):\n",
    "                            pur_dt=time_all[1]\n",
    "                        val_dt=time_all[1]\n",
    "                        coupon_dt = time_all[0]\n",
    "                        try:\n",
    "                            # 相减\n",
    "                            if(type_index[3]==-1 or time_limit==\"\"):\n",
    "                                d1 = datetime.datetime.strptime(val_dt, '%Y-%m-%d')\n",
    "                                d2 = datetime.datetime.strptime(coupon_dt, '%Y-%m-%d')\n",
    "                                d = d2 - d1\n",
    "                                time_limit = str(d.days) + '天'\n",
    "                        except Exception:\n",
    "                            coupon_dt = \"\"\n",
    "                            time_limit = \"\"\n",
    "                elif(len(time_all)==3):\n",
    "                        # print(time_all)\n",
    "                        pur_dt=time_all[2]\n",
    "                        val_dt=time_all[1]\n",
    "                        coupon_dt = time_all[0]\n",
    "                        try:\n",
    "                            # 相减\n",
    "                            d1 = datetime.datetime.strptime(pur_dt, '%Y-%m-%d')\n",
    "                            d2 = datetime.datetime.strptime(val_dt, '%Y-%m-%d')\n",
    "                            d = d2 - d1\n",
    "                            if str(d.days)==\"1\":\n",
    "                                d1 = datetime.datetime.strptime(val_dt, '%Y-%m-%d')\n",
    "                                d2 = datetime.datetime.strptime(coupon_dt, '%Y-%m-%d')\n",
    "                                d = d2 - d1\n",
    "                                time_limit=str(d.days)+\"天\"\n",
    "                            else:\n",
    "                                pur_dt=time_all[2]\n",
    "                                val_dt=pur_dt\n",
    "                                coupon_dt=\"\"\n",
    "                        except Exception:\n",
    "                            coupon_dt = \"\"\n",
    "                            time_limit = \"\"\n",
    "                elif(len(time_all)>4):\n",
    "                        time_all=sorted(time_all)\n",
    "                        pur_dt=time_all[0]\n",
    "                        val_dt=\"\"\n",
    "                        coupon_dt=\"\"\n",
    "                if pur_dt!=\"\" and coupon_dt==\"\" and time_limit!=\"\":\n",
    "                    try:\n",
    "                        if \"天\" in time_limit:\n",
    "                            coupon_dt=datetime.datetime.strftime(datetime.datetime.strptime(pur_dt, '%Y-%m-%d')+datetime.timedelta(days=int(re.search(\"\\d*\",time_limit).group())), '%Y-%m-%d')\n",
    "                        elif \"月\" in time_limit:\n",
    "                            coupon_dt=datetime.datetime.strftime(datetime.datetime.strptime(pur_dt, '%Y-%m-%d')+datetime.timedelta(months=int(re.search(\"\\d*\",time_limit).group())), '%Y-%m-%d')\n",
    "                        elif \"年\" in time_limit:\n",
    "                            coupon_dt=datetime.datetime.strftime(datetime.datetime.strptime(pur_dt, '%Y-%m-%d')+datetime.timedelta(years=int(re.search(\"\\d*\",time_limit).group())), '%Y-%m-%d')\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            if(announcement_dt==\"\"):\n",
    "                announcement_dt=time_list[time_list[\"sample_id\"]==sample_id][\"公告日期\"].values\n",
    "            \n",
    "            temp_single['sample_id'].append(sample_id)\n",
    "            temp_single['认购日期'].append(pur_dt)\n",
    "            temp_single['产品起息日'].append(val_dt)\n",
    "            temp_single['产品到息日'].append(coupon_dt)\n",
    "            temp_single['产品期限'] .append(time_limit)\n",
    "            temp_single['认购金额(万元)'].append(amt)\n",
    "            temp_single['产品发行方名称'] .append(counter_name)\n",
    "            temp_single['理财产品名称'] .append(product_name)\n",
    "            temp_single[\"实际购买公司名称\"].append(purchaser_name)\n",
    "            temp_single['公告日期'].append(announcement_dt)\n",
    "    \n",
    "        \n",
    "    temp_single=pd.DataFrame(temp_single)\n",
    "    return temp_single\n",
    "\n",
    "val_result_matrix=get_result_matrix(val_table_column,val_time)\n",
    "# test_result_matrix=get_result_matrix(test_table_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    0\n0   1\n1   2\n2  10\n3   4\n4   5\n5   6\n6   7\n7   8",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "a=[1,2,3,4,5,6,7,8]\n",
    "a=pd.DataFrame(a)\n",
    "a.iloc[2]=10\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_func(result_df):\n",
    "\n",
    "    #弃非空字段的空值\n",
    "    result_df=result_df.dropna(subset=[\"认购日期\",\"理财产品名称\",\"认购金额(万元)\"])\n",
    "    #弃重复值\n",
    "    result_df=result_df.sort_values(by=[\"公告日期\"],axis=0)\n",
    "    result_df=result_df.drop_duplicates(subset=[\"实际\",\"理财产品名称\",\"认购金额(万元)\",\"产品起息日\",\"产品到息日\",\"产品期限\"],keep=first)\n",
    "\n",
    "    return result_df.reset_index(True)\n",
    "\n",
    "val_history_result=reduction_func(val_result_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_table_answer_result(result_df,val_gm):\n",
    "\n",
    "    drop_list=[]\n",
    "    index=-1\n",
    "    for coupon_dt,announcement_dt in tqdm(result_df[[\"产品到息日\",\"公告日期\"]].values):\n",
    "        index+=1\n",
    "        if (coupon_dt !=\"\" and announcement_dt!=\"\"):\n",
    "            d1 = datetime.datetime.strptime(coupon_dt, '%Y-%m-%d')\n",
    "            d2 = datetime.datetime.strptime(announcement_dt, '%Y-%m-%d')\n",
    "            d = d2 - d1\n",
    "            if(int(d.days) > 90):\n",
    "                drop_list.append(index)\n",
    "        \n",
    "    result_df=result_df.drop(drop_list).reset_index(drop=True)\n",
    "\n",
    "    result_df_counter_none=result_df[result_df[\"实际购买公司名称\"]==\"\"]\n",
    "\n",
    "    result_df=result_df[result_df[\"实际购买公司名称\"]!=\"\"]\n",
    "\n",
    "    result_df_counter_none=result_df_counter_none.drop(subset=[\"实际购买公司名称\"],axis=1)\n",
    "\n",
    "    result_df_counter_none=pd.merge(result_df_counter_none,val_gm,on=[\"sample_id\"])\n",
    "\n",
    "    result_df=pd.concat([result_df,result_df_counter_none])\n",
    "     \n",
    "    return result_df\n",
    "\n",
    "val_answer_result=get_table_answer_result(val_history_result,val_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.抽取的是单独的数据包含\n",
    "#### 起息日，到息日， 金额，认购日期，产品发行方，理财产品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1122/1122 [04:45<00:00,  3.93it/s]\n100%|██████████| 5481/5481 [12:16<00:00,  7.45it/s]\n"
    }
   ],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def judge_type(columns):\n",
    "    type_index=[]#1:产品名,2:金额,3:发行方，4:期限\n",
    "    columns=columns.map(lambda x:x.replace(\"（\",\"(\").replace(\"）\",\")\"))\n",
    "    product_name_pos_words=[\"产品名称\",\"产品名册\",\"产品名\",\"理财产品\",\"项目名\",\"回购名\",\"回购品\",\"标的名\",\"金融产\",\"投资项\"]#\"存款种类\",\"基金类型\"#不能为空\n",
    "    # product_name_neg_words=[\"编号\",\"代码\"]\n",
    "    amt_pos_words=[\"存款金\",\"认购金\",\"投资金\",\"投入金\",\"受托金\",\"理财金\",\"金额\",\"（元\",\"(元\",\"(万元\",\"（万元\",\"(亿元\",\"（亿元\",\"人民币\",\"投资规\",\"认购规\",\"存款规\",\"投入规\",\"理财规\"]\n",
    "    counter_name_pos_words=[\"受托方\",\"银行机\",\"机构名\",\"合作方名\",\"合作银\",\"合作机\",\"受托人\",\"发行主\",\"签约方\",\"协议方\",\"受托机\",\"受托银\",\"认购银\",\"签约银\",\"签约机\",\"协议机\",\"发生主\",\"存放银\",\"存款银\",\"存款机\",\"存放机\",\"购买银\",\"购买机\",\"管理人\",\"管理银\",\"管理机\",\"银行名\",\"发行机\",\"发行主\",\"发行人\",\"对手方\",\"开户银\",\"开户行\",\"开户机\"]#可以为空\n",
    "    time_length_pos_words=[\"期限\",\"(天)\",\"持有时间\"]\n",
    "    for words in product_name_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        # columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==0))\n",
    "        if True in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==0):\n",
    "        for words in [\"种类\",\"类型\",\"类别\"]:\n",
    "            judge_flag=[]\n",
    "            columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "            # columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==0))\n",
    "            if True  in judge_flag:\n",
    "                type_index.append(judge_flag.index(True))\n",
    "                break\n",
    "        if(len(type_index)==0):\n",
    "            type_index.append(-1)\n",
    "        \n",
    "\n",
    "    for words in amt_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==1):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in counter_name_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==2):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in time_length_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==3):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    \n",
    "    return type_index\n",
    "def get_answer_matrix(result_matrix,sample_id=None):\n",
    "\n",
    "    temp_single={}\n",
    "    temp_single['认购日期'] = []\n",
    "    temp_single['产品起息日'] = []\n",
    "    temp_single['产品到息日'] = []\n",
    "    temp_single['产品期限'] = []\n",
    "    temp_single['认购金额(万元)'] = []\n",
    "    temp_single['产品发行方名称'] = []\n",
    "    temp_single['理财产品名称'] = []\n",
    "    temp_single['sample_id'] = []\n",
    "\n",
    "    if(sample_id is not None):\n",
    "        result_matrix=result_matrix[result_matrix[\"sample_id\"]==sample_id]\n",
    "    for sample_id,columns,product_df in tqdm(result_matrix[[\"sample_id\",\"columns\",\"product_df\"]].values):\n",
    "        \n",
    "        type_index=judge_type(columns)\n",
    "        # product_df\n",
    "        # columns\n",
    "        # type_index\n",
    "        for index in product_df.index:\n",
    "            tmp_df=product_df.loc[index]\n",
    "            if(len(tmp_df.shape) ==2 ):\n",
    "                tmp_df=tmp_df.reset_index(drop=True).loc[0]\n",
    "            product_name=\"\"\n",
    "            amt=\"\"\n",
    "            counter_name=\"\"\n",
    "            pur_dt=\"\"\n",
    "            val_dt=\"\"\n",
    "            coupon_dt=\"\"\n",
    "            time_limit=\"\"\n",
    "            #产品名\n",
    "            if(type_index[0]!=-1):\n",
    "                if (str(grocery.predict(tmp_df.loc[type_index[0]])) == \"理财产品\"):\n",
    "                    product_name=tmp_df.loc[type_index[0]]\n",
    "            else:\n",
    "                candidate_list={}\n",
    "                candidate_list[\"理财产品\"]=[]\n",
    "                candidate_list[\"发行方\"]=[]\n",
    "                candidate_list[\"其它\"]=[]\n",
    "                candidate_list[\"购买公司\"]=[]\n",
    "                for each_word in tmp_df.head(1):\n",
    "                    if not (is_number(each_word)):\n",
    "                        candidate_list[str(grocery.predict(each_word))].append(each_word)\n",
    "                if(len(candidate_list[\"理财产品\"])!=0):\n",
    "                    product_name=candidate_list[\"理财产品\"][0]\n",
    "            #金额\n",
    "            if(type_index[1]!=-1):\n",
    "                amt=tmp_df.loc[type_index[1]].replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\")\n",
    "                type_amt=0\n",
    "                if(\"万\" in amt or \"万\" in columns.loc[type_index[1]]):\n",
    "                    type_amt=1\n",
    "                if(\"亿\" in amt or \"亿\" in columns.loc[type_index[1]]):\n",
    "                    type_amt=2\n",
    "                amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "                amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "                if(is_number(amt)):\n",
    "                    amt=float(amt)\n",
    "                    if(type_amt==0 and amt/10000 >float(50)):\n",
    "                        amt/=10000\n",
    "                    if(type_amt==2):\n",
    "                        amt*=10000\n",
    "                # print(amt)\n",
    "            else:\n",
    "                candidate_list=[]\n",
    "                value_list=list(tmp_df)\n",
    "                for item in value_list:\n",
    "                    tmp=str(item).replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\").replace(\"亿\",\"\").replace(\"万\",\"\")\n",
    "                    tmp=re.sub(\"[^0-9.]*额[^0-9.]*\",\"\",tmp)\n",
    "                    tmp=re.sub(\"[^0-9.]*币[^0-9.]*\",\"\",tmp)\n",
    "                    if(is_number(tmp)):\n",
    "                        candidate_list.append(float(tmp))\n",
    "\n",
    "                if len(candidate_list)>0:\n",
    "                    real_tmp=sorted(candidate_list,reverse=True)[0]\n",
    "\n",
    "                    for item in value_list:\n",
    "                        tmp=str(item)\n",
    "                        tmp=re.sub(\"[^0-9.]*额[^0-9.]*\",\"\",tmp)\n",
    "                        tmp=re.sub(\"[^0-9.]*币[^0-9.]*\",\"\",tmp)\n",
    "                        if(is_number(tmp) and float(tmp)==real_tmp):\n",
    "                            amt=item\n",
    "                            type_amt=0\n",
    "                        else:\n",
    "                            continue\n",
    "                        if(\"万\" in amt ):\n",
    "                            type_amt=1\n",
    "                        if(\"亿\" in amt ):\n",
    "                            type_amt=2\n",
    "                        amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "                        if(is_number(amt)):\n",
    "                            amt=float(amt)\n",
    "                            # print(amt)\n",
    "                            if(type_amt==0 and amt/10000 >float(50)):\n",
    "                                amt/=10000\n",
    "                            if(type_amt==2):\n",
    "                                amt*=10000\n",
    "                        if(amt !=\"\" or amt!=np.nan):\n",
    "                            break\n",
    "            #发行方\n",
    "            if(type_index[2]!=-1):\n",
    "                if (str(grocery.predict(tmp_df.loc[type_index[2]])) == \"发行方\"):\n",
    "                    counter_name=tmp_df.loc[type_index[2]]\n",
    "            else:\n",
    "                candidate_list={}\n",
    "                candidate_list[\"理财产品\"]=[]\n",
    "                candidate_list[\"发行方\"]=[]\n",
    "                candidate_list[\"其它\"]=[]\n",
    "                candidate_list[\"购买公司\"]=[]\n",
    "                for each_word in tmp_df:\n",
    "                    if not (is_number(each_word)):\n",
    "                        each_word=each_word.replace(\"^\",\"\").replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "                        candidate_list[str(grocery.predict(each_word))].append(each_word)\n",
    "                # print(candidate_list)\n",
    "                if(len(candidate_list[\"发行方\"])!=0):\n",
    "                    counter_name=sorted(candidate_list[\"发行方\"],reverse=True)[0]\n",
    "            \n",
    "            #期限\n",
    "            if(type_index[3]!=-1):\n",
    "                text=str(tmp_df.loc[type_index[3]])\n",
    "                # print(text)\n",
    "                a=re.search(\"\\d+?[天]+?\",text)\n",
    "                if (a is None):\n",
    "                    a=re.search(\"\\d+?[个]+[月]+?\",text)\n",
    "                if a is None:\n",
    "                    a=re.search(\"[^\\d]\\d[年]+?\",text)\n",
    "                    if(a is not None):\n",
    "                        a=re.search(\"\\d[年]+?\",a.group())\n",
    "                if a is None:\n",
    "                    a=re.search(\"^\\d[年]+?\",text)\n",
    "                if a is not None:\n",
    "                    time_limit=a.group().replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\")\n",
    "                else:\n",
    "                    if(is_number(text) and type(text) is not float):\n",
    "                        time_limit=str(text)+\"天\"\n",
    "                        # print(time_limit)\n",
    "                    else:\n",
    "                        time_limit=\"\"\n",
    "\n",
    "            #三个日期\n",
    "            # tmp_df\n",
    "            value_list=[]\n",
    "            noshow=tmp_df.map(lambda x:value_list.append(str(x)))\n",
    "            sum_value=(\" and \").join(i for i in value_list)\n",
    "            t = TimeFinder()\n",
    "            time_all = t.find_time(sum_value)\n",
    "            # print(time_all)\n",
    "            if(time_all is not None):\n",
    "                time_all=sorted(list(set(time_all)),reverse=True)\n",
    "                # print(len(time_all))\n",
    "                # print(sum_value)\n",
    "                # print(product_name)\n",
    "                if(len(time_all)==1):\n",
    "                    pur_dt=time_all[0]\n",
    "                    val_dt=pur_dt\n",
    "                elif(len(time_all)==2):\n",
    "                    # time_all=sorted(list(set(time_all)),reverse=True)\n",
    "                    if(re.search(\"随时\",sum_value) is not None or re.search(\"工作日\",sum_value)):\n",
    "                        time_limit=\"\"\n",
    "                        pur_dt=time_all[1]\n",
    "                        val_dt=time_all[0]\n",
    "                        coupon_dt = \"\"\n",
    "                    else:\n",
    "                        pur_dt=time_all[1]\n",
    "                        val_dt=time_all[1]\n",
    "                        coupon_dt = time_all[0]\n",
    "                        try:\n",
    "                            # 相减\n",
    "                            if(type_index[3]==-1 or time_limit==\"\"):\n",
    "                                d1 = datetime.datetime.strptime(val_dt, '%Y-%m-%d')\n",
    "                                d2 = datetime.datetime.strptime(coupon_dt, '%Y-%m-%d')\n",
    "                                d = d2 - d1\n",
    "                                time_limit = str(d.days) + '天'\n",
    "                        except Exception:\n",
    "                            coupon_dt = \"\"\n",
    "                            time_limit = \"\"\n",
    "                elif(len(time_all)==3):\n",
    "                        # print(time_all)\n",
    "                        pur_dt=time_all[2]\n",
    "                        val_dt=time_all[1]\n",
    "                        coupon_dt = time_all[0]\n",
    "                        try:\n",
    "                            # 相减\n",
    "                            d1 = datetime.datetime.strptime(pur_dt, '%Y-%m-%d')\n",
    "                            d2 = datetime.datetime.strptime(val_dt, '%Y-%m-%d')\n",
    "                            d = d2 - d1\n",
    "                            if str(d.days)==\"1\":\n",
    "                                d1 = datetime.datetime.strptime(val_dt, '%Y-%m-%d')\n",
    "                                d2 = datetime.datetime.strptime(coupon_dt, '%Y-%m-%d')\n",
    "                                d = d2 - d1\n",
    "                                time_limit=str(d.days)+\"天\"\n",
    "                            else:\n",
    "                                pur_dt=time_all[2]\n",
    "                                val_dt=pur_dt\n",
    "                                coupon_dt=\"\"\n",
    "                        except Exception:\n",
    "                            coupon_dt = \"\"\n",
    "                            time_limit = \"\"\n",
    "                elif(len(time_all)>4):\n",
    "                        time_all=sorted(time_all)\n",
    "                        pur_dt=time_all[0]\n",
    "                        val_dt=\"\"\n",
    "                        coupon_dt=\"\"\n",
    "                if pur_dt!=\"\" and coupon_dt==\"\" and time_limit!=\"\":\n",
    "                    try:\n",
    "                        coupon_dt=datetime.datetime.strftime(datetime.datetime.strptime(pur_dt, '%Y-%m-%d')+datetime.timedelta(days=int(re.search(\"\\d*\",time_limit).group())), '%Y-%m-%d')\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            temp_single['认购日期'].append(pur_dt)\n",
    "            temp_single['产品起息日'].append(val_dt)\n",
    "            temp_single['产品到息日'].append(coupon_dt)\n",
    "            temp_single['产品期限'] .append(time_limit)\n",
    "            temp_single['认购金额(万元)'].append(amt)\n",
    "            temp_single['产品发行方名称'] .append(counter_name)\n",
    "            temp_single['理财产品名称'] .append(product_name)\n",
    "            temp_single['sample_id'].append(sample_id)\n",
    "        \n",
    "    temp_single=pd.DataFrame(temp_single)\n",
    "    return temp_single\n",
    "\n",
    "val_temp_single=get_answer_matrix(val_result_matrix)\n",
    "test_temp_single=get_answer_matrix(test_result_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.汇总整理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp_single裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_judge(judge_title_result,result,score_limit=31):\n",
    "    # global judge_title_result\n",
    "    drop_list=[]\n",
    "    index=0\n",
    "    for sample_id,product_name in tqdm(result[[\"sample_id\",\"理财产品名称\"]].values):\n",
    "        # print(sample_id)\n",
    "        score_list=[]\n",
    "        for text in judge_title_result[judge_title_result[\"sample_id\"]==int(sample_id)][4].values:\n",
    "            # print(text)\n",
    "            score_list.append(fuzz.partial_token_sort_ratio(product_name,text))\n",
    "        # print(score_list)\n",
    "        if  len(score_list)>0 and np.max(pd.DataFrame(score_list)[0])<=score_limit:\n",
    "            drop_list.append(index)\n",
    "        index+=1 \n",
    "    \n",
    "    return result.copy().reset_index(drop=True).drop(drop_list)\n",
    "\n",
    "# drop_judge(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 6022/6022 [00:18<00:00, 328.01it/s]\n100%|██████████| 15718/15718 [00:47<00:00, 334.33it/s]\n"
    }
   ],
   "source": [
    "\n",
    "val_time[\"sample_id\"]=val_time[\"sample_id\"].astype(str)\n",
    "val_gm[\"sample_id\"]=val_gm[\"sample_id\"].astype(str)\n",
    "test_time[\"sample_id\"]=test_time[\"sample_id\"].astype(str)\n",
    "test_gm[\"sample_id\"]=test_gm[\"sample_id\"].astype(str)\n",
    "\n",
    "def get_result(judge_title_result,time_list,gm_list,result_matrix):\n",
    "    tmp_result=pd.merge(time_list,gm_list,on=[\"sample_id\"])\n",
    "    tmp_result[\"sample_id\"]=tmp_result[\"sample_id\"].astype(str)\n",
    "    r=result_matrix.fillna(\"\").reset_index(drop=True)\n",
    "    i=0\n",
    "    i_list=[]\n",
    "    for index in r.index:\n",
    "        if r.loc[index].dropna().shape[0]<=5 or type(r.loc[index][\"理财产品名称\"]) is float or len(r.loc[index][\"理财产品名称\"])<2:\n",
    "            i_list.append(i)\n",
    "        i+=1\n",
    "    r=r.drop(i_list)\n",
    "    r=r.fillna(\"\").applymap(lambda x:str(x).replace(\" \",\"\"))\n",
    "    r=drop_judge(judge_title_result,r)\n",
    "\n",
    "\n",
    "    result=pd.merge(tmp_result,r,on=[\"sample_id\"]).reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "val_result=get_result(val_judge_title_result,val_time,val_gm,val_temp_single)\n",
    "test_result=get_result(test_judge_title_result,test_time,test_gm,test_temp_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.060510557831705"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "  def get_F1(val_pred, val_true):\n",
    "      val_pred = list(val_pred)\n",
    "      val_true = list(val_true)\n",
    "      curr = list(set(val_pred).intersection(set(val_true)))\n",
    "      R = len(curr)/len(val_true)\n",
    "      P = len(curr)/len(val_pred)\n",
    "      return 2*P*R/(P+R)\n",
    "\n",
    "  r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "  val_true = r['sample_id'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) +r['产品发行方名称'].astype(str) + r['认购日期'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str)+ r['公告日期'].astype(str)+ r['实际购买公司名称'].astype(str)\n",
    "    # r.to_excel(\"result_after_drop.xlsx\",index=None)\n",
    "  \n",
    "r=val_result\n",
    "\n",
    "\n",
    "val_pred = r['sample_id'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) +r['产品发行方名称'].astype(str) + r['认购日期'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str)+ r['公告日期'].astype(str)+ r['实际购买公司名称'].astype(str)\n",
    "score = get_F1(val_pred, val_true)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模过程（预处理模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.采用LSTM网络用提取好的部分跟pdf中的text做交互预测\n",
    "#### 理财类型、资金来源、实际购买公司和上市公司关系、买卖方是否有关联关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后一部分字段采用预测好的部分，跟提取的text做交互采用双输入lstm在dense层做交互预测最后几个字段\n",
    "train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "train_lstm_input = train_lstm_input.fillna('否')\n",
    "# label_1理财类型-10  label_2资金来源-3 label_3实际购买公司和上市公司关系-3 label_4买卖方是否有关联关系-2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_1 = LabelEncoder()\n",
    "label_2 = LabelEncoder()\n",
    "label_3 = LabelEncoder()\n",
    "label_4 = LabelEncoder()\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) + '_' + train_lstm_input['产品发行方名称'].astype(str)\n",
    "train_data['text_2'] = train_lstm_input['text'].astype(str)\n",
    "\n",
    "train_data['label_1'] = label_1.fit_transform(train_lstm_input['理财类型'])\n",
    "train_data['label_2'] = label_2.fit_transform(train_lstm_input['资金来源'])\n",
    "train_data['label_3'] = label_3.fit_transform(train_lstm_input['实际购买公司和上市公司关系'])\n",
    "train_data['label_4'] = label_4.fit_transform(train_lstm_input['买卖方是否有关联关系'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import *\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import gensim\n",
    "import jieba\n",
    "tqdm.pandas()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# 显卡使用（如没显卡需要注释掉）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.random.set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 27154/27154 [00:03<00:00, 7910.89it/s]\n100%|██████████| 27154/27154 [12:21<00:00, 36.61it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                           text_1  \\\n0               中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行   \n1               中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行   \n2                               与 利率 挂钩 的 结构性 产品 _ 中国民生银行 股份 有限公司   \n3            广发 银行 “ 薪 加薪 ” 16 号 XJXCKJ2578 _ 广发 银行 股份 有限公司 清远 分行   \n4  兴业银行 “ 金 雪球 - 优悦 ” 保本 开放式 人民币 理财产品 ( 2M ) _ 兴业银行 股份 有限公司 芜湖 分行   \n\n                                                                                                text_2  \\\n0  [ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都 科技                   公告 编号 ： 2016 - 099   \\ n   \\...   \n1  [ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都 科技                   公告 编号 ： 2016 - 099   \\ n   \\...   \n2  [ ' 证券 代码 ： 600211                             证券 简称 ： 西藏药业                           公告 编号 ： 20...   \n3  [ ' 证券 代码 ： 002171                               证券 简称 ： 楚江 新材                               公告 ...   \n4  [ ' 证券 代码 ： 002171                               证券 简称 ： 楚江 新材                               公告 ...   \n\n   label_1  label_2  label_3  label_4  \n0        9        1        0        0  \n1        9        1        0        0  \n2        9        0        0        0  \n3        9        1        2        0  \n4        9        1        0        0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_1</th>\n      <th>text_2</th>\n      <th>label_1</th>\n      <th>label_2</th>\n      <th>label_3</th>\n      <th>label_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行</td>\n      <td>[ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都 科技                   公告 编号 ： 2016 - 099   \\ n   \\...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行</td>\n      <td>[ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都 科技                   公告 编号 ： 2016 - 099   \\ n   \\...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>与 利率 挂钩 的 结构性 产品 _ 中国民生银行 股份 有限公司</td>\n      <td>[ ' 证券 代码 ： 600211                             证券 简称 ： 西藏药业                           公告 编号 ： 20...</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>广发 银行 “ 薪 加薪 ” 16 号 XJXCKJ2578 _ 广发 银行 股份 有限公司 清远 分行</td>\n      <td>[ ' 证券 代码 ： 002171                               证券 简称 ： 楚江 新材                               公告 ...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>兴业银行 “ 金 雪球 - 优悦 ” 保本 开放式 人民币 理财产品 ( 2M ) _ 兴业银行 股份 有限公司 芜湖 分行</td>\n      <td>[ ' 证券 代码 ： 002171                               证券 简称 ： 楚江 新材                               公告 ...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train_data['text_1'] = train_data['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data['text_2'] = train_data['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer 序列化文本\n",
    "def set_tokenizer(docs, split_char=' ', max_len=100):\n",
    "    '''\n",
    "    输入\n",
    "    docs:文本列表\n",
    "    split_char:按什么字符切割\n",
    "    max_len:截取的最大长度\n",
    "    \n",
    "    输出\n",
    "    X:序列化后的数据\n",
    "    word_index:文本和数字对应的索引\n",
    "    '''\n",
    "    tokenizer = Tokenizer(lower=False, char_level=False, split=split_char)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    X = tokenizer.texts_to_sequences(docs)\n",
    "    maxlen = max_len\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0)\n",
    "    word_index=tokenizer.word_index\n",
    "    return X, word_index, tokenizer\n",
    "\n",
    "### 做embedding 这里采用word2vec 可以换成其他例如（glove词向量）\n",
    "def trian_save_word2vec(docs, embed_size=300, save_name='w2v.txt', split_char=' '):\n",
    "    '''\n",
    "    输入\n",
    "    docs:输入的文本列表\n",
    "    embed_size:embed长度\n",
    "    save_name:保存的word2vec位置\n",
    "    \n",
    "    输出\n",
    "    w2v:返回的模型\n",
    "    '''\n",
    "    input_docs = []\n",
    "    for i in docs:\n",
    "        input_docs.append(i.split(split_char))\n",
    "    logging.basicConfig(\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "    w2v = Word2Vec(input_docs, size=embed_size, sg=1, window=8, seed=1017, workers=24, min_count=1, iter=10)\n",
    "    w2v.save(save_name)\n",
    "    print(\"w2v model done\")\n",
    "    return w2v\n",
    "\n",
    "# 得到embedding矩阵\n",
    "def get_embedding_matrix(word_index, embed_size=300, Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = Word2Vec.load(Emed_path)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  \n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "# 得到fasttext矩阵\n",
    "def load_fasttext(word_index, path):  \n",
    "    count=0\n",
    "    null_list=[]\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(path, encoding='utf-8') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words =  len(word_index)+1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            null_list.append(word)\n",
    "            count+=1\n",
    "    print(\"null cnt:\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embedding_matrix_txt(word_index,embed_size=200,Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        Emed_path, binary=False)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.训练得到word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "开始序列化\n序列化完成\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "27"
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "50:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:21,667:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:21,676:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:21,680:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:21,683:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:21,694:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:21,703:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:21,706:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:21,708:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:21,713:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:21,719:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:21,728:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:21,731:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:21,734:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:21,736:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:21,742:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:21,745:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:21,747:INFO:EPOCH - 2 : training on 175141 raw words (106648 effective words) took 0.3s, 345052 effective words/s\n2020-09-11 04:06:21,779:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:21,784:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:21,790:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:21,805:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:21,818:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:21,829:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:21,873:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:21,991:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:21,995:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:21,998:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:22,001:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:22,003:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:22,019:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:22,025:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:22,029:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:22,032:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:22,035:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:22,038:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:22,040:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:22,043:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:22,057:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:22,060:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:22,061:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:22,080:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:22,082:INFO:EPOCH - 3 : training on 175141 raw words (106634 effective words) took 0.3s, 338651 effective words/s\n2020-09-11 04:06:22,116:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:22,129:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:22,135:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:22,159:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:22,166:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:22,170:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:22,186:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:22,311:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:22,322:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:22,328:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:22,334:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:22,338:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:22,346:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:22,364:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:22,366:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:22,369:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:22,373:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:22,375:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:22,377:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:22,385:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:22,390:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:22,395:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:22,398:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:22,417:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:22,418:INFO:EPOCH - 4 : training on 175141 raw words (106547 effective words) took 0.3s, 338606 effective words/s\n2020-09-11 04:06:22,453:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:22,457:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:22,464:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:22,481:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:22,494:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:22,501:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:22,542:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:22,642:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:22,657:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:22,661:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:22,667:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:22,684:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:22,689:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:22,691:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:22,696:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:22,699:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:22,701:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:22,707:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:22,717:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:22,721:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:22,724:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:22,749:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:22,750:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:22,765:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:22,767:INFO:EPOCH - 5 : training on 175141 raw words (106414 effective words) took 0.3s, 323812 effective words/s\n2020-09-11 04:06:22,798:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:22,807:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:22,808:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:22,810:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:22,821:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:22,841:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:22,869:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:22,945:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:23,012:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:23,015:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:23,017:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:23,019:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:23,032:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:23,050:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:23,053:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:23,057:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:23,062:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:23,064:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:23,069:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:23,072:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:23,076:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:23,079:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:23,083:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:23,100:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:23,102:INFO:EPOCH - 6 : training on 175141 raw words (106479 effective words) took 0.3s, 336192 effective words/s\n2020-09-11 04:06:23,135:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:23,139:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:23,142:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:23,167:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:23,175:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:23,185:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:23,199:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:23,342:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:23,350:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:23,352:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:23,353:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:23,362:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:23,365:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:23,374:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:23,375:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:23,384:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:23,390:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:23,392:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:23,395:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:23,400:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:23,403:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:23,406:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:23,410:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:23,423:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:23,425:INFO:EPOCH - 7 : training on 175141 raw words (106634 effective words) took 0.3s, 350969 effective words/s\n2020-09-11 04:06:23,457:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:23,459:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:23,463:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:23,472:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:23,490:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:23,509:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:23,524:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:23,639:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:23,664:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:23,669:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:23,672:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:23,675:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:23,688:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:23,690:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:23,706:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:23,710:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:23,713:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:23,717:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:23,718:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:23,728:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:23,731:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:23,736:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:23,738:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:23,756:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:23,758:INFO:EPOCH - 8 : training on 175141 raw words (106491 effective words) took 0.3s, 336979 effective words/s\n2020-09-11 04:06:23,788:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:23,790:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:23,799:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:23,822:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:23,834:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:23,839:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:23,926:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:23,999:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:24,001:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:24,002:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:24,006:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:24,011:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:24,014:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:24,026:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:24,030:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:24,033:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:24,037:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:24,043:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:24,046:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:24,051:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:24,056:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:24,068:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:24,070:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:24,082:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:24,084:INFO:EPOCH - 9 : training on 175141 raw words (106489 effective words) took 0.3s, 346559 effective words/s\n2020-09-11 04:06:24,114:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:06:24,116:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:06:24,117:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:06:24,128:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:06:24,150:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:06:24,164:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:06:24,279:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:06:24,287:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:06:24,314:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:06:24,321:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:06:24,330:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:06:24,333:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:06:24,338:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:06:24,357:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:06:24,361:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:06:24,368:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:06:24,371:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:06:24,373:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:06:24,376:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:06:24,387:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:06:24,398:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:06:24,403:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:06:24,406:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:06:24,406:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:06:24,407:INFO:EPOCH - 10 : training on 175141 raw words (106926 effective words) took 0.3s, 352365 effective words/s\n2020-09-11 04:06:24,408:INFO:training on a 1751410 raw words (1065584 effective words) took 3.3s, 321442 effective words/s\n2020-09-11 04:06:24,409:WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n2020-09-11 04:06:24,410:INFO:saving Word2Vec object under models/w2v_300_1.txt, separately None\n2020-09-11 04:06:24,411:INFO:not storing attribute vectors_norm\n2020-09-11 04:06:24,412:INFO:not storing attribute cum_table\n2020-09-11 04:06:24,593:INFO:saved models/w2v_300_1.txt\nw2v model done\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<gensim.models.word2vec.Word2Vec at 0x21a42f25c70>"
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": " 04:11:16,814:INFO:EPOCH 8 - PROGRESS: at 5.75% examples, 244475 words/s, in_qsize 47, out_qsize 1\n2020-09-11 04:11:17,822:INFO:EPOCH 8 - PROGRESS: at 7.75% examples, 256251 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:18,836:INFO:EPOCH 8 - PROGRESS: at 10.20% examples, 259623 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:19,898:INFO:EPOCH 8 - PROGRESS: at 12.50% examples, 260836 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:11:20,929:INFO:EPOCH 8 - PROGRESS: at 15.52% examples, 263385 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:21,974:INFO:EPOCH 8 - PROGRESS: at 18.32% examples, 263234 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:22,996:INFO:EPOCH 8 - PROGRESS: at 20.80% examples, 266654 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:24,009:INFO:EPOCH 8 - PROGRESS: at 22.66% examples, 263391 words/s, in_qsize 47, out_qsize 1\n2020-09-11 04:11:25,071:INFO:EPOCH 8 - PROGRESS: at 25.81% examples, 266710 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:26,075:INFO:EPOCH 8 - PROGRESS: at 29.15% examples, 270188 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:27,104:INFO:EPOCH 8 - PROGRESS: at 31.92% examples, 270063 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:28,147:INFO:EPOCH 8 - PROGRESS: at 34.26% examples, 267829 words/s, in_qsize 46, out_qsize 2\n2020-09-11 04:11:29,160:INFO:EPOCH 8 - PROGRESS: at 35.54% examples, 269215 words/s, in_qsize 44, out_qsize 3\n2020-09-11 04:11:30,174:INFO:EPOCH 8 - PROGRESS: at 37.21% examples, 268190 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:31,181:INFO:EPOCH 8 - PROGRESS: at 39.61% examples, 268717 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:32,182:INFO:EPOCH 8 - PROGRESS: at 41.26% examples, 269488 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:11:33,209:INFO:EPOCH 8 - PROGRESS: at 42.40% examples, 267170 words/s, in_qsize 48, out_qsize 1\n2020-09-11 04:11:34,237:INFO:EPOCH 8 - PROGRESS: at 44.66% examples, 268785 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:11:35,243:INFO:EPOCH 8 - PROGRESS: at 46.49% examples, 269063 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:11:36,256:INFO:EPOCH 8 - PROGRESS: at 48.12% examples, 269368 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:11:37,260:INFO:EPOCH 8 - PROGRESS: at 50.64% examples, 270327 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:38,269:INFO:EPOCH 8 - PROGRESS: at 53.07% examples, 270315 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:39,329:INFO:EPOCH 8 - PROGRESS: at 55.70% examples, 270278 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:40,336:INFO:EPOCH 8 - PROGRESS: at 57.92% examples, 271250 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:41,368:INFO:EPOCH 8 - PROGRESS: at 59.70% examples, 270650 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:11:42,375:INFO:EPOCH 8 - PROGRESS: at 61.82% examples, 270926 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:43,386:INFO:EPOCH 8 - PROGRESS: at 64.27% examples, 269111 words/s, in_qsize 48, out_qsize 1\n2020-09-11 04:11:44,435:INFO:EPOCH 8 - PROGRESS: at 69.12% examples, 270939 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:45,625:INFO:EPOCH 8 - PROGRESS: at 72.70% examples, 270355 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:46,627:INFO:EPOCH 8 - PROGRESS: at 76.63% examples, 271441 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:47,672:INFO:EPOCH 8 - PROGRESS: at 79.28% examples, 270550 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:48,710:INFO:EPOCH 8 - PROGRESS: at 82.91% examples, 270835 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:49,724:INFO:EPOCH 8 - PROGRESS: at 86.72% examples, 271036 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:11:50,733:INFO:EPOCH 8 - PROGRESS: at 90.63% examples, 271274 words/s, in_qsize 48, out_qsize 1\n2020-09-11 04:11:51,747:INFO:EPOCH 8 - PROGRESS: at 94.41% examples, 271002 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:52,751:INFO:EPOCH 8 - PROGRESS: at 98.34% examples, 271188 words/s, in_qsize 38, out_qsize 0\n2020-09-11 04:11:53,048:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:11:53,050:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:11:53,053:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:11:53,056:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:11:53,080:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:11:53,115:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:11:53,118:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:11:53,131:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:11:53,142:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:11:53,149:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:11:53,153:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:11:53,156:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:11:53,164:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:11:53,173:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:11:53,181:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:11:53,188:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:11:53,202:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:11:53,205:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:11:53,241:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:11:53,264:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:11:53,267:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:11:53,269:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:11:53,283:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:11:53,285:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:11:53,287:INFO:EPOCH - 8 : training on 21113789 raw words (10772099 effective words) took 39.6s, 271821 effective words/s\n2020-09-11 04:11:54,384:INFO:EPOCH 9 - PROGRESS: at 1.46% examples, 199016 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:11:55,401:INFO:EPOCH 9 - PROGRESS: at 3.15% examples, 238905 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:56,411:INFO:EPOCH 9 - PROGRESS: at 5.67% examples, 248998 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:11:57,414:INFO:EPOCH 9 - PROGRESS: at 7.66% examples, 258458 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:11:58,418:INFO:EPOCH 9 - PROGRESS: at 9.73% examples, 253986 words/s, in_qsize 48, out_qsize 1\n2020-09-11 04:11:59,422:INFO:EPOCH 9 - PROGRESS: at 12.03% examples, 257113 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:00,436:INFO:EPOCH 9 - PROGRESS: at 14.68% examples, 258967 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:01,446:INFO:EPOCH 9 - PROGRESS: at 17.31% examples, 261106 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:02,455:INFO:EPOCH 9 - PROGRESS: at 20.10% examples, 259526 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:03,473:INFO:EPOCH 9 - PROGRESS: at 22.16% examples, 262770 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:04,478:INFO:EPOCH 9 - PROGRESS: at 24.46% examples, 262406 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:05,507:INFO:EPOCH 9 - PROGRESS: at 27.83% examples, 264786 words/s, in_qsize 48, out_qsize 1\n2020-09-11 04:12:06,545:INFO:EPOCH 9 - PROGRESS: at 30.73% examples, 264176 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:07,567:INFO:EPOCH 9 - PROGRESS: at 33.22% examples, 265008 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:08,574:INFO:EPOCH 9 - PROGRESS: at 34.90% examples, 263801 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:09,589:INFO:EPOCH 9 - PROGRESS: at 36.13% examples, 264136 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:10,715:INFO:EPOCH 9 - PROGRESS: at 38.95% examples, 265580 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:12:11,718:INFO:EPOCH 9 - PROGRESS: at 40.62% examples, 264294 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:12,759:INFO:EPOCH 9 - PROGRESS: at 42.11% examples, 266566 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:13,856:INFO:EPOCH 9 - PROGRESS: at 44.34% examples, 266109 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:14,883:INFO:EPOCH 9 - PROGRESS: at 45.99% examples, 265968 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:15,935:INFO:EPOCH 9 - PROGRESS: at 47.61% examples, 265633 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:12:16,984:INFO:EPOCH 9 - PROGRESS: at 49.92% examples, 266185 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:17,987:INFO:EPOCH 9 - PROGRESS: at 52.45% examples, 267386 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:19,010:INFO:EPOCH 9 - PROGRESS: at 55.04% examples, 267032 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:20,017:INFO:EPOCH 9 - PROGRESS: at 57.24% examples, 268004 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:21,060:INFO:EPOCH 9 - PROGRESS: at 59.40% examples, 268710 words/s, in_qsize 45, out_qsize 2\n2020-09-11 04:12:22,061:INFO:EPOCH 9 - PROGRESS: at 61.28% examples, 268620 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:23,119:INFO:EPOCH 9 - PROGRESS: at 64.10% examples, 268510 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:24,167:INFO:EPOCH 9 - PROGRESS: at 68.08% examples, 267839 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:25,177:INFO:EPOCH 9 - PROGRESS: at 71.57% examples, 268994 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:12:26,241:INFO:EPOCH 9 - PROGRESS: at 74.89% examples, 268474 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:27,266:INFO:EPOCH 9 - PROGRESS: at 78.52% examples, 269184 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:28,296:INFO:EPOCH 9 - PROGRESS: at 82.26% examples, 269615 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:29,305:INFO:EPOCH 9 - PROGRESS: at 85.05% examples, 268799 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:30,318:INFO:EPOCH 9 - PROGRESS: at 88.33% examples, 267938 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:31,321:INFO:EPOCH 9 - PROGRESS: at 92.13% examples, 267821 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:32,332:INFO:EPOCH 9 - PROGRESS: at 96.01% examples, 268143 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:12:33,186:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:12:33,204:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:12:33,217:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:12:33,219:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:12:33,220:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:12:33,221:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:12:33,224:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:12:33,226:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:12:33,228:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:12:33,229:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:12:33,230:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:12:33,232:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:12:33,235:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:12:33,253:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:12:33,255:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:12:33,257:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:12:33,262:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:12:33,290:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:12:33,301:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:12:33,305:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:12:33,315:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:12:33,322:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:12:33,341:INFO:EPOCH 9 - PROGRESS: at 99.97% examples, 269363 words/s, in_qsize 1, out_qsize 1\n2020-09-11 04:12:33,343:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:12:33,365:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:12:33,366:INFO:EPOCH - 9 : training on 21113789 raw words (10777233 effective words) took 40.0s, 269316 effective words/s\n2020-09-11 04:12:34,531:INFO:EPOCH 10 - PROGRESS: at 1.47% examples, 206173 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:35,531:INFO:EPOCH 10 - PROGRESS: at 3.10% examples, 235510 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:36,532:INFO:EPOCH 10 - PROGRESS: at 5.53% examples, 247715 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:37,535:INFO:EPOCH 10 - PROGRESS: at 7.35% examples, 251303 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:12:38,543:INFO:EPOCH 10 - PROGRESS: at 9.66% examples, 254656 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:39,543:INFO:EPOCH 10 - PROGRESS: at 11.96% examples, 256875 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:40,549:INFO:EPOCH 10 - PROGRESS: at 14.79% examples, 263572 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:41,549:INFO:EPOCH 10 - PROGRESS: at 17.20% examples, 261697 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:42,567:INFO:EPOCH 10 - PROGRESS: at 19.88% examples, 258106 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:43,568:INFO:EPOCH 10 - PROGRESS: at 21.61% examples, 256798 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:44,583:INFO:EPOCH 10 - PROGRESS: at 23.83% examples, 257588 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:45,607:INFO:EPOCH 10 - PROGRESS: at 27.48% examples, 262275 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:46,608:INFO:EPOCH 10 - PROGRESS: at 29.94% examples, 260619 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:12:47,636:INFO:EPOCH 10 - PROGRESS: at 32.77% examples, 262644 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:12:48,695:INFO:EPOCH 10 - PROGRESS: at 34.84% examples, 263012 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:49,698:INFO:EPOCH 10 - PROGRESS: at 35.91% examples, 263082 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:50,715:INFO:EPOCH 10 - PROGRESS: at 38.64% examples, 265801 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:51,782:INFO:EPOCH 10 - PROGRESS: at 40.25% examples, 262499 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:52,845:INFO:EPOCH 10 - PROGRESS: at 41.82% examples, 262915 words/s, in_qsize 48, out_qsize 0\n2020-09-11 04:12:53,858:INFO:EPOCH 10 - PROGRESS: at 43.33% examples, 261626 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:54,892:INFO:EPOCH 10 - PROGRESS: at 44.93% examples, 261872 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:55,923:INFO:EPOCH 10 - PROGRESS: at 46.83% examples, 263058 words/s, in_qsize 43, out_qsize 4\n2020-09-11 04:12:56,956:INFO:EPOCH 10 - PROGRESS: at 48.79% examples, 264064 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:57,965:INFO:EPOCH 10 - PROGRESS: at 51.86% examples, 266399 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:12:58,985:INFO:EPOCH 10 - PROGRESS: at 54.29% examples, 265858 words/s, in_qsize 44, out_qsize 3\n2020-09-11 04:12:59,989:INFO:EPOCH 10 - PROGRESS: at 56.58% examples, 266266 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:00,992:INFO:EPOCH 10 - PROGRESS: at 58.28% examples, 265293 words/s, in_qsize 48, out_qsize 1\n2020-09-11 04:13:01,994:INFO:EPOCH 10 - PROGRESS: at 60.12% examples, 264909 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:03,041:INFO:EPOCH 10 - PROGRESS: at 61.87% examples, 263789 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:04,051:INFO:EPOCH 10 - PROGRESS: at 64.91% examples, 263504 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:05,062:INFO:EPOCH 10 - PROGRESS: at 68.19% examples, 262028 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:06,103:INFO:EPOCH 10 - PROGRESS: at 71.41% examples, 262171 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:07,169:INFO:EPOCH 10 - PROGRESS: at 74.71% examples, 262085 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:08,228:INFO:EPOCH 10 - PROGRESS: at 78.61% examples, 263184 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:09,256:INFO:EPOCH 10 - PROGRESS: at 82.51% examples, 264343 words/s, in_qsize 46, out_qsize 1\n2020-09-11 04:13:10,290:INFO:EPOCH 10 - PROGRESS: at 85.39% examples, 263412 words/s, in_qsize 48, out_qsize 1\n2020-09-11 04:13:11,290:INFO:EPOCH 10 - PROGRESS: at 88.98% examples, 263610 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:12,296:INFO:EPOCH 10 - PROGRESS: at 92.70% examples, 263303 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:13,302:INFO:EPOCH 10 - PROGRESS: at 95.98% examples, 262476 words/s, in_qsize 47, out_qsize 0\n2020-09-11 04:13:14,094:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 04:13:14,100:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 04:13:14,103:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 04:13:14,134:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 04:13:14,157:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 04:13:14,170:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 04:13:14,192:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 04:13:14,216:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 04:13:14,222:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 04:13:14,228:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 04:13:14,230:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 04:13:14,247:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 04:13:14,250:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 04:13:14,268:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 04:13:14,270:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 04:13:14,275:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 04:13:14,287:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 04:13:14,289:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 04:13:14,290:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 04:13:14,292:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 04:13:14,305:INFO:EPOCH 10 - PROGRESS: at 99.89% examples, 263766 words/s, in_qsize 3, out_qsize 1\n2020-09-11 04:13:14,307:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 04:13:14,309:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 04:13:14,369:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 04:13:14,373:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 04:13:14,375:INFO:EPOCH - 10 : training on 21113789 raw words (10775049 effective words) took 40.9s, 263677 effective words/s\n2020-09-11 04:13:14,377:INFO:training on a 211137890 raw words (107752024 effective words) took 395.1s, 272720 effective words/s\n2020-09-11 04:13:14,378:INFO:saving Word2Vec object under models/w2v_300_3.txt, separately None\n2020-09-11 04:13:14,380:INFO:storing np array 'vectors' to models/w2v_300_3.txt.wv.vectors.npy\n2020-09-11 04:13:14,597:INFO:not storing attribute vectors_norm\n2020-09-11 04:13:14,598:INFO:storing np array 'syn1neg' to models/w2v_300_3.txt.trainables.syn1neg.npy\n2020-09-11 04:13:14,802:INFO:not storing attribute cum_table\n2020-09-11 04:13:14,950:INFO:saved models/w2v_300_3.txt\nw2v model done\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<gensim.models.word2vec.Word2Vec at 0x21a3da85e50>"
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 18
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-11 04:13:16,007:INFO:loading Word2Vec object from models/w2v_300_1.txt\n2020-09-11 04:13:16,195:INFO:loading wv recursively from models/w2v_300_1.txt.wv.* with mmap=None\n2020-09-11 04:13:16,197:INFO:setting ignored attribute vectors_norm to None\n2020-09-11 04:13:16,198:INFO:loading vocabulary recursively from models/w2v_300_1.txt.vocabulary.* with mmap=None\n2020-09-11 04:13:16,199:INFO:loading trainables recursively from models/w2v_300_1.txt.trainables.* with mmap=None\n2020-09-11 04:13:16,200:INFO:setting ignored attribute cum_table to None\n2020-09-11 04:13:16,202:INFO:loaded models/w2v_300_1.txt\n100%|██████████| 7189/7189 [00:00<00:00, 53306.27it/s]\n2020-09-11 04:13:16,377:INFO:loading Word2Vec object from models/w2v_300_3.txt\n2020-09-11 04:13:16,499:INFO:loading wv recursively from models/w2v_300_3.txt.wv.* with mmap=None\n2020-09-11 04:13:16,500:INFO:loading vectors from models/w2v_300_3.txt.wv.vectors.npy with mmap=None\n2020-09-11 04:13:16,543:INFO:setting ignored attribute vectors_norm to None\nnull cnt 5\n2020-09-11 04:13:16,546:INFO:loading vocabulary recursively from models/w2v_300_3.txt.vocabulary.* with mmap=None\n2020-09-11 04:13:16,548:INFO:loading trainables recursively from models/w2v_300_3.txt.trainables.* with mmap=None\n2020-09-11 04:13:16,549:INFO:loading syn1neg from models/w2v_300_3.txt.trainables.syn1neg.npy with mmap=None\n2020-09-11 04:13:16,592:INFO:setting ignored attribute cum_table to None\n2020-09-11 04:13:16,594:INFO:loaded models/w2v_300_3.txt\n100%|██████████| 28458/28458 [00:00<00:00, 72116.05it/s]null cnt 1202\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "31"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "text_1_list = np.unique(train_data['text_1'])\n",
    "text_3_list = np.unique(train_data['text_2'])\n",
    "\n",
    "print('开始序列化')\n",
    "x1, index_1, token_1 = set_tokenizer(train_data['text_1'], split_char=' ', max_len=30)\n",
    "x3, index_3, token_3 = set_tokenizer(train_data['text_2'], split_char=' ', max_len=600)\n",
    "print('序列化完成')\n",
    "gc.collect()\n",
    "\n",
    "trian_save_word2vec(text_1_list, save_name='models/w2v_300_1.txt', split_char=' ')\n",
    "gc.collect()\n",
    "trian_save_word2vec(text_3_list, save_name='models/w2v_300_3.txt', split_char=' ')\n",
    "gc.collect()\n",
    "\n",
    "# 得到emb矩阵\n",
    "emb1 = get_embedding_matrix(index_1, Emed_path='models/w2v_300_1.txt')\n",
    "emb3 = get_embedding_matrix(index_3, Emed_path='models/w2v_300_3.txt')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建抽取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "from keras.initializers import *\n",
    "\n",
    "def model_conv(emb1, emb3):\n",
    "    '''\n",
    "    注意这个inputs\n",
    "    seq1、seq2分别是两个输入\n",
    "    是否做emb可选可不选，\n",
    "    这个就是我们之前训练已经得到的用于embedding的（embedding_matrix1， embedding_matrix2）\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=30,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=600,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    seq1 = Input(shape=(30,))\n",
    "    seq3 = Input(shape=(600,))    \n",
    "    \n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    \n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x3 = sdrop(x3)\n",
    "     \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x1))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x3))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    \n",
    "    x = Multiply()([merged_1, merged_3])\n",
    "    \n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(x))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    pred_1 = Dense(10, activation='softmax')(x)\n",
    "    pred_2 = Dense(3, activation='softmax')(x)\n",
    "    pred_3 = Dense(3, activation='softmax')(x)\n",
    "    pred_4 = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=[seq1, seq3], outputs=[pred_1, pred_2, pred_3, pred_4])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0001),metrics=[\"accuracy\"])\n",
    "    return model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 30)]         0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 600)]        0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 30, 300)      2157000     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 600, 300)     8537700     input_2[0][0]                    \n__________________________________________________________________________________________________\nspatial_dropout1d (SpatialDropo multiple             0           embedding[0][0]                  \n                                                                 embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 30, 256)      330240      spatial_dropout1d[0][0]          \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 600, 256)     330240      spatial_dropout1d[1][0]          \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 30, 256)      0           bidirectional[0][0]              \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 600, 256)     0           bidirectional_1[0][0]            \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, 30, 100)      25700       dropout[0][0]                    \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 600, 100)     25700       dropout_1[0][0]                  \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 100)          0           time_distributed[0][0]           \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 100)          0           time_distributed_1[0][0]         \n__________________________________________________________________________________________________\nmultiply (Multiply)             (None, 100)          0           lambda[0][0]                     \n                                                                 lambda_1[0][0]                   \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1000)         101000      multiply[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 1000)         4000        dense_2[0][0]                    \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 1000)         0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 1000)         0           activation[0][0]                 \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 500)          500500      dropout_2[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 500)          2000        dense_3[0][0]                    \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 500)          0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 10)           5010        activation_1[0][0]               \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 3)            1503        activation_1[0][0]               \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 3)            1503        activation_1[0][0]               \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 2)            1002        activation_1[0][0]               \n==================================================================================================\nTotal params: 12,023,098\nTrainable params: 1,325,398\nNon-trainable params: 10,697,700\n__________________________________________________________________________________________________\nEpoch 1/8\n107/107 [==============================] - 1876s 18s/step - loss: 2.7605 - dense_4_loss: 0.7574 - dense_5_loss: 0.7978 - dense_6_loss: 0.9498 - dense_7_loss: 0.2555 - dense_4_accuracy: 0.8017 - dense_5_accuracy: 0.6351 - dense_6_accuracy: 0.5759 - dense_7_accuracy: 0.9183\nEpoch 2/8\n107/107 [==============================] - 2606s 24s/step - loss: 1.3951 - dense_4_loss: 0.1619 - dense_5_loss: 0.6551 - dense_6_loss: 0.5328 - dense_7_loss: 0.0453 - dense_4_accuracy: 0.9656 - dense_5_accuracy: 0.6889 - dense_6_accuracy: 0.7868 - dense_7_accuracy: 0.9986\nEpoch 3/8\n107/107 [==============================] - 2671s 25s/step - loss: 1.1727 - dense_4_loss: 0.1237 - dense_5_loss: 0.5820 - dense_6_loss: 0.4424 - dense_7_loss: 0.0246 - dense_4_accuracy: 0.9718 - dense_5_accuracy: 0.7271 - dense_6_accuracy: 0.8111 - dense_7_accuracy: 0.9986\nEpoch 4/8\n107/107 [==============================] - 2690s 25s/step - loss: 1.0310 - dense_4_loss: 0.1028 - dense_5_loss: 0.5136 - dense_6_loss: 0.3968 - dense_7_loss: 0.0178 - dense_4_accuracy: 0.9760 - dense_5_accuracy: 0.7685 - dense_6_accuracy: 0.8259 - dense_7_accuracy: 0.9986\nEpoch 5/8\n107/107 [==============================] - 2697s 25s/step - loss: 0.9312 - dense_4_loss: 0.0894 - dense_5_loss: 0.4584 - dense_6_loss: 0.3685 - dense_7_loss: 0.0148 - dense_4_accuracy: 0.9780 - dense_5_accuracy: 0.8018 - dense_6_accuracy: 0.8397 - dense_7_accuracy: 0.9986\nEpoch 6/8\n107/107 [==============================] - 2705s 25s/step - loss: 0.8400 - dense_4_loss: 0.0815 - dense_5_loss: 0.4054 - dense_6_loss: 0.3403 - dense_7_loss: 0.0128 - dense_4_accuracy: 0.9799 - dense_5_accuracy: 0.8307 - dense_6_accuracy: 0.8520 - dense_7_accuracy: 0.9986\nEpoch 7/8\n107/107 [==============================] - 2712s 25s/step - loss: 0.7805 - dense_4_loss: 0.0724 - dense_5_loss: 0.3718 - dense_6_loss: 0.3248 - dense_7_loss: 0.0115 - dense_4_accuracy: 0.9816 - dense_5_accuracy: 0.8479 - dense_6_accuracy: 0.8612 - dense_7_accuracy: 0.9986\nEpoch 8/8\n107/107 [==============================] - 2731s 26s/step - loss: 0.7300 - dense_4_loss: 0.0672 - dense_5_loss: 0.3447 - dense_6_loss: 0.3076 - dense_7_loss: 0.0106 - dense_4_accuracy: 0.9831 - dense_5_accuracy: 0.8576 - dense_6_accuracy: 0.8679 - dense_7_accuracy: 0.9986\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21a6d959a60>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model = model_conv(emb1, emb3)\n",
    "model.summary()\n",
    "l1 = to_categorical(train_data['label_1'], 10)\n",
    "l2 = to_categorical(train_data['label_2'], 3)\n",
    "l3 = to_categorical(train_data['label_3'], 3)\n",
    "l4 = to_categorical(train_data['label_4'], 2)\n",
    "model.fit([x1, x3],[l1, l2, l3, l4], batch_size=256, epochs=8, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存权重\n",
    "model.save_weights('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-26ce19f84820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 预测验证集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mval_result_for_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财产品名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品发行方名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m ) -> \"DataFrame\":\n\u001b[1;32m---> 73\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# to avoid incompat dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1144\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                 ):\n\u001b[1;32m-> 1146\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[1;31m# datetimelikes must match exactly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "# 预测验证集\n",
    "val_df[\"sample_id\"]=val_df[\"sample_id\"].astype(str)\n",
    "val_result_for_pred = pd.merge(val_result, val_df, on='sample_id', how='left')\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['理财产品名称'].astype(str) + '_' + val_result_for_pred['产品发行方名称'].astype(str)\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text'].astype(str)\n",
    "\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "\n",
    "x1 = token_1.texts_to_sequences(val_result_for_pred['text_1'])\n",
    "x1 = pad_sequences(x1, maxlen=30, value=0)\n",
    "x3 = token_3.texts_to_sequences(val_result_for_pred['text_2'])\n",
    "x3 = pad_sequences(x3, maxlen=600, value=0)\n",
    "pred_result = model.predict([x1, x3], batch_size=1024, verbose=1)\n",
    "pred_1 = label_1.inverse_transform(np.argmax(pred_result[0], axis=1))\n",
    "pred_2 = label_2.inverse_transform(np.argmax(pred_result[1], axis=1))\n",
    "pred_3 = label_3.inverse_transform(np.argmax(pred_result[2], axis=1))\n",
    "pred_4 = label_4.inverse_transform(np.argmax(pred_result[3], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pred_1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-33bb5074b888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财类型'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'资金来源'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司和上市公司关系'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'买卖方是否有关联关系'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_1' is not defined"
     ]
    }
   ],
   "source": [
    "val_result['理财类型'] = pred_1\n",
    "val_result['资金来源'] = pred_2\n",
    "val_result['实际购买公司和上市公司关系'] = pred_3\n",
    "val_result['买卖方是否有关联关系'] = pred_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.离线验证评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'认购日期'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '认购日期'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9836af1d8ef2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'认购日期'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财产品名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财类型'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'认购金额(万元)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品起息日'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品到息日'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品期限'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'资金来源'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司和上市公司关系'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'买卖方是否有关联关系'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'公告日期'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_F1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '认购日期'"
     ]
    }
   ],
   "source": [
    "# 计算线下f1\n",
    "# R（召回率）=抽取正确的记录数量/（抽取正确的目标数量+漏抽取的记录数量）\n",
    "# P（准确率）=抽取正确的记录数量/（抽取错误的记录数量+抽取正确的记录数量）\n",
    "def get_F1(val_pred, val_true):\n",
    "    val_pred = list(val_pred)\n",
    "    val_true = list(val_true)\n",
    "    curr = list(set(val_pred).intersection(set(val_true)))\n",
    "    R = len(curr)/len(val_true)\n",
    "    P = len(curr)/len(val_pred)\n",
    "    return 2*P*R/(P+R)\n",
    "\n",
    "train_outputs[\"sample_id\"]=train_outputs[\"sample_id\"].astype(str)\n",
    "r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "val_true = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['理财类型'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) + r['资金来源'].astype(str) + r['实际购买公司名称'].astype(str) + r['实际购买公司和上市公司关系'].astype(str) + r['买卖方是否有关联关系'].astype(str) + r['公告日期'].astype(str)\n",
    "\n",
    "r = val_result\n",
    "val_pred = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['理财类型'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) + r['资金来源'].astype(str) + r['实际购买公司名称'].astype(str) + r['实际购买公司和上市公司关系'].astype(str) + r['买卖方是否有关联关系'].astype(str) + r['公告日期'].astype(str)\n",
    "\n",
    "score = get_F1(val_pred, val_true)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-24e817b98e00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_pred_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_true_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_true_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m ) -> \"DataFrame\":\n\u001b[1;32m---> 73\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# to avoid incompat dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1144\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                 ):\n\u001b[1;32m-> 1146\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[1;31m# datetimelikes must match exactly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "val_true_file=pl.Path(\"results/val_true.csv\")\n",
    "val_pred_file=pl.Path(\"results/val_pred.csv\")\n",
    "\n",
    "tr_true_file=pl.Path(\"results/tr_true.csv\")\n",
    "val_result.copy().sort_values(by=\"sample_id\").sort_index(axis=1).to_csv(val_pred_file,index=None)\n",
    "pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left').sort_index(axis=1).sort_values(by=\"sample_id\").to_csv(val_true_file,index=None)\n",
    "pd.merge(train_df[['sample_id']], train_outputs, on='sample_id', how='left').sort_index(axis=1).sort_values(by=\"sample_id\").to_csv(tr_true_file,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.最终输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_result' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2e1b40a56c0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 预测测试集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_result_for_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财产品名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品发行方名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_result' is not defined"
     ]
    }
   ],
   "source": [
    "# 预测测试集\n",
    "test_df[\"sample_id\"]=test_df[\"sample_id\"].astype(str)\n",
    "test_result_for_pred = pd.merge(test_result, test_df, on='sample_id', how='left')\n",
    "test_result_for_pred['text_1'] = test_result_for_pred['理财产品名称'].astype(str) + '_' + test_result_for_pred['产品发行方名称'].astype(str)\n",
    "test_result_for_pred['text_2'] = test_result_for_pred['text'].astype(str)\n",
    "\n",
    "test_result_for_pred['text_1'] = test_result_for_pred['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "test_result_for_pred['text_2'] = test_result_for_pred['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "\n",
    "x1 = token_1.texts_to_sequences(test_result_for_pred['text_1'])\n",
    "x1 = pad_sequences(x1, maxlen=30, value=0)\n",
    "x3 = token_3.texts_to_sequences(test_result_for_pred['text_2'])\n",
    "x3 = pad_sequences(x3, maxlen=600, value=0)\n",
    "pred_result = model.predict([x1, x3], batch_size=1024, verbose=1)\n",
    "pred_1 = label_1.inverse_transform(np.argmax(pred_result[0], axis=1))\n",
    "pred_2 = label_2.inverse_transform(np.argmax(pred_result[1], axis=1))\n",
    "pred_3 = label_3.inverse_transform(np.argmax(pred_result[2], axis=1))\n",
    "pred_4 = label_4.inverse_transform(np.argmax(pred_result[3], axis=1))\n",
    "\n",
    "test_result['理财类型'] = pred_1\n",
    "test_result['资金来源'] = pred_2\n",
    "test_result['实际购买公司和上市公司关系'] = pred_3\n",
    "test_result['买卖方是否有关联关系'] = pred_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_result' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8d62d995a84d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results/re_lstm_base.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results/re_lstm_base.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gb18030\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_result' is not defined"
     ]
    }
   ],
   "source": [
    "test_result.to_csv('results/re_lstm_base.csv',encoding=\"utf8\", index=False)\n",
    "test_result.to_excel('results/re_lstm_base.xlsx',encoding=\"gb18030\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "files_num=int(test_df.shape[0]/4)\n",
    "test_time[\"sample_id\"]=test_time[\"sample_id\"].astype(str)\n",
    "\n",
    "# test_time.shape\n",
    "# test_time.dropna(subset=[\"公告日期\"]).shape\n",
    "import random\n",
    "for i in range(1,20):\n",
    "    random_seed=random.randint(0,10000)\n",
    "    random_test_df = test_df.sample(frac=1, random_state=random_seed)\n",
    "    change_df=random_test_df[:files_num]\n",
    "    stable_df=random_test_df[files_num:]\n",
    "    random_test_time=pd.merge(change_df,test_time,on=[\"sample_id\"])\n",
    "    random_test_time.shape\n",
    "    random_test_time=random_test_time.dropna(subset=[\"公告日期\"])\n",
    "    random_test_time.shape\n",
    "    random_test_time[\"公告日期\"]=random_test_time[\"公告日期\"].map(lambda x:datetime.datetime.strftime(datetime.datetime.strptime(str(x.replace(\"\\r\",\"\")), '%Y-%m-%d'),'%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-4c1b78c9b79b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4663\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# sample_id=7123\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0msample_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tabel\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtable_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_rows_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirst_line_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproduct_df_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow_combine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2138\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2061\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "sample_id=4663\n",
    "# sample_id=7123\n",
    "tabel=val_df[val_df[\"sample_id\"]==sample_id][\"tabel\"].iloc[0]\n",
    "table_result,start_rows_list,first_line_list,product_df_list=row_combine(sample_id,tabel)\n",
    "index=0\n",
    "table_result[index]\n",
    "product_df_list_ele=product_df_list[index].reset_index(drop=True)\n",
    "product_df_list_ele\n",
    "# product_df_list_ele\n",
    "each_sum_rows=get_each_product_row(table_result[index],product_df_list_ele.reset_index(drop=True))\n",
    "\n",
    "each_sum_rows2=result_matrix[result_matrix[\"sample_id\"]==sample_id][\"product_df\"].iloc[0]\n",
    "# # each_sum_rows\n",
    "each_sum_rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}