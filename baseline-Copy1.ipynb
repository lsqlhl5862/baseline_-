{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from datetime import datetime \n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from tqdm.autonotebook import *\n",
    "import pdfplumber\n",
    "tqdm.pandas()\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "def extract_pdf_content(pdf_path):\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index_page in np.arange(0, len(pdf.pages), 1):\n",
    "            # 读取多页\n",
    "            page = pdf.pages[index_page]   # 第n页的信息\n",
    "            text = page.extract_text()\n",
    "            text_list.append(text)\n",
    "            table = page.extract_tables()\n",
    "            for t in table:\n",
    "                table_list.append(t)\n",
    "    return text_list, table_list\n",
    "\n",
    "# a='train_output'\n",
    "# b=\"train\"+r\"_\\d\"\n",
    "# print(re.match(b,a))\n",
    "def csv_2_df(file_path):\n",
    "    file_list=file_path.parents[0].glob(\"*.csv\")\n",
    "    result=None\n",
    "    for item in file_list:\n",
    "        if re.match(file_path.stem.split(\"_\")[0]+r\"_\\d\",item.stem) is not None:\n",
    "            print(item.name)\n",
    "            result=pd.read_csv(item) if result is None else pd.concat([result,pd.read_csv(item)])\n",
    "    return result\n",
    "\n",
    "test_path = pl.Path('datasets/train_1.csv')\n",
    "\n",
    "test_df = csv_2_df(test_path)\n",
    "test_df.to_csv(\"datasets/train.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF解析原始数据 \n",
    "## 加载数据并采用pdfplumber抽取PDF中的文字和表格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id       认购日期        理财产品名称     产品发行方名称    理财类型  认购金额(万元)  \\\n0          1 2019-03-27         汇聚金1号  中融国际信托有限公司      信托   10000.0   \n1          1 2019-03-27  招商银行步步生金8699        招商银行  银行理财产品     200.0   \n\n       产品起息日      产品到息日  产品期限  资金来源    实际购买公司名称 实际购买公司和上市公司关系 买卖方是否有关联关系  \\\n0 2019-03-27 2019-09-23  180天  自有资金  恒生电子股份有限公司          公司本身          否   \n1 2019-03-27        NaT   NaN  自有资金  恒生电子股份有限公司          公司本身          否   \n\n        公告日期  \n0 2019-04-25  \n1 2019-04-25  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>认购日期</th>\n      <th>理财产品名称</th>\n      <th>产品发行方名称</th>\n      <th>理财类型</th>\n      <th>认购金额(万元)</th>\n      <th>产品起息日</th>\n      <th>产品到息日</th>\n      <th>产品期限</th>\n      <th>资金来源</th>\n      <th>实际购买公司名称</th>\n      <th>实际购买公司和上市公司关系</th>\n      <th>买卖方是否有关联关系</th>\n      <th>公告日期</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2019-03-27</td>\n      <td>汇聚金1号</td>\n      <td>中融国际信托有限公司</td>\n      <td>信托</td>\n      <td>10000.0</td>\n      <td>2019-03-27</td>\n      <td>2019-09-23</td>\n      <td>180天</td>\n      <td>自有资金</td>\n      <td>恒生电子股份有限公司</td>\n      <td>公司本身</td>\n      <td>否</td>\n      <td>2019-04-25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2019-03-27</td>\n      <td>招商银行步步生金8699</td>\n      <td>招商银行</td>\n      <td>银行理财产品</td>\n      <td>200.0</td>\n      <td>2019-03-27</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>自有资金</td>\n      <td>恒生电子股份有限公司</td>\n      <td>公司本身</td>\n      <td>否</td>\n      <td>2019-04-25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id                  file_path  \\\n0          1  datasets/train_data/1.PDF   \n1          2  datasets/train_data/2.PDF   \n\n                                                text  \\\n0  ['                                            ...   \n1  ['                                            ...   \n\n                                               tabel  \n0  [[['', None, None, '', None, None, '', None, N...  \n1  [[['', None, None, '', None, None, '', None, N...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>file_path</th>\n      <th>text</th>\n      <th>tabel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>datasets/train_data/1.PDF</td>\n      <td>['                                            ...</td>\n      <td>[[['', None, None, '', None, None, '', None, N...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>datasets/train_data/2.PDF</td>\n      <td>['                                            ...</td>\n      <td>[[['', None, None, '', None, None, '', None, N...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id                     file_path  \\\n0      11188  datasets/test_data/11188.PDF   \n1      11189  datasets/test_data/11189.PDF   \n\n                                                text tabel  \n0  ['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...    []  \n1  ['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...    []  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>file_path</th>\n      <th>text</th>\n      <th>tabel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11188</td>\n      <td>datasets/test_data/11188.PDF</td>\n      <td>['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11189</td>\n      <td>datasets/test_data/11189.PDF</td>\n      <td>['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# 数据准备(train_output文件中格式有点问题，需要提前用excel或者wps打开然后另存为excel文件)\n",
    "train_outputs = pd.read_excel('datasets/train_output.xlsx')\n",
    "\n",
    "# 获取pdf中文字和表格\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index_page in np.arange(0, len(pdf.pages), 1):\n",
    "            # 读取多页\n",
    "            page = pdf.pages[index_page]   # 第n页的信息\n",
    "            text = page.extract_text()\n",
    "            text_list.append(text)\n",
    "            table = page.extract_tables()\n",
    "            for t in table:\n",
    "                table_list.append(t)\n",
    "    return text_list, table_list\n",
    "\n",
    "def get_dir_file(path):\n",
    "    '''\n",
    "    输入文件夹位置，输出整理好的dataframe\n",
    "    '''\n",
    "    path_list = os.listdir(path)\n",
    "    id_list = []\n",
    "    file_path_list = []\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    for i in tqdm(path_list):\n",
    "        if '.PDF' in i:\n",
    "            file_path = path + i\n",
    "            id_list.append(int(i.split('.')[0]))\n",
    "            file_path_list.append(file_path)\n",
    "            try:\n",
    "                text_temp, table_temp = extract_pdf_content(file_path)\n",
    "            except Exception:\n",
    "                print('此pdf无法读取')\n",
    "                text_temp, table_temp = [], []\n",
    "            text_list.append(text_temp)\n",
    "            table_list.append(table_temp)\n",
    "            \n",
    "    df = pd.DataFrame()\n",
    "    df['sample_id'] = id_list\n",
    "    df['file_path'] = file_path_list\n",
    "    df['text'] = text_list\n",
    "    df['tabel'] = table_list\n",
    "    df = df.sort_values('sample_id')\n",
    "    return df\n",
    "\n",
    "# 文件处理太慢，可持续化保存文件\n",
    "train_path = 'datasets/train.csv'\n",
    "if os.path.exists(train_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "else:\n",
    "    train_df = get_dir_file('datasets/train_data/')\n",
    "    train_df.to_csv(train_path,index=False)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "test_path =  'datasets/test.csv'\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "else:\n",
    "    test_df = get_dir_file('datasets/test_data/')\n",
    "    test_df.to_csv(test_path,index=False)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_outputs.head(2)\n",
    "train_df.head(2)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集验证集\n",
    "train_df = train_df.sample(frac=1, random_state=1017)\n",
    "val_df = train_df[:1800]\n",
    "train_df = train_df[1800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "## 抽取整体数据（一个sampleid内此字段内容都相同）\n",
    "## 公告时间，实际购买公司"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.抽取公告时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1800/1800 [00:00<00:00, 4142.13it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.4583333333333333"
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1800/1800 [00:00<00:00, 4161.26it/s]\n100%|██████████| 8660/8660 [00:01<00:00, 5407.87it/s]\n"
    }
   ],
   "source": [
    "# 首先针对任务抽取时间（每个时间跟每个id是一一对应的）\n",
    "# 要不是取第一个时间，要不就是取最后一个时间（或者时间加一）这里可以建立一个模型预测\n",
    "# base这里面直接取最后一个时间作为发布日期\n",
    "\n",
    "CN_NUM = {\n",
    "    u'〇': 0, u'一': 1, u'二': 2, u'三': 3,\n",
    "    u'四': 4, u'五': 5, u'六': 6, u'七': 7,\n",
    "    u'八': 8, u'九': 9, u'零': 0, u'壹': 1,\n",
    "    u'贰': 2, u'叁': 3, u'肆': 4, u'伍': 5,\n",
    "    u'陆': 6, u'柒': 7, u'捌': 8, u'玖': 9,\n",
    "    u'貮': 2, u'两': 2,\n",
    "}\n",
    "\n",
    "\n",
    "def get_put_time_from_text(row):\n",
    "    row = row.replace(' ', '').replace('\\\\n', '')\n",
    "    for key in CN_NUM:\n",
    "        row = row.replace(key, str(CN_NUM[key]))   \n",
    "    r = row.replace(\"年\", \"-\").replace(\"月\", \"-\").replace(\"日\", \" \").replace(\"/\", \"-\").strip()\n",
    "    regex = \"(\\d{4}-\\d{1,2}-\\d{1,2})\"\n",
    "    r = re.findall(regex, r)\n",
    "    if len(r)==0:\n",
    "        return np.nan\n",
    "    time_str = r[-1]\n",
    "    first = time_str.split('-')[0]\n",
    "    second = time_str.split('-')[1]\n",
    "    last = time_str.split('-')[-1]\n",
    "    second = str.zfill(second, 2)\n",
    "    last = str.zfill(last, 2)\n",
    "    r = '-'.join([first, second, last])\n",
    "    return r\n",
    "\n",
    "val_result = pd.DataFrame()\n",
    "val_result['sample_id'] = val_df['sample_id']\n",
    "val_result['predict_time'] = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "test_gg = train_outputs.groupby('sample_id').apply(lambda row:list(row['公告日期'])[0]).reset_index()\n",
    "test_gg.columns = ['sample_id', 'time']\n",
    "val_result = pd.merge(val_result, test_gg, on='sample_id', how='left')\n",
    "\n",
    "# 判断验证集的准确率\n",
    "np.sum(val_result['predict_time'].astype(str) == val_result['time'].astype(str))/len(val_result)\n",
    "\n",
    "val_time = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "test_time = test_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.抽取实际购买公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1800/1800 [00:00<00:00, 3367.90it/s]\n100%|██████████| 8660/8660 [00:01<00:00, 5263.39it/s]\n"
    }
   ],
   "source": [
    "# 抽取购买公司\n",
    "# 前几句话出现\n",
    "# 将其按照\\\\n 和空格切割\n",
    "def get_gm(row):\n",
    "    result = re.split('[\\\\\\\\n ]',row)\n",
    "    for i in result:\n",
    "        if '公司' in i:\n",
    "            return i\n",
    "\n",
    "val_gm = val_df.progress_apply(lambda row:get_gm(row['text']), axis=1)\n",
    "test_gm = test_df.progress_apply(lambda row:get_gm(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.清洗提取出来的tabel数据，主要是清洗掉有问题的列 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 7217/7217 [00:10<00:00, 701.46it/s]\n100%|██████████| 1800/1800 [00:02<00:00, 781.03it/s]\n100%|██████████| 8660/8660 [00:04<00:00, 2132.03it/s]\n"
    }
   ],
   "source": [
    "# 将table转换格式以及处理\n",
    "def deal_tabel(row):\n",
    "    row = eval(row)\n",
    "    if len(row)==0:\n",
    "        return []\n",
    "    else:\n",
    "        new_row = []\n",
    "        for i in row:\n",
    "            for d in i:\n",
    "                new_temp = []\n",
    "                for h in d:\n",
    "                    # 这里处理空数据或者错误的数据\n",
    "                    h = str(h).replace('None', '').replace('\\n','').replace(' ', '')                    \n",
    "                    if h=='':\n",
    "                        continue\n",
    "                    if h=='.':\n",
    "                        continue\n",
    "                    if h=='/':\n",
    "                        continue\n",
    "                    new_temp.append(h)\n",
    "                new_row.append(new_temp)\n",
    "        # 这里判断是否构成一个完整得认购数据(通过一个list进行判断)\n",
    "        new_new_row = []\n",
    "        for i in new_row:\n",
    "            if len(i) == 0:\n",
    "                continue\n",
    "            elif len(i) <= 4:\n",
    "                continue\n",
    "            else:\n",
    "                new_new_row.append(i)\n",
    "        return new_new_row\n",
    "train_df_tabel = train_df['tabel'].progress_apply(lambda row:deal_tabel(row))\n",
    "val_df_tabel = val_df['tabel'].progress_apply(lambda row:deal_tabel(row))\n",
    "test_df_tabel = test_df['tabel'].progress_apply(lambda row:deal_tabel(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.抽取的是单独的数据包含\n",
    "#### 起息日，到息日， 金额，认购日期，产品发行方，理财产品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 1800/1800 [08:23<00:00,  3.57it/s]\n100%|██████████| 8660/8660 [16:57<00:00,  8.51it/s]\n"
    }
   ],
   "source": [
    "# 直接提取时间\n",
    "# 如果出现两个时间第一个就是起息日，第二个就是到期日\n",
    "# 如果出现一个时间就是起息日\n",
    "# 出现的第一个money就是最后的金额\n",
    "# 从这里面抽取所有序列\n",
    "# 这里认为有逗号出现的就是money\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "def get_list_data(df):\n",
    "    df = list(df)\n",
    "    new_df = []\n",
    "    for i in tqdm(df):\n",
    "        temp_df = []\n",
    "        for h in i:\n",
    "            new_h = []\n",
    "            for digital in h:\n",
    "                if ',' in digital:\n",
    "                    # 这里也是为了统一数据有些是用元，有些是用万元\n",
    "                    try:\n",
    "                        ttt = float(digital.replace(',', '').replace('万元', '').replace('人民币', '').replace('元', ''))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if ttt > 20000:\n",
    "                        ttt = ttt/10000\n",
    "                    new_h.append(ttt)\n",
    "                else:\n",
    "                    continue\n",
    "            if len(new_h) == 0:\n",
    "                continue\n",
    "            temp_single = {}\n",
    "            a = '_'.join(h)\n",
    "            # 抽取时间和money\n",
    "            t = TimeFinder()\n",
    "            time_all = t.find_time(a)\n",
    "            if time_all == None:\n",
    "                continue\n",
    "            rgrq = time_all[0]\n",
    "            cpqxr = time_all[0]\n",
    "            if len(time_all) > 1:\n",
    "                try:\n",
    "                    cpdxr = time_all[1]\n",
    "                    # 相减\n",
    "                    d1 = datetime.datetime.strptime(cpqxr, '%Y-%m-%d')\n",
    "                    d2 = datetime.datetime.strptime(cpdxr, '%Y-%m-%d')\n",
    "                    d = d2 - d1\n",
    "                    cpqx = str(d.days) + '天'\n",
    "                except Exception:\n",
    "                    cpdxr = np.nan\n",
    "                    cpqx = np.nan\n",
    "            else:\n",
    "                cpdxr = np.nan\n",
    "                cpqx = np.nan\n",
    "                \n",
    "            # 筛选出除开数字与包含时间的列\n",
    "            # 末尾是\n",
    "            last_two = ['公司', '银行', '信托', '证券',  '分行', '支行', '中心', '业部', '商行', '建行']\n",
    "            mowei = np.nan\n",
    "            selected_bank_and_works = []\n",
    "            for l in h:\n",
    "                new_l = list(str(l))\n",
    "                new_l_test = ''.join(l[-2:])\n",
    "                if new_l_test in last_two:\n",
    "                    mowei = l\n",
    "                    continue\n",
    "                if '资金' in l:\n",
    "                    continue\n",
    "                if '收益' in l:\n",
    "                    continue\n",
    "                if '到期' in l:\n",
    "                    continue\n",
    "                if ',' in l:\n",
    "                    continue\n",
    "                if '.' in l:\n",
    "                    continue\n",
    "                if '/' in l:\n",
    "                    continue\n",
    "                if '年' in l:\n",
    "                    continue\n",
    "                if '-' in l:\n",
    "                    continue\n",
    "                if len(l) < 4:\n",
    "                    continue\n",
    "                if is_number(l):\n",
    "                    continue\n",
    "                selected_bank_and_works.append(l)\n",
    "            if len(selected_bank_and_works) < 1:\n",
    "                continue\n",
    "            \n",
    "            temp_single['认购日期'] = rgrq\n",
    "            temp_single['产品起息日'] = cpqxr\n",
    "            temp_single['产品到期日'] = cpdxr\n",
    "            temp_single['产品期限'] = cpqx\n",
    "            temp_single['认购金额(万元)'] = new_h[0]\n",
    "            temp_single['产品发行方名称'] = mowei\n",
    "            temp_single['理财产品名称'] = selected_bank_and_works[0]\n",
    "            temp_df.append(temp_single)\n",
    "        new_df.append(temp_df)\n",
    "    return new_df\n",
    "\n",
    "val_contain_date = get_list_data(val_df_tabel)\n",
    "test_contain_data = get_list_data(test_df_tabel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.汇总整理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将前面提取到的数据整理成对应格式\n",
    "sample_id_list = []\n",
    "rgrq_list = []\n",
    "lccp_list = []\n",
    "cpfxf_list = []\n",
    "rgje_list = []\n",
    "cpqxr_list = []\n",
    "cpdxr_list = []\n",
    "cpqx_list = []\n",
    "sjgmgsmc_list = []\n",
    "ggrq_list = []\n",
    "\n",
    "sample_id = list(val_df['sample_id'])\n",
    "gg = list(val_gm)\n",
    "time = list(val_time)\n",
    "for i, value in enumerate(sample_id):\n",
    "    for j in val_contain_date[i]:\n",
    "        sample_id_list.append(sample_id[i])\n",
    "        rgrq_list.append(j['认购日期'])\n",
    "        lccp_list.append(j['理财产品名称'])\n",
    "        cpfxf_list.append(j['产品发行方名称'])\n",
    "        rgje_list.append(j['认购金额(万元)'])\n",
    "        cpqxr_list.append(j['产品起息日'])\n",
    "        cpdxr_list.append(j['产品到期日'])\n",
    "        cpqx_list.append(j['产品期限'])\n",
    "        sjgmgsmc_list.append(gg[i])\n",
    "        ggrq_list.append(time[i])\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['sample_id'] = sample_id_list\n",
    "result['认购日期'] = rgrq_list\n",
    "result['理财产品名称'] = lccp_list\n",
    "result['产品发行方名称'] = cpfxf_list\n",
    "result['认购金额(万元)'] = rgje_list\n",
    "result['产品起息日'] = cpqxr_list\n",
    "result['产品到期日'] = cpdxr_list\n",
    "result['产品期限'] = cpqx_list\n",
    "result['实际购买公司名称'] = sjgmgsmc_list\n",
    "result['公告日期'] = ggrq_list\n",
    "val_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       sample_id        认购日期                         理财产品名称  \\\n0          11190  2016-06-28                           星河文化   \n1          11190  2016-07-01                           西藏星河   \n2          11190  2016-07-28                          持有期不定   \n3          11190  2016-08-03                           西藏星河   \n4          11190  2016-10-10                           北京京艺   \n...          ...         ...                            ...   \n24566      22332  2015-10-16         平安银行卓越计划滚动型保本人民币公司理财产品   \n24567      22337  2019-03-01  中国工商银行法人“添利宝”净值型理财产品（TLB1801)   \n24568      22338  2019-01-07                          结构性存款   \n24569      22340  2017-01-19         中信理财之共赢保本步步高升B款人民币理财产品   \n24570      22360  2016-10-12                           花王股份   \n\n                 产品发行方名称  认购金额(万元)       产品起息日       产品到期日  产品期限  \\\n0                   建设银行    1000.0  2016-06-28         NaN   NaN   \n1                   包商银行   11500.0  2016-07-01  2016-12-26  178天   \n2                   工商银行   13700.0  2016-07-28         NaN   NaN   \n3                   南京银行    1000.0  2016-08-03  2016-11-12  101天   \n4                   交通银行    1000.0  2016-10-10  2016-11-07   28天   \n...                  ...       ...         ...         ...   ...   \n24566                NaN    3900.0  2015-10-16         NaN   NaN   \n24567             中国工商银行   10000.0  2019-03-01         NaN   NaN   \n24568  招商银行股份有限公司长沙韶山路支行   12000.0  2019-01-07  2019-03-08   60天   \n24569               中信银行    4100.0  2017-01-19         NaN   NaN   \n24570           中信银行丹阳支行   10000.0  2016-10-12  2017-01-11   91天   \n\n               实际购买公司名称        公告日期  \n0        北京京西文化旅游股份有限公司  2016-11-23  \n1        北京京西文化旅游股份有限公司  2016-11-23  \n2        北京京西文化旅游股份有限公司  2016-11-23  \n3        北京京西文化旅游股份有限公司  2016-11-23  \n4        北京京西文化旅游股份有限公司  2016-11-23  \n...                 ...         ...  \n24566    赛轮金宇集团股份有限公司关于  2016-05-12  \n24567  北京东方新星石化工程股份有限公司  2019-03-04  \n24568      三诺生物传感股份有限公司  2019-01-72  \n24569      丝路视觉科技股份有限公司  2018-01-25  \n24570      江苏花王园艺股份有限公司  2016-10-14  \n\n[24571 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>认购日期</th>\n      <th>理财产品名称</th>\n      <th>产品发行方名称</th>\n      <th>认购金额(万元)</th>\n      <th>产品起息日</th>\n      <th>产品到期日</th>\n      <th>产品期限</th>\n      <th>实际购买公司名称</th>\n      <th>公告日期</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11190</td>\n      <td>2016-06-28</td>\n      <td>星河文化</td>\n      <td>建设银行</td>\n      <td>1000.0</td>\n      <td>2016-06-28</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>北京京西文化旅游股份有限公司</td>\n      <td>2016-11-23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11190</td>\n      <td>2016-07-01</td>\n      <td>西藏星河</td>\n      <td>包商银行</td>\n      <td>11500.0</td>\n      <td>2016-07-01</td>\n      <td>2016-12-26</td>\n      <td>178天</td>\n      <td>北京京西文化旅游股份有限公司</td>\n      <td>2016-11-23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11190</td>\n      <td>2016-07-28</td>\n      <td>持有期不定</td>\n      <td>工商银行</td>\n      <td>13700.0</td>\n      <td>2016-07-28</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>北京京西文化旅游股份有限公司</td>\n      <td>2016-11-23</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11190</td>\n      <td>2016-08-03</td>\n      <td>西藏星河</td>\n      <td>南京银行</td>\n      <td>1000.0</td>\n      <td>2016-08-03</td>\n      <td>2016-11-12</td>\n      <td>101天</td>\n      <td>北京京西文化旅游股份有限公司</td>\n      <td>2016-11-23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11190</td>\n      <td>2016-10-10</td>\n      <td>北京京艺</td>\n      <td>交通银行</td>\n      <td>1000.0</td>\n      <td>2016-10-10</td>\n      <td>2016-11-07</td>\n      <td>28天</td>\n      <td>北京京西文化旅游股份有限公司</td>\n      <td>2016-11-23</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>24566</th>\n      <td>22332</td>\n      <td>2015-10-16</td>\n      <td>平安银行卓越计划滚动型保本人民币公司理财产品</td>\n      <td>NaN</td>\n      <td>3900.0</td>\n      <td>2015-10-16</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>赛轮金宇集团股份有限公司关于</td>\n      <td>2016-05-12</td>\n    </tr>\n    <tr>\n      <th>24567</th>\n      <td>22337</td>\n      <td>2019-03-01</td>\n      <td>中国工商银行法人“添利宝”净值型理财产品（TLB1801)</td>\n      <td>中国工商银行</td>\n      <td>10000.0</td>\n      <td>2019-03-01</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>北京东方新星石化工程股份有限公司</td>\n      <td>2019-03-04</td>\n    </tr>\n    <tr>\n      <th>24568</th>\n      <td>22338</td>\n      <td>2019-01-07</td>\n      <td>结构性存款</td>\n      <td>招商银行股份有限公司长沙韶山路支行</td>\n      <td>12000.0</td>\n      <td>2019-01-07</td>\n      <td>2019-03-08</td>\n      <td>60天</td>\n      <td>三诺生物传感股份有限公司</td>\n      <td>2019-01-72</td>\n    </tr>\n    <tr>\n      <th>24569</th>\n      <td>22340</td>\n      <td>2017-01-19</td>\n      <td>中信理财之共赢保本步步高升B款人民币理财产品</td>\n      <td>中信银行</td>\n      <td>4100.0</td>\n      <td>2017-01-19</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>丝路视觉科技股份有限公司</td>\n      <td>2018-01-25</td>\n    </tr>\n    <tr>\n      <th>24570</th>\n      <td>22360</td>\n      <td>2016-10-12</td>\n      <td>花王股份</td>\n      <td>中信银行丹阳支行</td>\n      <td>10000.0</td>\n      <td>2016-10-12</td>\n      <td>2017-01-11</td>\n      <td>91天</td>\n      <td>江苏花王园艺股份有限公司</td>\n      <td>2016-10-14</td>\n    </tr>\n  </tbody>\n</table>\n<p>24571 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "sample_id_list = []\n",
    "rgrq_list = []\n",
    "lccp_list = []\n",
    "cpfxf_list = []\n",
    "rgje_list = []\n",
    "cpqxr_list = []\n",
    "cpdxr_list = []\n",
    "cpqx_list = []\n",
    "sjgmgsmc_list = []\n",
    "ggrq_list = []\n",
    "\n",
    "sample_id = list(test_df['sample_id'])\n",
    "gg = list(test_gm)\n",
    "time = list(test_time)\n",
    "for i, value in enumerate(sample_id):\n",
    "    for j in test_contain_data[i]:\n",
    "        sample_id_list.append(sample_id[i])\n",
    "        rgrq_list.append(j['认购日期'])\n",
    "        lccp_list.append(j['理财产品名称'])\n",
    "        cpfxf_list.append(j['产品发行方名称'])\n",
    "        rgje_list.append(j['认购金额(万元)'])\n",
    "        cpqxr_list.append(j['产品起息日'])\n",
    "        cpdxr_list.append(j['产品到期日'])\n",
    "        cpqx_list.append(j['产品期限'])\n",
    "        sjgmgsmc_list.append(gg[i])\n",
    "        ggrq_list.append(time[i])\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['sample_id'] = sample_id_list\n",
    "result['认购日期'] = rgrq_list\n",
    "result['理财产品名称'] = lccp_list\n",
    "result['产品发行方名称'] = cpfxf_list\n",
    "result['认购金额(万元)'] = rgje_list\n",
    "result['产品起息日'] = cpqxr_list\n",
    "result['产品到期日'] = cpdxr_list\n",
    "result['产品期限'] = cpqx_list\n",
    "result['实际购买公司名称'] = sjgmgsmc_list\n",
    "result['公告日期'] = ggrq_list\n",
    "test_result = result\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模过程（预处理模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.采用LSTM网络用提取好的部分跟pdf中的text做交互预测\n",
    "#### 理财类型、资金来源、实际购买公司和上市公司关系、买卖方是否有关联关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后一部分字段采用预测好的部分，跟提取的text做交互采用双输入lstm在dense层做交互预测最后几个字段\n",
    "train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "train_lstm_input = train_lstm_input.fillna('否')\n",
    "# label_1理财类型-10  label_2资金来源-3 label_3实际购买公司和上市公司关系-3 label_4买卖方是否有关联关系-2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_1 = LabelEncoder()\n",
    "label_2 = LabelEncoder()\n",
    "label_3 = LabelEncoder()\n",
    "label_4 = LabelEncoder()\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) + '_' + train_lstm_input['产品发行方名称'].astype(str)\n",
    "train_data['text_2'] = train_lstm_input['text'].astype(str)\n",
    "\n",
    "train_data['label_1'] = label_1.fit_transform(train_lstm_input['理财类型'])\n",
    "train_data['label_2'] = label_2.fit_transform(train_lstm_input['资金来源'])\n",
    "train_data['label_3'] = label_3.fit_transform(train_lstm_input['实际购买公司和上市公司关系'])\n",
    "train_data['label_4'] = label_4.fit_transform(train_lstm_input['买卖方是否有关联关系'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import *\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import gensim\n",
    "import jieba\n",
    "tqdm.pandas()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# 显卡使用（如没显卡需要注释掉）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.random.set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 27154/27154 [00:04<00:00, 6045.74it/s]\n100%|██████████| 27154/27154 [14:40<00:00, 30.85it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              text_1  \\\n0  中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行   \n1  中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行   \n2                  与 利率 挂钩 的 结构性 产品 _ 中国民生银行 股份 有限公司   \n3  广发 银行 “ 薪 加薪 ” 16 号 XJXCKJ2578 _ 广发 银行 股份 有限公司...   \n4  兴业银行 “ 金 雪球 - 优悦 ” 保本 开放式 人民币 理财产品 ( 2M ) _ 兴业...   \n\n                                              text_2  label_1  label_2  \\\n0  [ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...        9        1   \n1  [ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...        9        1   \n2  [ ' 证券 代码 ： 600211                            ...        9        0   \n3  [ ' 证券 代码 ： 002171                            ...        9        1   \n4  [ ' 证券 代码 ： 002171                            ...        9        1   \n\n   label_3  label_4  \n0        0        0  \n1        0        0  \n2        0        0  \n3        2        0  \n4        0        0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_1</th>\n      <th>text_2</th>\n      <th>label_1</th>\n      <th>label_2</th>\n      <th>label_3</th>\n      <th>label_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行</td>\n      <td>[ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行</td>\n      <td>[ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>与 利率 挂钩 的 结构性 产品 _ 中国民生银行 股份 有限公司</td>\n      <td>[ ' 证券 代码 ： 600211                            ...</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>广发 银行 “ 薪 加薪 ” 16 号 XJXCKJ2578 _ 广发 银行 股份 有限公司...</td>\n      <td>[ ' 证券 代码 ： 002171                            ...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>兴业银行 “ 金 雪球 - 优悦 ” 保本 开放式 人民币 理财产品 ( 2M ) _ 兴业...</td>\n      <td>[ ' 证券 代码 ： 002171                            ...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "train_data['text_1'] = train_data['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data['text_2'] = train_data['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer 序列化文本\n",
    "def set_tokenizer(docs, split_char=' ', max_len=100):\n",
    "    '''\n",
    "    输入\n",
    "    docs:文本列表\n",
    "    split_char:按什么字符切割\n",
    "    max_len:截取的最大长度\n",
    "    \n",
    "    输出\n",
    "    X:序列化后的数据\n",
    "    word_index:文本和数字对应的索引\n",
    "    '''\n",
    "    tokenizer = Tokenizer(lower=False, char_level=False, split=split_char)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    X = tokenizer.texts_to_sequences(docs)\n",
    "    maxlen = max_len\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0)\n",
    "    word_index=tokenizer.word_index\n",
    "    return X, word_index, tokenizer\n",
    "\n",
    "### 做embedding 这里采用word2vec 可以换成其他例如（glove词向量）\n",
    "def trian_save_word2vec(docs, embed_size=300, save_name='w2v.txt', split_char=' '):\n",
    "    '''\n",
    "    输入\n",
    "    docs:输入的文本列表\n",
    "    embed_size:embed长度\n",
    "    save_name:保存的word2vec位置\n",
    "    \n",
    "    输出\n",
    "    w2v:返回的模型\n",
    "    '''\n",
    "    input_docs = []\n",
    "    for i in docs:\n",
    "        input_docs.append(i.split(split_char))\n",
    "    logging.basicConfig(\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "    w2v = Word2Vec(input_docs, size=embed_size, sg=1, window=8, seed=1017, workers=24, min_count=1, iter=10)\n",
    "    w2v.save(save_name)\n",
    "    print(\"w2v model done\")\n",
    "    return w2v\n",
    "\n",
    "# 得到embedding矩阵\n",
    "def get_embedding_matrix(word_index, embed_size=300, Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = Word2Vec.load(Emed_path)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  \n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "# 得到fasttext矩阵\n",
    "def load_fasttext(word_index, path):  \n",
    "    count=0\n",
    "    null_list=[]\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(path, encoding='utf-8') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words =  len(word_index)+1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            null_list.append(word)\n",
    "            count+=1\n",
    "    print(\"null cnt:\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embedding_matrix_txt(word_index,embed_size=200,Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        Emed_path, binary=False)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.训练得到word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "开始序列化\n序列化完成\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20040"
     },
     "metadata": {},
     "execution_count": 43
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "read finished; awaiting finish of 17 more threads\n2020-08-28 19:17:08,181:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:08,191:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:08,216:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:08,221:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:08,238:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:08,241:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:08,242:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:08,244:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:08,254:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:08,260:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:08,264:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:08,272:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:08,276:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:08,287:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:08,289:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:08,294:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:08,297:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:08,299:INFO:EPOCH - 2 : training on 175141 raw words (106398 effective words) took 0.3s, 310515 effective words/s\n2020-08-28 19:17:08,335:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:08,340:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:08,344:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:08,352:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:08,370:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:08,383:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:08,399:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:08,537:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:08,541:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:08,544:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:08,561:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:08,572:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:08,576:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:08,591:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:08,594:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:08,599:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:08,604:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:08,607:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:08,620:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:08,628:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:08,636:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:08,640:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:08,644:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:08,648:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:08,650:INFO:EPOCH - 3 : training on 175141 raw words (106515 effective words) took 0.3s, 323945 effective words/s\n2020-08-28 19:17:08,690:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:08,696:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:08,742:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:08,746:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:08,780:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:08,796:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:08,824:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:08,856:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:08,913:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:08,922:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:08,925:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:08,927:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:08,936:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:08,954:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:08,957:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:08,961:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:08,964:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:08,967:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:08,971:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:08,975:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:08,978:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:08,994:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:08,997:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:09,009:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:09,010:INFO:EPOCH - 4 : training on 175141 raw words (106286 effective words) took 0.3s, 314360 effective words/s\n2020-08-28 19:17:09,049:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:09,057:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:09,061:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:09,069:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:09,082:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:09,090:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:09,207:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:09,260:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:09,262:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:09,264:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:09,270:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:09,296:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:09,305:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:09,308:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:09,313:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:09,321:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:09,329:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:09,343:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:09,351:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:09,375:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:09,378:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:09,379:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:09,391:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:09,396:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:09,397:INFO:EPOCH - 5 : training on 175141 raw words (106686 effective words) took 0.4s, 292234 effective words/s\n2020-08-28 19:17:09,449:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:09,460:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:09,461:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:09,485:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:09,510:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:09,530:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:09,563:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:09,668:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:09,685:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:09,693:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:09,697:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:09,708:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:09,710:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:09,713:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:09,716:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:09,724:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:09,728:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:09,745:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:09,749:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:09,751:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:09,759:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:09,765:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:09,768:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:09,779:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:09,781:INFO:EPOCH - 6 : training on 175141 raw words (106334 effective words) took 0.4s, 300724 effective words/s\n2020-08-28 19:17:09,822:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:09,826:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:09,831:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:09,842:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:09,865:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:09,887:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:09,896:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:09,934:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:10,012:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:10,050:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:10,077:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:10,082:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:10,094:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:10,109:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:10,111:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:10,112:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:10,121:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:10,123:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:10,132:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:10,135:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:10,139:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:10,141:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:10,144:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:10,167:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:10,169:INFO:EPOCH - 7 : training on 175141 raw words (106781 effective words) took 0.4s, 292908 effective words/s\n2020-08-28 19:17:10,207:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:10,209:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:10,213:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:10,232:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:10,250:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:10,263:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:10,267:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:10,418:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:10,435:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:10,455:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:10,469:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:10,472:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:10,474:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:10,476:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:10,484:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:10,488:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:10,492:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:10,503:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:10,510:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:10,516:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:10,519:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:10,524:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:10,535:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:10,540:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:10,541:INFO:EPOCH - 8 : training on 175141 raw words (106688 effective words) took 0.4s, 303865 effective words/s\n2020-08-28 19:17:10,573:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:10,574:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:10,577:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:10,590:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:10,604:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:10,622:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:10,690:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:10,731:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:10,773:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:10,782:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:10,806:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:10,808:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:10,821:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:10,829:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:10,833:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:10,839:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:10,849:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:10,859:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:10,864:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:10,870:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:10,874:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:10,886:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:10,890:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:10,904:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:10,906:INFO:EPOCH - 9 : training on 175141 raw words (106540 effective words) took 0.3s, 309435 effective words/s\n2020-08-28 19:17:10,954:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-08-28 19:17:10,967:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-08-28 19:17:10,974:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-08-28 19:17:10,978:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-08-28 19:17:10,997:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-08-28 19:17:11,031:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-08-28 19:17:11,135:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-08-28 19:17:11,149:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-08-28 19:17:11,155:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-08-28 19:17:11,157:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-08-28 19:17:11,166:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-08-28 19:17:11,173:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-08-28 19:17:11,181:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-08-28 19:17:11,184:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-08-28 19:17:11,191:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-08-28 19:17:11,204:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-08-28 19:17:11,209:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-08-28 19:17:11,212:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-08-28 19:17:11,216:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-08-28 19:17:11,221:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-08-28 19:17:11,246:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-08-28 19:17:11,248:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-08-28 19:17:11,250:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-08-28 19:17:11,252:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-08-28 19:17:11,254:INFO:EPOCH - 10 : training on 175141 raw words (106542 effective words) took 0.3s, 329922 effective words/s\n2020-08-28 19:17:11,255:INFO:training on a 1751410 raw words (1065241 effective words) took 3.7s, 291007 effective words/s\n2020-08-28 19:17:11,256:WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n2020-08-28 19:17:11,257:INFO:saving Word2Vec object under models/w2v_300_1.txt, separately None\n2020-08-28 19:17:11,257:INFO:not storing attribute vectors_norm\n2020-08-28 19:17:11,258:INFO:not storing attribute cum_table\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/w2v_300_1.txt'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m             \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved %s object\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-4cbcedaadcaa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtrian_save_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_1_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'models/w2v_300_1.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_char\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtrian_save_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_3_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'models/w2v_300_3.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_char\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-7cd16fc13c91>\u001b[0m in \u001b[0;36mtrian_save_word2vec\u001b[1;34m(docs, embed_size, save_name, split_char)\u001b[0m\n\u001b[0;32m     36\u001b[0m     format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n\u001b[0;32m     37\u001b[0m     \u001b[0mw2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membed_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1017\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w2v model done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1266\u001b[0m         \u001b[1;31m# don't bother storing the cached normalized vectors, recalculable table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'vectors_norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cum_table'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_latest_training_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, **kwargs)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m         \"\"\"\n\u001b[1;32m--> 621\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseAny2VecModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    693\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved %s object\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `fname_or_handle` does not have write attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_smart_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseparately\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep_limit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36m_smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    547\u001b[0m                                        compress, subname)\n\u001b[0;32m    548\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m             \u001b[0mpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;31m# restore attribs handled specially\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mpickle\u001b[1;34m(obj, fname, protocol)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \"\"\"\n\u001b[1;32m-> 1363\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'b' for binary, needed on Windows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1364\u001b[0m         \u001b[0m_pickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/w2v_300_1.txt'"
     ]
    }
   ],
   "source": [
    "text_1_list = np.unique(train_data['text_1'])\n",
    "text_3_list = np.unique(train_data['text_2'])\n",
    "\n",
    "print('开始序列化')\n",
    "x1, index_1, token_1 = set_tokenizer(train_data['text_1'], split_char=' ', max_len=30)\n",
    "x3, index_3, token_3 = set_tokenizer(train_data['text_2'], split_char=' ', max_len=600)\n",
    "print('序列化完成')\n",
    "gc.collect()\n",
    "\n",
    "trian_save_word2vec(text_1_list, save_name='models/w2v_300_1.txt', split_char=' ')\n",
    "gc.collect()\n",
    "trian_save_word2vec(text_3_list, save_name='models/w2v_300_3.txt', split_char=' ')\n",
    "gc.collect()\n",
    "\n",
    "# 得到emb矩阵\n",
    "emb1 = get_embedding_matrix(index_1, Emed_path='models/w2v_300_1.txt')\n",
    "emb3 = get_embedding_matrix(index_3, Emed_path='models/w2v_300_3.txt')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建抽取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "19992"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "from keras.initializers import *\n",
    "\n",
    "def model_conv(emb1, emb3):\n",
    "    '''\n",
    "    注意这个inputs\n",
    "    seq1、seq2分别是两个输入\n",
    "    是否做emb可选可不选，\n",
    "    这个就是我们之前训练已经得到的用于embedding的（embedding_matrix1， embedding_matrix2）\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=30,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=600,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    seq1 = Input(shape=(30,))\n",
    "    seq3 = Input(shape=(600,))    \n",
    "    \n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    \n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x3 = sdrop(x3)\n",
    "     \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x1))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x3))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    \n",
    "    x = Multiply()([merged_1, merged_3])\n",
    "    \n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(x))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    pred_1 = Dense(10, activation='softmax')(x)\n",
    "    pred_2 = Dense(3, activation='softmax')(x)\n",
    "    pred_3 = Dense(3, activation='softmax')(x)\n",
    "    pred_4 = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=[seq1, seq3], outputs=[pred_1, pred_2, pred_3, pred_4])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0001),metrics=[\"accuracy\"])\n",
    "    return model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'emb1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-dda8c2f57a68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ml3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label_3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emb1' is not defined"
     ]
    }
   ],
   "source": [
    "model = model_conv(emb1, emb3)\n",
    "model.summary()\n",
    "l1 = to_categorical(train_data['label_1'], 10)\n",
    "l2 = to_categorical(train_data['label_2'], 3)\n",
    "l3 = to_categorical(train_data['label_3'], 3)\n",
    "l4 = to_categorical(train_data['label_4'], 2)\n",
    "model.fit([x1, x3],[l1, l2, l3, l4], batch_size=256, epochs=8, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-f293a16ee7f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#保存权重\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models/lstm_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#保存权重\n",
    "model.save_weights('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 13095/13095 [00:01<00:00, 9147.57it/s]\n100%|██████████| 13095/13095 [06:00<00:00, 36.35it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-be9ab3669bd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mpred_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mpred_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpred_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 预测验证集\n",
    "val_result_for_pred = pd.merge(val_result, val_df, on='sample_id', how='left')\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['理财产品名称'].astype(str) + '_' + val_result_for_pred['产品发行方名称'].astype(str)\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text'].astype(str)\n",
    "\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "\n",
    "x1 = token_1.texts_to_sequences(val_result_for_pred['text_1'])\n",
    "x1 = pad_sequences(x1, maxlen=30, value=0)\n",
    "x3 = token_3.texts_to_sequences(val_result_for_pred['text_2'])\n",
    "x3 = pad_sequences(x3, maxlen=600, value=0)\n",
    "pred_result = model.predict([x1, x3], batch_size=1024, verbose=1)\n",
    "pred_1 = label_1.inverse_transform(np.argmax(pred_result[0], axis=1))\n",
    "pred_2 = label_2.inverse_transform(np.argmax(pred_result[1], axis=1))\n",
    "pred_3 = label_3.inverse_transform(np.argmax(pred_result[2], axis=1))\n",
    "pred_4 = label_4.inverse_transform(np.argmax(pred_result[3], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pred_1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-33bb5074b888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财类型'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'资金来源'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司和上市公司关系'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'买卖方是否有关联关系'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_1' is not defined"
     ]
    }
   ],
   "source": [
    "val_result['理财类型'] = pred_1\n",
    "val_result['资金来源'] = pred_2\n",
    "val_result['实际购买公司和上市公司关系'] = pred_3\n",
    "val_result['买卖方是否有关联关系'] = pred_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.离线验证评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'理财类型'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '理财类型'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-ed2171a5992b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'认购日期'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财产品名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财类型'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'认购金额(万元)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品起息日'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品到期日'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品期限'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'资金来源'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司和上市公司关系'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'买卖方是否有关联关系'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'公告日期'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_F1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '理财类型'"
     ]
    }
   ],
   "source": [
    "# 计算线下f1\n",
    "# R（召回率）=抽取正确的记录数量/（抽取正确的目标数量+漏抽取的记录数量）\n",
    "# P（准确率）=抽取正确的记录数量/（抽取错误的记录数量+抽取正确的记录数量）\n",
    "def get_F1(val_pred, val_true):\n",
    "    val_pred = list(val_pred)\n",
    "    val_true = list(val_true)\n",
    "    curr = list(set(val_pred).intersection(set(val_true)))\n",
    "    R = len(curr)/len(val_true)\n",
    "    P = len(curr)/len(val_pred)\n",
    "    return 2*P*R/(P+R)\n",
    "\n",
    "r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "val_true = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['理财类型'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) + r['资金来源'].astype(str) + r['实际购买公司名称'].astype(str) + r['实际购买公司和上市公司关系'].astype(str) + r['买卖方是否有关联关系'].astype(str) + r['公告日期'].astype(str)\n",
    "\n",
    "r = val_result\n",
    "val_pred = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['理财类型'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到期日'].astype(str) + r['产品期限'].astype(str) + r['资金来源'].astype(str) + r['实际购买公司名称'].astype(str) + r['实际购买公司和上市公司关系'].astype(str) + r['买卖方是否有关联关系'].astype(str) + r['公告日期'].astype(str)\n",
    "\n",
    "score = get_F1(val_pred, val_true)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.最终输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 24571/24571 [00:03<00:00, 8030.03it/s]\n100%|██████████| 24571/24571 [07:13<00:00, 56.72it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-f7f39f3b8a78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mx3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mpred_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mpred_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mpred_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 预测测试集\n",
    "test_result_for_pred = pd.merge(test_result, test_df, on='sample_id', how='left')\n",
    "test_result_for_pred['text_1'] = test_result_for_pred['理财产品名称'].astype(str) + '_' + test_result_for_pred['产品发行方名称'].astype(str)\n",
    "test_result_for_pred['text_2'] = test_result_for_pred['text'].astype(str)\n",
    "\n",
    "test_result_for_pred['text_1'] = test_result_for_pred['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "test_result_for_pred['text_2'] = test_result_for_pred['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "\n",
    "x1 = token_1.texts_to_sequences(test_result_for_pred['text_1'])\n",
    "x1 = pad_sequences(x1, maxlen=30, value=0)\n",
    "x3 = token_3.texts_to_sequences(test_result_for_pred['text_2'])\n",
    "x3 = pad_sequences(x3, maxlen=600, value=0)\n",
    "pred_result = model.predict([x1, x3], batch_size=1024, verbose=1)\n",
    "pred_1 = label_1.inverse_transform(np.argmax(pred_result[0], axis=1))\n",
    "pred_2 = label_2.inverse_transform(np.argmax(pred_result[1], axis=1))\n",
    "pred_3 = label_3.inverse_transform(np.argmax(pred_result[2], axis=1))\n",
    "pred_4 = label_4.inverse_transform(np.argmax(pred_result[3], axis=1))\n",
    "\n",
    "test_result['理财类型'] = pred_1\n",
    "test_result['资金来源'] = pred_2\n",
    "test_result['实际购买公司和上市公司关系'] = pred_3\n",
    "test_result['买卖方是否有关联关系'] = pred_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('results/re_lstm_base.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}