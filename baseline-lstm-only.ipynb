{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from datetime import datetime \n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from tqdm.autonotebook import *\n",
    "import pdfplumber\n",
    "tqdm.pandas()\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF解析原始数据 \n",
    "## 加载数据并采用pdfplumber抽取PDF中的文字和表格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id       认购日期        理财产品名称     产品发行方名称    理财类型  认购金额(万元)  \\\n0          1 2019-03-27         汇聚金1号  中融国际信托有限公司      信托   10000.0   \n1          1 2019-03-27  招商银行步步生金8699        招商银行  银行理财产品     200.0   \n\n       产品起息日      产品到息日  产品期限  资金来源    实际购买公司名称 实际购买公司和上市公司关系 买卖方是否有关联关系  \\\n0 2019-03-27 2019-09-23  180天  自有资金  恒生电子股份有限公司          公司本身          否   \n1 2019-03-27        NaT   NaN  自有资金  恒生电子股份有限公司          公司本身          否   \n\n        公告日期  \n0 2019-04-25  \n1 2019-04-25  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>认购日期</th>\n      <th>理财产品名称</th>\n      <th>产品发行方名称</th>\n      <th>理财类型</th>\n      <th>认购金额(万元)</th>\n      <th>产品起息日</th>\n      <th>产品到息日</th>\n      <th>产品期限</th>\n      <th>资金来源</th>\n      <th>实际购买公司名称</th>\n      <th>实际购买公司和上市公司关系</th>\n      <th>买卖方是否有关联关系</th>\n      <th>公告日期</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2019-03-27</td>\n      <td>汇聚金1号</td>\n      <td>中融国际信托有限公司</td>\n      <td>信托</td>\n      <td>10000.0</td>\n      <td>2019-03-27</td>\n      <td>2019-09-23</td>\n      <td>180天</td>\n      <td>自有资金</td>\n      <td>恒生电子股份有限公司</td>\n      <td>公司本身</td>\n      <td>否</td>\n      <td>2019-04-25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2019-03-27</td>\n      <td>招商银行步步生金8699</td>\n      <td>招商银行</td>\n      <td>银行理财产品</td>\n      <td>200.0</td>\n      <td>2019-03-27</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>自有资金</td>\n      <td>恒生电子股份有限公司</td>\n      <td>公司本身</td>\n      <td>否</td>\n      <td>2019-04-25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id                  file_path  \\\n0          1  datasets/train_data/1.PDF   \n1          2  datasets/train_data/2.PDF   \n\n                                                text  \\\n0  ['                                            ...   \n1  ['                                            ...   \n\n                                               tabel  \n0  [[['', None, None, '', None, None, '', None, N...  \n1  [[['', None, None, '', None, None, '', None, N...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>file_path</th>\n      <th>text</th>\n      <th>tabel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>datasets/train_data/1.PDF</td>\n      <td>['                                            ...</td>\n      <td>[[['', None, None, '', None, None, '', None, N...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>datasets/train_data/2.PDF</td>\n      <td>['                                            ...</td>\n      <td>[[['', None, None, '', None, None, '', None, N...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   sample_id                     file_path  \\\n0      11188  datasets/test_data/11188.PDF   \n1      11189  datasets/test_data/11189.PDF   \n\n                                                text tabel  \n0  ['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...    []  \n1  ['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...    []  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>file_path</th>\n      <th>text</th>\n      <th>tabel</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11188</td>\n      <td>datasets/test_data/11188.PDF</td>\n      <td>['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>11189</td>\n      <td>datasets/test_data/11189.PDF</td>\n      <td>['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# 数据准备(train_output文件中格式有点问题，需要提前用excel或者wps打开然后另存为excel文件)\n",
    "train_outputs = pd.read_excel('datasets/train_output.xlsx')\n",
    "\n",
    "# 获取pdf中文字和表格\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index_page in np.arange(0, len(pdf.pages), 1):\n",
    "            # 读取多页\n",
    "            page = pdf.pages[index_page]   # 第n页的信息\n",
    "            text = page.extract_text()\n",
    "            text_list.append(text)\n",
    "            table = page.extract_tables()\n",
    "            for t in table:\n",
    "                table_list.append(t)\n",
    "    return text_list, table_list\n",
    "\n",
    "def get_dir_file(path):\n",
    "    '''\n",
    "    输入文件夹位置，输出整理好的dataframe\n",
    "    '''\n",
    "    path_list = os.listdir(path)\n",
    "    id_list = []\n",
    "    file_path_list = []\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    for i in tqdm(path_list):\n",
    "        if '.PDF' in i:\n",
    "            file_path = path + i\n",
    "            id_list.append(int(i.split('.')[0]))\n",
    "            file_path_list.append(file_path)\n",
    "            try:\n",
    "                text_temp, table_temp = extract_pdf_content(file_path)\n",
    "            except Exception:\n",
    "                print('此pdf无法读取')\n",
    "                text_temp, table_temp = [], []\n",
    "            text_list.append(text_temp)\n",
    "            table_list.append(table_temp)\n",
    "            \n",
    "    df = pd.DataFrame()\n",
    "    df['sample_id'] = id_list\n",
    "    df['file_path'] = file_path_list\n",
    "    df['text'] = text_list\n",
    "    df['tabel'] = table_list\n",
    "    df = df.sort_values('sample_id')\n",
    "    return df\n",
    "\n",
    "# 文件处理太慢，可持续化保存文件\n",
    "train_path = 'datasets/train.csv'\n",
    "if os.path.exists(train_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "else:\n",
    "    train_df = get_dir_file('datasets/train_data/')\n",
    "    train_df.to_csv(train_path,index=False)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "test_path =  'datasets/test.csv'\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "else:\n",
    "    test_df = get_dir_file('datasets/test_data/')\n",
    "    test_df.to_csv(test_path,index=False)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_outputs.head(2)\n",
    "train_df.head(2)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集验证集\n",
    "train_df = train_df.sample(frac=1, random_state=1017)\n",
    "val_df = train_df[:1800]\n",
    "train_df = train_df[1800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.采用LSTM网络用提取好的部分跟pdf中的text做交互预测\n",
    "#### 理财类型、资金来源、实际购买公司和上市公司关系、买卖方是否有关联关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后一部分字段采用预测好的部分，跟提取的text做交互采用双输入lstm在dense层做交互预测最后几个字段\n",
    "train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "train_lstm_input = train_lstm_input.fillna('否')\n",
    "# label_1理财类型-10  label_2资金来源-3 label_3实际购买公司和上市公司关系-3 label_4买卖方是否有关联关系-2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_1 = LabelEncoder()\n",
    "label_2 = LabelEncoder()\n",
    "label_3 = LabelEncoder()\n",
    "label_4 = LabelEncoder()\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) + '_' + train_lstm_input['产品发行方名称'].astype(str)\n",
    "train_data['text_2'] = train_lstm_input['text'].astype(str)\n",
    "\n",
    "train_data['label_1'] = label_1.fit_transform(train_lstm_input['理财类型'])\n",
    "train_data['label_2'] = label_2.fit_transform(train_lstm_input['资金来源'])\n",
    "train_data['label_3'] = label_3.fit_transform(train_lstm_input['实际购买公司和上市公司关系'])\n",
    "train_data['label_4'] = label_4.fit_transform(train_lstm_input['买卖方是否有关联关系'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import *\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import gensim\n",
    "import jieba\n",
    "tqdm.pandas()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# 显卡使用（如没显卡需要注释掉）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.random.set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/27154 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\lsqlh\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 1.153 seconds.\nPrefix dict has been built successfully.\n100%|██████████| 27154/27154 [00:06<00:00, 4267.83it/s]\n100%|██████████| 27154/27154 [17:27<00:00, 25.91it/s]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                              text_1  \\\n0  中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行   \n1  中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行   \n2                  与 利率 挂钩 的 结构性 产品 _ 中国民生银行 股份 有限公司   \n3  广发 银行 “ 薪 加薪 ” 16 号 XJXCKJ2578 _ 广发 银行 股份 有限公司...   \n4  兴业银行 “ 金 雪球 - 优悦 ” 保本 开放式 人民币 理财产品 ( 2M ) _ 兴业...   \n\n                                              text_2  label_1  label_2  \\\n0  [ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...        9        1   \n1  [ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...        9        1   \n2  [ ' 证券 代码 ： 600211                            ...        9        0   \n3  [ ' 证券 代码 ： 002171                            ...        9        1   \n4  [ ' 证券 代码 ： 002171                            ...        9        1   \n\n   label_3  label_4  \n0        0        0  \n1        0        0  \n2        0        0  \n3        2        0  \n4        0        0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_1</th>\n      <th>text_2</th>\n      <th>label_1</th>\n      <th>label_2</th>\n      <th>label_3</th>\n      <th>label_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行</td>\n      <td>[ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>中银 保本 理财 - 人民币 按期 开放 理财产品 _ 中国银行 股份 有限公司 广州 东山 支行</td>\n      <td>[ ' 证券 代码 ： 600728                 证券 简称 ： 佳 都...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>与 利率 挂钩 的 结构性 产品 _ 中国民生银行 股份 有限公司</td>\n      <td>[ ' 证券 代码 ： 600211                            ...</td>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>广发 银行 “ 薪 加薪 ” 16 号 XJXCKJ2578 _ 广发 银行 股份 有限公司...</td>\n      <td>[ ' 证券 代码 ： 002171                            ...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>兴业银行 “ 金 雪球 - 优悦 ” 保本 开放式 人民币 理财产品 ( 2M ) _ 兴业...</td>\n      <td>[ ' 证券 代码 ： 002171                            ...</td>\n      <td>9</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "train_data['text_1'] = train_data['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data['text_2'] = train_data['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer 序列化文本\n",
    "def set_tokenizer(docs, split_char=' ', max_len=100):\n",
    "    '''\n",
    "    输入\n",
    "    docs:文本列表\n",
    "    split_char:按什么字符切割\n",
    "    max_len:截取的最大长度\n",
    "    \n",
    "    输出\n",
    "    X:序列化后的数据\n",
    "    word_index:文本和数字对应的索引\n",
    "    '''\n",
    "    tokenizer = Tokenizer(lower=False, char_level=False, split=split_char)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    X = tokenizer.texts_to_sequences(docs)\n",
    "    maxlen = max_len\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0)\n",
    "    word_index=tokenizer.word_index\n",
    "    return X, word_index, tokenizer\n",
    "\n",
    "### 做embedding 这里采用word2vec 可以换成其他例如（glove词向量）\n",
    "def trian_save_word2vec(docs, embed_size=300, save_name='w2v.txt', split_char=' '):\n",
    "    '''\n",
    "    输入\n",
    "    docs:输入的文本列表\n",
    "    embed_size:embed长度\n",
    "    save_name:保存的word2vec位置\n",
    "    \n",
    "    输出\n",
    "    w2v:返回的模型\n",
    "    '''\n",
    "    input_docs = []\n",
    "    for i in docs:\n",
    "        input_docs.append(i.split(split_char))\n",
    "    logging.basicConfig(\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "    w2v = Word2Vec(input_docs, size=embed_size, sg=1, window=8, seed=1017, workers=24, min_count=1, iter=10)\n",
    "    w2v.save(save_name)\n",
    "    print(\"w2v model done\")\n",
    "    return w2v\n",
    "\n",
    "# 得到embedding矩阵\n",
    "def get_embedding_matrix(word_index, embed_size=300, Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = Word2Vec.load(Emed_path)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  \n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "# 得到fasttext矩阵\n",
    "def load_fasttext(word_index, path):  \n",
    "    count=0\n",
    "    null_list=[]\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(path, encoding='utf-8') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words =  len(word_index)+1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            null_list.append(word)\n",
    "            count+=1\n",
    "    print(\"null cnt:\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embedding_matrix_txt(word_index,embed_size=200,Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        Emed_path, binary=False)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.训练得到word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "开始序列化\n序列化完成\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "75:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:43,177:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:43,190:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:43,194:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:43,199:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:43,202:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:43,205:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:43,219:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:43,226:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:43,230:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:43,246:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:43,251:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:43,255:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:43,257:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:43,260:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:43,271:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:43,273:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:43,274:INFO:EPOCH - 2 : training on 175141 raw words (106614 effective words) took 0.4s, 296596 effective words/s\n2020-09-11 13:18:43,310:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:43,322:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:43,361:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:43,364:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:43,389:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:43,395:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:43,398:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:43,452:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:43,521:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:43,547:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:43,554:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:43,563:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:43,565:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:43,570:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:43,592:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:43,595:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:43,602:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:43,608:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:43,613:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:43,616:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:43,627:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:43,633:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:43,638:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:43,650:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:43,651:INFO:EPOCH - 3 : training on 175141 raw words (106380 effective words) took 0.4s, 300169 effective words/s\n2020-09-11 13:18:43,701:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:43,703:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:43,708:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:43,717:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:43,748:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:43,758:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:43,847:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:43,850:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:43,914:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:43,917:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:43,929:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:43,932:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:43,948:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:43,953:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:43,972:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:44,001:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:44,008:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:44,011:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:44,013:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:44,015:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:44,018:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:44,021:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:44,041:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:44,049:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:44,052:INFO:EPOCH - 4 : training on 175141 raw words (106631 effective words) took 0.4s, 289070 effective words/s\n2020-09-11 13:18:44,099:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:44,110:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:44,122:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:44,124:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:44,150:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:44,169:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:44,192:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:44,302:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:44,334:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:44,345:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:44,382:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:44,385:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:44,387:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:44,388:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:44,389:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:44,400:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:44,404:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:44,409:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:44,410:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:44,412:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:44,414:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:44,423:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:44,426:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:44,453:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:44,456:INFO:EPOCH - 5 : training on 175141 raw words (106571 effective words) took 0.4s, 282518 effective words/s\n2020-09-11 13:18:44,575:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:44,587:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:44,599:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:44,609:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:44,631:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:44,643:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:44,649:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:44,668:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:44,737:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:44,740:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:44,764:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:44,772:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:44,777:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:44,784:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:44,789:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:44,791:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:44,800:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:44,813:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:44,815:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:44,821:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:44,826:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:44,840:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:44,853:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:44,856:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:44,859:INFO:EPOCH - 6 : training on 175141 raw words (106479 effective words) took 0.4s, 279373 effective words/s\n2020-09-11 13:18:44,906:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:44,920:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:44,938:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:44,946:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:44,951:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:44,986:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:45,081:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:45,083:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:45,104:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:45,106:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:45,134:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:45,171:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:45,184:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:45,190:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:45,193:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:45,195:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:45,197:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:45,213:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:45,221:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:45,225:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:45,229:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:45,231:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:45,245:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:45,256:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:45,259:INFO:EPOCH - 7 : training on 175141 raw words (106786 effective words) took 0.4s, 285504 effective words/s\n2020-09-11 13:18:45,309:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:45,318:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:45,354:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:45,388:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:45,411:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:45,414:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:45,418:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:45,504:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:45,549:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:45,577:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:45,582:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:45,594:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:45,600:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:45,603:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:45,607:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:45,614:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:45,619:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:45,622:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:45,634:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:45,637:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:45,638:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:45,640:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:45,650:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:45,663:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:45,666:INFO:EPOCH - 8 : training on 175141 raw words (106489 effective words) took 0.4s, 280368 effective words/s\n2020-09-11 13:18:45,719:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:45,732:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:45,739:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:45,783:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:45,801:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:45,810:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:45,904:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:45,923:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:45,929:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:45,932:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:45,955:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:45,969:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:45,982:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:45,998:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:46,000:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:46,009:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:46,011:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:46,015:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:46,020:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:46,022:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:46,025:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:46,032:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:46,057:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:46,067:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:46,069:INFO:EPOCH - 9 : training on 175141 raw words (106625 effective words) took 0.4s, 281368 effective words/s\n2020-09-11 13:18:46,113:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:18:46,116:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:18:46,144:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:18:46,168:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:18:46,185:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:18:46,216:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:18:46,268:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:18:46,348:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:18:46,379:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:18:46,392:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:18:46,395:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:18:46,404:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:18:46,409:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:18:46,426:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:18:46,428:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:18:46,446:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:18:46,450:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:18:46,453:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:18:46,455:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:18:46,458:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:18:46,461:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:18:46,473:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:18:46,481:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:18:46,486:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:18:46,488:INFO:EPOCH - 10 : training on 175141 raw words (106592 effective words) took 0.4s, 273997 effective words/s\n2020-09-11 13:18:46,490:INFO:training on a 1751410 raw words (1065676 effective words) took 4.0s, 267596 effective words/s\n2020-09-11 13:18:46,492:WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n2020-09-11 13:18:46,494:INFO:saving Word2Vec object under models/w2v_300_1.txt, separately None\n2020-09-11 13:18:46,495:INFO:not storing attribute vectors_norm\n2020-09-11 13:18:46,496:INFO:not storing attribute cum_table\n2020-09-11 13:18:46,709:INFO:saved models/w2v_300_1.txt\nw2v model done\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<gensim.models.word2vec.Word2Vec at 0x1d5f8f22cd0>"
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "476:INFO:EPOCH 8 - PROGRESS: at 49.04% examples, 214513 words/s, in_qsize 48, out_qsize 1\n2020-09-11 13:25:10,479:INFO:EPOCH 8 - PROGRESS: at 51.59% examples, 215987 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:11,548:INFO:EPOCH 8 - PROGRESS: at 54.07% examples, 216986 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:25:12,552:INFO:EPOCH 8 - PROGRESS: at 56.12% examples, 217769 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:13,560:INFO:EPOCH 8 - PROGRESS: at 58.24% examples, 219845 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:25:14,610:INFO:EPOCH 8 - PROGRESS: at 59.75% examples, 219646 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:15,685:INFO:EPOCH 8 - PROGRESS: at 61.72% examples, 220444 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:25:16,701:INFO:EPOCH 8 - PROGRESS: at 64.42% examples, 220858 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:17,750:INFO:EPOCH 8 - PROGRESS: at 68.12% examples, 221404 words/s, in_qsize 48, out_qsize 1\n2020-09-11 13:25:18,795:INFO:EPOCH 8 - PROGRESS: at 71.19% examples, 222223 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:19,798:INFO:EPOCH 8 - PROGRESS: at 73.58% examples, 221494 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:20,891:INFO:EPOCH 8 - PROGRESS: at 76.69% examples, 221406 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:21,913:INFO:EPOCH 8 - PROGRESS: at 79.28% examples, 221962 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:22,975:INFO:EPOCH 8 - PROGRESS: at 82.33% examples, 221673 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:23,997:INFO:EPOCH 8 - PROGRESS: at 84.86% examples, 221509 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:25,004:INFO:EPOCH 8 - PROGRESS: at 87.97% examples, 221489 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:26,041:INFO:EPOCH 8 - PROGRESS: at 91.09% examples, 221254 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:27,043:INFO:EPOCH 8 - PROGRESS: at 93.57% examples, 220427 words/s, in_qsize 48, out_qsize 2\n2020-09-11 13:25:28,054:INFO:EPOCH 8 - PROGRESS: at 96.60% examples, 220406 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:28,833:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:25:28,853:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:25:28,857:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:25:28,866:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:25:28,875:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:25:28,882:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:25:28,885:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:25:28,888:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:25:28,889:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:25:28,897:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:25:28,902:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:25:28,905:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:25:28,914:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:25:28,922:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:25:28,924:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:25:28,930:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:25:28,948:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:25:28,957:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:25:28,987:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:25:28,999:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:25:29,004:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:25:29,029:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:25:29,041:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:25:29,078:INFO:EPOCH 8 - PROGRESS: at 100.00% examples, 221548 words/s, in_qsize 0, out_qsize 1\n2020-09-11 13:25:29,079:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:25:29,080:INFO:EPOCH - 8 : training on 21113789 raw words (10775557 effective words) took 48.6s, 221535 effective words/s\n2020-09-11 13:25:30,387:INFO:EPOCH 9 - PROGRESS: at 0.97% examples, 124565 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:31,413:INFO:EPOCH 9 - PROGRESS: at 2.04% examples, 164636 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:32,428:INFO:EPOCH 9 - PROGRESS: at 3.55% examples, 177489 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:33,449:INFO:EPOCH 9 - PROGRESS: at 5.57% examples, 184369 words/s, in_qsize 48, out_qsize 1\n2020-09-11 13:25:34,477:INFO:EPOCH 9 - PROGRESS: at 6.94% examples, 184387 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:35,497:INFO:EPOCH 9 - PROGRESS: at 8.58% examples, 190731 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:25:36,681:INFO:EPOCH 9 - PROGRESS: at 10.42% examples, 190453 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:37,686:INFO:EPOCH 9 - PROGRESS: at 12.77% examples, 198763 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:38,749:INFO:EPOCH 9 - PROGRESS: at 14.50% examples, 194470 words/s, in_qsize 48, out_qsize 2\n2020-09-11 13:25:39,760:INFO:EPOCH 9 - PROGRESS: at 16.66% examples, 197009 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:40,762:INFO:EPOCH 9 - PROGRESS: at 19.09% examples, 197498 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:41,782:INFO:EPOCH 9 - PROGRESS: at 20.69% examples, 199089 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:42,893:INFO:EPOCH 9 - PROGRESS: at 22.38% examples, 199571 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:43,903:INFO:EPOCH 9 - PROGRESS: at 24.12% examples, 198803 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:25:44,911:INFO:EPOCH 9 - PROGRESS: at 27.65% examples, 205319 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:45,928:INFO:EPOCH 9 - PROGRESS: at 29.23% examples, 203365 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:46,937:INFO:EPOCH 9 - PROGRESS: at 31.92% examples, 206700 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:47,946:INFO:EPOCH 9 - PROGRESS: at 34.39% examples, 209314 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:48,985:INFO:EPOCH 9 - PROGRESS: at 35.37% examples, 210716 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:50,008:INFO:EPOCH 9 - PROGRESS: at 36.88% examples, 212831 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:51,016:INFO:EPOCH 9 - PROGRESS: at 39.22% examples, 214948 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:52,044:INFO:EPOCH 9 - PROGRESS: at 40.57% examples, 213959 words/s, in_qsize 48, out_qsize 1\n2020-09-11 13:25:53,066:INFO:EPOCH 9 - PROGRESS: at 42.02% examples, 217439 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:25:54,093:INFO:EPOCH 9 - PROGRESS: at 43.80% examples, 217634 words/s, in_qsize 47, out_qsize 1\n2020-09-11 13:25:55,117:INFO:EPOCH 9 - PROGRESS: at 45.28% examples, 219647 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:56,159:INFO:EPOCH 9 - PROGRESS: at 46.90% examples, 220161 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:25:57,261:INFO:EPOCH 9 - PROGRESS: at 48.61% examples, 221443 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:25:58,312:INFO:EPOCH 9 - PROGRESS: at 50.81% examples, 221411 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:25:59,312:INFO:EPOCH 9 - PROGRESS: at 52.64% examples, 220523 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:00,378:INFO:EPOCH 9 - PROGRESS: at 54.73% examples, 220001 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:01,382:INFO:EPOCH 9 - PROGRESS: at 57.01% examples, 222142 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:02,406:INFO:EPOCH 9 - PROGRESS: at 58.56% examples, 222028 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:03,429:INFO:EPOCH 9 - PROGRESS: at 60.44% examples, 222356 words/s, in_qsize 48, out_qsize 1\n2020-09-11 13:26:04,444:INFO:EPOCH 9 - PROGRESS: at 62.02% examples, 222476 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:05,473:INFO:EPOCH 9 - PROGRESS: at 64.74% examples, 222345 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:06,486:INFO:EPOCH 9 - PROGRESS: at 67.98% examples, 222059 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:07,563:INFO:EPOCH 9 - PROGRESS: at 70.21% examples, 221062 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:08,564:INFO:EPOCH 9 - PROGRESS: at 72.71% examples, 220582 words/s, in_qsize 47, out_qsize 1\n2020-09-11 13:26:09,632:INFO:EPOCH 9 - PROGRESS: at 75.88% examples, 220725 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:26:10,652:INFO:EPOCH 9 - PROGRESS: at 78.24% examples, 220585 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:11,674:INFO:EPOCH 9 - PROGRESS: at 81.25% examples, 220364 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:12,700:INFO:EPOCH 9 - PROGRESS: at 83.27% examples, 219907 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:13,714:INFO:EPOCH 9 - PROGRESS: at 86.24% examples, 219801 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:26:14,734:INFO:EPOCH 9 - PROGRESS: at 88.59% examples, 218898 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:15,802:INFO:EPOCH 9 - PROGRESS: at 91.75% examples, 218255 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:26:16,804:INFO:EPOCH 9 - PROGRESS: at 94.80% examples, 218176 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:17,811:INFO:EPOCH 9 - PROGRESS: at 97.09% examples, 217490 words/s, in_qsize 48, out_qsize 1\n2020-09-11 13:26:18,296:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:26:18,339:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:26:18,356:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:26:18,397:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:26:18,412:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:26:18,425:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:26:18,434:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:26:18,446:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:26:18,448:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:26:18,449:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:26:18,450:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:26:18,451:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:26:18,457:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:26:18,464:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:26:18,466:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:26:18,481:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:26:18,483:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:26:18,492:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:26:18,498:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:26:18,519:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:26:18,562:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:26:18,575:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:26:18,611:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:26:18,627:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:26:18,628:INFO:EPOCH - 9 : training on 21113789 raw words (10775794 effective words) took 49.3s, 218735 effective words/s\n2020-09-11 13:26:19,968:INFO:EPOCH 10 - PROGRESS: at 0.96% examples, 120636 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:20,972:INFO:EPOCH 10 - PROGRESS: at 1.93% examples, 157198 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:26:22,090:INFO:EPOCH 10 - PROGRESS: at 3.58% examples, 173779 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:26:23,095:INFO:EPOCH 10 - PROGRESS: at 5.74% examples, 185282 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:24,096:INFO:EPOCH 10 - PROGRESS: at 7.15% examples, 189206 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:25,126:INFO:EPOCH 10 - PROGRESS: at 8.76% examples, 192249 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:26,135:INFO:EPOCH 10 - PROGRESS: at 10.46% examples, 193587 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:27,140:INFO:EPOCH 10 - PROGRESS: at 12.45% examples, 197074 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:28,141:INFO:EPOCH 10 - PROGRESS: at 14.66% examples, 199753 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:29,219:INFO:EPOCH 10 - PROGRESS: at 16.91% examples, 202038 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:30,241:INFO:EPOCH 10 - PROGRESS: at 19.96% examples, 207036 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:31,278:INFO:EPOCH 10 - PROGRESS: at 21.66% examples, 209609 words/s, in_qsize 48, out_qsize 1\n2020-09-11 13:26:32,293:INFO:EPOCH 10 - PROGRESS: at 23.65% examples, 211187 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:33,330:INFO:EPOCH 10 - PROGRESS: at 26.40% examples, 214273 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:26:34,355:INFO:EPOCH 10 - PROGRESS: at 28.66% examples, 214302 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:35,361:INFO:EPOCH 10 - PROGRESS: at 31.41% examples, 217297 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:36,362:INFO:EPOCH 10 - PROGRESS: at 33.37% examples, 217186 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:37,371:INFO:EPOCH 10 - PROGRESS: at 34.87% examples, 217246 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:38,388:INFO:EPOCH 10 - PROGRESS: at 35.91% examples, 218807 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:39,390:INFO:EPOCH 10 - PROGRESS: at 38.28% examples, 221268 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:40,424:INFO:EPOCH 10 - PROGRESS: at 39.60% examples, 219255 words/s, in_qsize 45, out_qsize 2\n2020-09-11 13:26:41,485:INFO:EPOCH 10 - PROGRESS: at 41.01% examples, 219787 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:42,495:INFO:EPOCH 10 - PROGRESS: at 41.97% examples, 217778 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:43,521:INFO:EPOCH 10 - PROGRESS: at 43.83% examples, 218444 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:44,574:INFO:EPOCH 10 - PROGRESS: at 45.36% examples, 220974 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:45,576:INFO:EPOCH 10 - PROGRESS: at 46.90% examples, 221460 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:46,582:INFO:EPOCH 10 - PROGRESS: at 48.40% examples, 221903 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:47,596:INFO:EPOCH 10 - PROGRESS: at 50.60% examples, 222467 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:48,625:INFO:EPOCH 10 - PROGRESS: at 52.42% examples, 221637 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:49,632:INFO:EPOCH 10 - PROGRESS: at 54.40% examples, 220982 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:50,633:INFO:EPOCH 10 - PROGRESS: at 56.22% examples, 220827 words/s, in_qsize 43, out_qsize 4\n2020-09-11 13:26:51,634:INFO:EPOCH 10 - PROGRESS: at 58.06% examples, 221872 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:26:52,645:INFO:EPOCH 10 - PROGRESS: at 59.57% examples, 221817 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:53,655:INFO:EPOCH 10 - PROGRESS: at 61.22% examples, 221820 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:54,655:INFO:EPOCH 10 - PROGRESS: at 63.15% examples, 221309 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:26:55,663:INFO:EPOCH 10 - PROGRESS: at 66.07% examples, 221066 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:56,665:INFO:EPOCH 10 - PROGRESS: at 68.91% examples, 220482 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:57,719:INFO:EPOCH 10 - PROGRESS: at 71.34% examples, 220029 words/s, in_qsize 45, out_qsize 2\n2020-09-11 13:26:58,728:INFO:EPOCH 10 - PROGRESS: at 73.74% examples, 219607 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:26:59,779:INFO:EPOCH 10 - PROGRESS: at 76.63% examples, 219076 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:27:00,824:INFO:EPOCH 10 - PROGRESS: at 78.82% examples, 218578 words/s, in_qsize 46, out_qsize 1\n2020-09-11 13:27:01,850:INFO:EPOCH 10 - PROGRESS: at 81.62% examples, 218100 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:27:02,854:INFO:EPOCH 10 - PROGRESS: at 83.84% examples, 217778 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:27:03,867:INFO:EPOCH 10 - PROGRESS: at 86.79% examples, 217655 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:27:04,903:INFO:EPOCH 10 - PROGRESS: at 89.18% examples, 216789 words/s, in_qsize 48, out_qsize 0\n2020-09-11 13:27:05,927:INFO:EPOCH 10 - PROGRESS: at 92.70% examples, 217263 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:27:06,930:INFO:EPOCH 10 - PROGRESS: at 95.27% examples, 216358 words/s, in_qsize 47, out_qsize 0\n2020-09-11 13:27:07,982:INFO:EPOCH 10 - PROGRESS: at 98.79% examples, 216704 words/s, in_qsize 32, out_qsize 0\n2020-09-11 13:27:08,301:INFO:worker thread finished; awaiting finish of 23 more threads\n2020-09-11 13:27:08,361:INFO:worker thread finished; awaiting finish of 22 more threads\n2020-09-11 13:27:08,362:INFO:worker thread finished; awaiting finish of 21 more threads\n2020-09-11 13:27:08,363:INFO:worker thread finished; awaiting finish of 20 more threads\n2020-09-11 13:27:08,364:INFO:worker thread finished; awaiting finish of 19 more threads\n2020-09-11 13:27:08,365:INFO:worker thread finished; awaiting finish of 18 more threads\n2020-09-11 13:27:08,366:INFO:worker thread finished; awaiting finish of 17 more threads\n2020-09-11 13:27:08,367:INFO:worker thread finished; awaiting finish of 16 more threads\n2020-09-11 13:27:08,370:INFO:worker thread finished; awaiting finish of 15 more threads\n2020-09-11 13:27:08,372:INFO:worker thread finished; awaiting finish of 14 more threads\n2020-09-11 13:27:08,375:INFO:worker thread finished; awaiting finish of 13 more threads\n2020-09-11 13:27:08,377:INFO:worker thread finished; awaiting finish of 12 more threads\n2020-09-11 13:27:08,378:INFO:worker thread finished; awaiting finish of 11 more threads\n2020-09-11 13:27:08,379:INFO:worker thread finished; awaiting finish of 10 more threads\n2020-09-11 13:27:08,383:INFO:worker thread finished; awaiting finish of 9 more threads\n2020-09-11 13:27:08,393:INFO:worker thread finished; awaiting finish of 8 more threads\n2020-09-11 13:27:08,395:INFO:worker thread finished; awaiting finish of 7 more threads\n2020-09-11 13:27:08,401:INFO:worker thread finished; awaiting finish of 6 more threads\n2020-09-11 13:27:08,405:INFO:worker thread finished; awaiting finish of 5 more threads\n2020-09-11 13:27:08,491:INFO:worker thread finished; awaiting finish of 4 more threads\n2020-09-11 13:27:08,505:INFO:worker thread finished; awaiting finish of 3 more threads\n2020-09-11 13:27:08,513:INFO:worker thread finished; awaiting finish of 2 more threads\n2020-09-11 13:27:08,554:INFO:worker thread finished; awaiting finish of 1 more threads\n2020-09-11 13:27:08,585:INFO:worker thread finished; awaiting finish of 0 more threads\n2020-09-11 13:27:08,586:INFO:EPOCH - 10 : training on 21113789 raw words (10772277 effective words) took 49.7s, 216941 effective words/s\n2020-09-11 13:27:08,588:INFO:training on a 211137890 raw words (107752856 effective words) took 485.1s, 222104 effective words/s\n2020-09-11 13:27:08,590:INFO:saving Word2Vec object under models/w2v_300_3.txt, separately None\n2020-09-11 13:27:08,591:INFO:storing np array 'vectors' to models/w2v_300_3.txt.wv.vectors.npy\n2020-09-11 13:27:08,804:INFO:not storing attribute vectors_norm\n2020-09-11 13:27:08,806:INFO:storing np array 'syn1neg' to models/w2v_300_3.txt.trainables.syn1neg.npy\n2020-09-11 13:27:09,025:INFO:not storing attribute cum_table\n2020-09-11 13:27:09,247:INFO:saved models/w2v_300_3.txt\nw2v model done\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<gensim.models.word2vec.Word2Vec at 0x1d5bf854fd0>"
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 8
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-11 13:27:10,150:INFO:loading Word2Vec object from models/w2v_300_1.txt\n2020-09-11 13:27:10,450:INFO:loading wv recursively from models/w2v_300_1.txt.wv.* with mmap=None\n2020-09-11 13:27:10,452:INFO:setting ignored attribute vectors_norm to None\n2020-09-11 13:27:10,454:INFO:loading vocabulary recursively from models/w2v_300_1.txt.vocabulary.* with mmap=None\n2020-09-11 13:27:10,456:INFO:loading trainables recursively from models/w2v_300_1.txt.trainables.* with mmap=None\n2020-09-11 13:27:10,458:INFO:setting ignored attribute cum_table to None\n2020-09-11 13:27:10,461:INFO:loaded models/w2v_300_1.txt\n100%|██████████| 7189/7189 [00:00<00:00, 37094.49it/s]\n2020-09-11 13:27:10,711:INFO:loading Word2Vec object from models/w2v_300_3.txt\n2020-09-11 13:27:10,877:INFO:loading wv recursively from models/w2v_300_3.txt.wv.* with mmap=None\n2020-09-11 13:27:10,880:INFO:loading vectors from models/w2v_300_3.txt.wv.vectors.npy with mmap=None\nnull cnt 5\n2020-09-11 13:27:10,933:INFO:setting ignored attribute vectors_norm to None\n2020-09-11 13:27:10,937:INFO:loading vocabulary recursively from models/w2v_300_3.txt.vocabulary.* with mmap=None\n2020-09-11 13:27:10,940:INFO:loading trainables recursively from models/w2v_300_3.txt.trainables.* with mmap=None\n2020-09-11 13:27:10,943:INFO:loading syn1neg from models/w2v_300_3.txt.trainables.syn1neg.npy with mmap=None\n2020-09-11 13:27:11,001:INFO:setting ignored attribute cum_table to None\n2020-09-11 13:27:11,004:INFO:loaded models/w2v_300_3.txt\n100%|██████████| 28458/28458 [00:00<00:00, 38600.47it/s]\nnull cnt 1202\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "31"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "text_1_list = np.unique(train_data['text_1'])\n",
    "text_3_list = np.unique(train_data['text_2'])\n",
    "\n",
    "print('开始序列化')\n",
    "x1, index_1, token_1 = set_tokenizer(train_data['text_1'], split_char=' ', max_len=30)\n",
    "x3, index_3, token_3 = set_tokenizer(train_data['text_2'], split_char=' ', max_len=600)\n",
    "print('序列化完成')\n",
    "gc.collect()\n",
    "\n",
    "trian_save_word2vec(text_1_list, save_name='models/w2v_300_1.txt', split_char=' ')\n",
    "gc.collect()\n",
    "trian_save_word2vec(text_3_list, save_name='models/w2v_300_3.txt', split_char=' ')\n",
    "gc.collect()\n",
    "\n",
    "# 得到emb矩阵\n",
    "emb1 = get_embedding_matrix(index_1, Emed_path='models/w2v_300_1.txt')\n",
    "emb3 = get_embedding_matrix(index_3, Emed_path='models/w2v_300_3.txt')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建抽取模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from keras.initializers import *\n",
    "\n",
    "def model_conv(emb1, emb3):\n",
    "    '''\n",
    "    注意这个inputs\n",
    "    seq1、seq2分别是两个输入\n",
    "    是否做emb可选可不选，\n",
    "    这个就是我们之前训练已经得到的用于embedding的（embedding_matrix1， embedding_matrix2）\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=30,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=600,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    seq1 = Input(shape=(30,))\n",
    "    seq3 = Input(shape=(600,))    \n",
    "    \n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    \n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x3 = sdrop(x3)\n",
    "     \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x1))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x3))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    \n",
    "    x = Multiply()([merged_1, merged_3])\n",
    "    \n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(x))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    pred_1 = Dense(10, activation='softmax')(x)\n",
    "    pred_2 = Dense(3, activation='softmax')(x)\n",
    "    pred_3 = Dense(3, activation='softmax')(x)\n",
    "    pred_4 = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=[seq1, seq3], outputs=[pred_1, pred_2, pred_3, pred_4])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0001),metrics=[\"accuracy\"])\n",
    "    return model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 30)]         0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 600)]        0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 30, 300)      2157000     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 600, 300)     8537700     input_2[0][0]                    \n__________________________________________________________________________________________________\nspatial_dropout1d (SpatialDropo multiple             0           embedding[0][0]                  \n                                                                 embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 30, 256)      330240      spatial_dropout1d[0][0]          \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 600, 256)     330240      spatial_dropout1d[1][0]          \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 30, 256)      0           bidirectional[0][0]              \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 600, 256)     0           bidirectional_1[0][0]            \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, 30, 100)      25700       dropout[0][0]                    \n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, 600, 100)     25700       dropout_1[0][0]                  \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 100)          0           time_distributed[0][0]           \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 100)          0           time_distributed_1[0][0]         \n__________________________________________________________________________________________________\nmultiply (Multiply)             (None, 100)          0           lambda[0][0]                     \n                                                                 lambda_1[0][0]                   \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1000)         101000      multiply[0][0]                   \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 1000)         4000        dense_2[0][0]                    \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 1000)         0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 1000)         0           activation[0][0]                 \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 500)          500500      dropout_2[0][0]                  \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 500)          2000        dense_3[0][0]                    \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 500)          0           batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 10)           5010        activation_1[0][0]               \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 3)            1503        activation_1[0][0]               \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 3)            1503        activation_1[0][0]               \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 2)            1002        activation_1[0][0]               \n==================================================================================================\nTotal params: 12,023,098\nTrainable params: 1,325,398\nNon-trainable params: 10,697,700\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model = model_conv(emb1, emb3)\n",
    "model.summary()\n",
    "l1 = to_categorical(train_data['label_1'], 10)\n",
    "l2 = to_categorical(train_data['label_2'], 3)\n",
    "l3 = to_categorical(train_data['label_3'], 3)\n",
    "l4 = to_categorical(train_data['label_4'], 2)\n",
    "# model.fit([x1, x3],[l1, l2, l3, l4], batch_size=256, epochs=8, verbose=1, shuffle=True)\n",
    "model.load_weights('models\\lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存权重\n",
    "model.save_weights('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.保存结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'val_result' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-26ce19f84820>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 预测验证集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mval_result_for_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财产品名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品发行方名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_result' is not defined"
     ]
    }
   ],
   "source": [
    "# 预测验证集\n",
    "val_df[\"sample_id\"]=val_df[\"sample_id\"].astype(str)\n",
    "val_result_for_pred = pd.merge(val_result, val_df, on='sample_id', how='left')\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['理财产品名称'].astype(str) + '_' + val_result_for_pred['产品发行方名称'].astype(str)\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text'].astype(str)\n",
    "\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "\n",
    "x1 = token_1.texts_to_sequences(val_result_for_pred['text_1'])\n",
    "x1 = pad_sequences(x1, maxlen=30, value=0)\n",
    "x3 = token_3.texts_to_sequences(val_result_for_pred['text_2'])\n",
    "x3 = pad_sequences(x3, maxlen=600, value=0)\n",
    "pred_result = model.predict([x1, x3], batch_size=1024, verbose=1)\n",
    "pred_1 = label_1.inverse_transform(np.argmax(pred_result[0], axis=1))\n",
    "pred_2 = label_2.inverse_transform(np.argmax(pred_result[1], axis=1))\n",
    "pred_3 = label_3.inverse_transform(np.argmax(pred_result[2], axis=1))\n",
    "pred_4 = label_4.inverse_transform(np.argmax(pred_result[3], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pred_1' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-33bb5074b888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财类型'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'资金来源'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司和上市公司关系'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'买卖方是否有关联关系'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred_1' is not defined"
     ]
    }
   ],
   "source": [
    "val_result['理财类型'] = pred_1\n",
    "val_result['资金来源'] = pred_2\n",
    "val_result['实际购买公司和上市公司关系'] = pred_3\n",
    "val_result['买卖方是否有关联关系'] = pred_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.离线验证评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'认购日期'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2645\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2646\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '认购日期'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9836af1d8ef2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mval_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'认购日期'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财产品名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财类型'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'认购金额(万元)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品起息日'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品到息日'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品期限'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'资金来源'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'实际购买公司和上市公司关系'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'买卖方是否有关联关系'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'公告日期'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_F1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2798\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2799\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2800\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2646\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2647\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2648\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '认购日期'"
     ]
    }
   ],
   "source": [
    "# 计算线下f1\n",
    "# R（召回率）=抽取正确的记录数量/（抽取正确的目标数量+漏抽取的记录数量）\n",
    "# P（准确率）=抽取正确的记录数量/（抽取错误的记录数量+抽取正确的记录数量）\n",
    "def get_F1(val_pred, val_true):\n",
    "    val_pred = list(val_pred)\n",
    "    val_true = list(val_true)\n",
    "    curr = list(set(val_pred).intersection(set(val_true)))\n",
    "    R = len(curr)/len(val_true)\n",
    "    P = len(curr)/len(val_pred)\n",
    "    return 2*P*R/(P+R)\n",
    "\n",
    "train_outputs[\"sample_id\"]=train_outputs[\"sample_id\"].astype(str)\n",
    "r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "val_true = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['理财类型'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) + r['资金来源'].astype(str) + r['实际购买公司名称'].astype(str) + r['实际购买公司和上市公司关系'].astype(str) + r['买卖方是否有关联关系'].astype(str) + r['公告日期'].astype(str)\n",
    "\n",
    "r = val_result\n",
    "val_pred = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['理财类型'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) + r['资金来源'].astype(str) + r['实际购买公司名称'].astype(str) + r['实际购买公司和上市公司关系'].astype(str) + r['买卖方是否有关联关系'].astype(str) + r['公告日期'].astype(str)\n",
    "\n",
    "score = get_F1(val_pred, val_true)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-24e817b98e00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mval_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_pred_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_true_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_true_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m ) -> \"DataFrame\":\n\u001b[1;32m---> 73\u001b[1;33m     op = _MergeOperation(\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# to avoid incompat dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;31m# If argument passed to validate,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1144\u001b[0m                     \u001b[0minferred_right\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minferred_left\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m                 ):\n\u001b[1;32m-> 1146\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m             \u001b[1;31m# datetimelikes must match exactly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on int64 and object columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "val_true_file=pl.Path(\"results/val_true.csv\")\n",
    "val_pred_file=pl.Path(\"results/val_pred.csv\")\n",
    "\n",
    "tr_true_file=pl.Path(\"results/tr_true.csv\")\n",
    "val_result.copy().sort_values(by=\"sample_id\").sort_index(axis=1).to_csv(val_pred_file,index=None)\n",
    "pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left').sort_index(axis=1).sort_values(by=\"sample_id\").to_csv(val_true_file,index=None)\n",
    "pd.merge(train_df[['sample_id']], train_outputs, on='sample_id', how='left').sort_index(axis=1).sort_values(by=\"sample_id\").to_csv(tr_true_file,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.最终输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_result' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2e1b40a56c0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 预测测试集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_result_for_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sample_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'理财产品名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'产品发行方名称'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_result_for_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_result' is not defined"
     ]
    }
   ],
   "source": [
    "# 预测测试集\n",
    "test_df[\"sample_id\"]=test_df[\"sample_id\"].astype(str)\n",
    "test_result_for_pred = pd.merge(test_result, test_df, on='sample_id', how='left')\n",
    "test_result_for_pred['text_1'] = test_result_for_pred['理财产品名称'].astype(str) + '_' + test_result_for_pred['产品发行方名称'].astype(str)\n",
    "test_result_for_pred['text_2'] = test_result_for_pred['text'].astype(str)\n",
    "\n",
    "test_result_for_pred['text_1'] = test_result_for_pred['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "test_result_for_pred['text_2'] = test_result_for_pred['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "\n",
    "x1 = token_1.texts_to_sequences(test_result_for_pred['text_1'])\n",
    "x1 = pad_sequences(x1, maxlen=30, value=0)\n",
    "x3 = token_3.texts_to_sequences(test_result_for_pred['text_2'])\n",
    "x3 = pad_sequences(x3, maxlen=600, value=0)\n",
    "pred_result = model.predict([x1, x3], batch_size=1024, verbose=1)\n",
    "pred_1 = label_1.inverse_transform(np.argmax(pred_result[0], axis=1))\n",
    "pred_2 = label_2.inverse_transform(np.argmax(pred_result[1], axis=1))\n",
    "pred_3 = label_3.inverse_transform(np.argmax(pred_result[2], axis=1))\n",
    "pred_4 = label_4.inverse_transform(np.argmax(pred_result[3], axis=1))\n",
    "\n",
    "test_result['理财类型'] = pred_1\n",
    "test_result['资金来源'] = pred_2\n",
    "test_result['实际购买公司和上市公司关系'] = pred_3\n",
    "test_result['买卖方是否有关联关系'] = pred_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_result' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8d62d995a84d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results/re_lstm_base.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'results/re_lstm_base.xlsx'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gb18030\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_result' is not defined"
     ]
    }
   ],
   "source": [
    "test_result.to_csv('results/re_lstm_base.csv',encoding=\"utf8\", index=False)\n",
    "test_result.to_excel('results/re_lstm_base.xlsx',encoding=\"gb18030\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "files_num=int(test_df.shape[0]/4)\n",
    "test_time[\"sample_id\"]=test_time[\"sample_id\"].astype(str)\n",
    "\n",
    "# test_time.shape\n",
    "# test_time.dropna(subset=[\"公告日期\"]).shape\n",
    "import random\n",
    "for i in range(1,20):\n",
    "    random_seed=random.randint(0,10000)\n",
    "    random_test_df = test_df.sample(frac=1, random_state=random_seed)\n",
    "    change_df=random_test_df[:files_num]\n",
    "    stable_df=random_test_df[files_num:]\n",
    "    random_test_time=pd.merge(change_df,test_time,on=[\"sample_id\"])\n",
    "    random_test_time.shape\n",
    "    random_test_time=random_test_time.dropna(subset=[\"公告日期\"])\n",
    "    random_test_time.shape\n",
    "    random_test_time[\"公告日期\"]=random_test_time[\"公告日期\"].map(lambda x:datetime.datetime.strftime(datetime.datetime.strptime(str(x.replace(\"\\r\",\"\")), '%Y-%m-%d'),'%Y-%m-%d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-340577d4a7ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4663\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# sample_id=7123\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0msample_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"tabel\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtable_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstart_rows_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfirst_line_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproduct_df_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow_combine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;31m# validate the location\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2138\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   2061\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2064\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "sample_id=4663\n",
    "# sample_id=7123\n",
    "tabel=val_df[val_df[\"sample_id\"]==sample_id][\"tabel\"].iloc[0]\n",
    "table_result,start_rows_list,first_line_list,product_df_list=row_combine(sample_id,tabel)\n",
    "index=0\n",
    "table_result[index]\n",
    "product_df_list_ele=product_df_list[index].reset_index(drop=True)\n",
    "product_df_list_ele\n",
    "# product_df_list_ele\n",
    "each_sum_rows=get_each_product_row(table_result[index],product_df_list_ele.reset_index(drop=True))\n",
    "\n",
    "each_sum_rows2=result_matrix[result_matrix[\"sample_id\"]==sample_id][\"product_df\"].iloc[0]\n",
    "# # each_sum_rows\n",
    "each_sum_rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}