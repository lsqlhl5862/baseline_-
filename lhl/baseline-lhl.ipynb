{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from datetime import datetime \n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from tqdm.autonotebook import *\n",
    "import pdfplumber\n",
    "tqdm.pandas()\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.append(\"..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF解析原始数据 \n",
    "## 加载数据并采用pdfplumber抽取PDF中的文字和表格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据准备(train_output文件中格式有点问题，需要提前用excel或者wps打开然后另存为excel文件)\n",
    "train_outputs = pd.read_excel('../datasets/train_output.xlsx')\n",
    "\n",
    "# 获取pdf中文字和表格\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index_page in np.arange(0, len(pdf.pages), 1):\n",
    "            # 读取多页\n",
    "            page = pdf.pages[index_page]   # 第n页的信息\n",
    "            text = page.extract_text()\n",
    "            text_list.append(text)\n",
    "            table = page.extract_tables()\n",
    "            for t in table:\n",
    "                table_list.append(t)\n",
    "    return text_list, table_list\n",
    "\n",
    "def get_dir_file(path):\n",
    "    '''\n",
    "    输入文件夹位置，输出整理好的dataframe\n",
    "    '''\n",
    "    path_list = os.listdir(path)\n",
    "    id_list = []\n",
    "    file_path_list = []\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    for i in tqdm(path_list):\n",
    "        if '.PDF' in i:\n",
    "            file_path = path + i\n",
    "            id_list.append(int(i.split('.')[0]))\n",
    "            file_path_list.append(file_path)\n",
    "            try:\n",
    "                text_temp, table_temp = extract_pdf_content(file_path)\n",
    "            except Exception:\n",
    "                print('此pdf无法读取')\n",
    "                text_temp, table_temp = [], []\n",
    "            text_list.append(text_temp)\n",
    "            table_list.append(table_temp)\n",
    "            \n",
    "    df = pd.DataFrame()\n",
    "    df['sample_id'] = id_list\n",
    "    df['file_path'] = file_path_list\n",
    "    df['text'] = text_list\n",
    "    df['tabel'] = table_list\n",
    "    df = df.sort_values('sample_id')\n",
    "    return df\n",
    "\n",
    "# 文件处理太慢，可持续化保存文件\n",
    "train_path = '../datasets/train.csv'\n",
    "if os.path.exists(train_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "else:\n",
    "    train_df = get_dir_file('datasets/train_data/')\n",
    "    train_df.to_csv(train_path,index=False)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "test_path =  '../datasets/test.csv'\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "else:\n",
    "    test_df = get_dir_file('datasets/test_data/')\n",
    "    test_df.to_csv(test_path,index=False)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_outputs.head(2)\n",
    "train_df.head(2)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集验证集\n",
    "train_df = train_df.sample(frac=1, random_state=1017)\n",
    "val_df = train_df[:1800]\n",
    "train_df = train_df[1800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "## 抽取整体数据（一个sampleid内此字段内容都相同）\n",
    "## 公告时间，实际购买公司"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.抽取公告时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 首先针对任务抽取时间（每个时间跟每个id是一一对应的）\n",
    "# 要不是取第一个时间，要不就是取最后一个时间（或者时间加一）这里可以建立一个模型预测\n",
    "# base这里面直接取最后一个时间作为发布日期\n",
    "\n",
    "CN_NUM = {\n",
    "    u'〇': 0, u'一': 1, u'二': 2, u'三': 3,\n",
    "    u'四': 4, u'五': 5, u'六': 6, u'七': 7,\n",
    "    u'八': 8, u'九': 9, u'零': 0, u'壹': 1,\n",
    "    u'贰': 2, u'叁': 3, u'肆': 4, u'伍': 5,\n",
    "    u'陆': 6, u'柒': 7, u'捌': 8, u'玖': 9,\n",
    "    u'貮': 2, u'两': 2,\n",
    "}\n",
    "\n",
    "\n",
    "def get_put_time_from_text(row):\n",
    "    row = row.replace(' ', '').replace('\\\\n', '')\n",
    "    for key in CN_NUM:\n",
    "        row = row.replace(key, str(CN_NUM[key]))   \n",
    "    r = row.replace(\"年\", \"-\").replace(\"月\", \"-\").replace(\"日\", \" \").replace(\"/\", \"-\").strip()\n",
    "    regex = \"(\\d{4}-\\d{1,2}-\\d{1,2})\"\n",
    "    r = re.findall(regex, r)\n",
    "    if len(r)==0:\n",
    "        return np.nan\n",
    "    time_str = r[-1]\n",
    "    first = time_str.split('-')[0]\n",
    "    second = time_str.split('-')[1]\n",
    "    last = time_str.split('-')[-1]\n",
    "    second = str.zfill(second, 2)\n",
    "    last = str.zfill(last, 2)\n",
    "    r = '-'.join([first, second, last])\n",
    "    return r\n",
    "\n",
    "val_result = pd.DataFrame()\n",
    "val_result['sample_id'] = val_df['sample_id']\n",
    "val_result['predict_time'] = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "test_gg = train_outputs.groupby('sample_id').apply(lambda row:list(row['公告日期'])[0]).reset_index()\n",
    "test_gg.columns = ['sample_id', 'time']\n",
    "val_result = pd.merge(val_result, test_gg, on='sample_id', how='left')\n",
    "\n",
    "# 判断验证集的准确率\n",
    "np.sum(val_result['predict_time'].astype(str) == val_result['time'].astype(str))/len(val_result)\n",
    "\n",
    "val_time = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "# test_time = test_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.抽取实际购买公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 抽取购买公司\n",
    "# 前几句话出现\n",
    "# 将其按照\\\\n 和空格切割\n",
    "def get_gm(row):\n",
    "    result = re.split('[\\\\\\\\n ]',row)\n",
    "    for i in result:\n",
    "        if '公司' in i:\n",
    "            return i\n",
    "\n",
    "val_gm = val_df.progress_apply(lambda row:get_gm(row['text']), axis=1)\n",
    "# test_gm = test_df.progress_apply(lambda row:get_gm(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.清洗提取出来的tabel数据，主要是清洗掉有问题的列 \n",
    "# 重写清洗方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 将table转换格式以及处理\n",
    "def deal_tabel(row):\n",
    "    row = eval(row)\n",
    "    if len(row)==0:\n",
    "        return []\n",
    "    else:\n",
    "        new_row = []\n",
    "        for i in row:\n",
    "            for d in i:\n",
    "                new_temp = []\n",
    "                for h in d:\n",
    "                    # 这里处理空数据或者错误的数据\n",
    "                    h = str(h).replace('None', '').replace('\\n','').replace(' ', '')                    \n",
    "                    if h=='':\n",
    "                        continue\n",
    "                    if h=='.':\n",
    "                        continue\n",
    "                    if h=='/':\n",
    "                        continue\n",
    "                    new_temp.append(h)\n",
    "                new_row.append(new_temp)\n",
    "        # 这里判断是否构成一个完整得认购数据(通过一个list进行判断)\n",
    "        new_new_row = []\n",
    "        for i in new_row:\n",
    "            if len(i) == 0:\n",
    "                continue\n",
    "            elif len(i) <= 4:\n",
    "                continue\n",
    "            else:\n",
    "                new_new_row.append(i)\n",
    "        return new_new_row\n",
    "# train_df_tabel = train_df['tabel'].progress_apply(lambda row:deal_tabel(row))\n",
    "val_df_tabel = val_df['tabel'].progress_apply(lambda row:deal_tabel(row))\n",
    "\n",
    "# test_df_tabel = test_df['tabel'].progress_apply(lambda row:deal_tabel(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 将table转换格式以及处理\n",
    "def deal_tabel(row):\n",
    "    row = eval(row)\n",
    "    if len(row)==0:\n",
    "        return []\n",
    "    else:\n",
    "        new_row = []\n",
    "        for i in row:\n",
    "            for d in i:\n",
    "                new_temp = []\n",
    "                for h in d:\n",
    "                    # 这里处理空数据或者错误的数据\n",
    "                    h = str(h).replace('None', '').replace('\\n','').replace(' ', '')                    \n",
    "                    if h=='':\n",
    "                        continue\n",
    "                    if h=='.':\n",
    "                        continue\n",
    "                    if h=='/':\n",
    "                        continue\n",
    "                    new_temp.append(h)\n",
    "                new_row.append(new_temp)\n",
    "        # 这里判断是否构成一个完整得认购数据(通过一个list进行判断)\n",
    "        new_new_row = []\n",
    "        for i in new_row:\n",
    "            if len(i) == 0:\n",
    "                continue\n",
    "            elif len(i) <= 4:\n",
    "                continue\n",
    "            else:\n",
    "                new_new_row.append(i)\n",
    "        return new_new_row\n",
    "# train_df_tabel = train_df['tabel'].progress_apply(lambda row:deal_tabel(row))\n",
    "val_df_tabel = val_df['tabel'].progress_apply(lambda row:deal_tabel(row))\n",
    "\n",
    "# test_df_tabel = test_df['tabel'].progress_apply(lambda row:deal_tabel(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_table=pd.concat([val_df[\"sample_id\"],pd.DataFrame(val_df_tabel)],axis=1)\n",
    "tmp_table[\"len_table\"]=tmp_table[\"tabel\"].apply(lambda x:[len(i) for i in x])\n",
    "# tmp_table.to_excel(\"table矩阵检验1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#大量PDF的理财购买信息不在table中，而是在text中，此段为判断理财信息是否只需要从text中提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_num_char=[\"一、\",\"二、\",\"三、\",\"四、\",\"五、\",\"六、\",\"七、\",\"八、\",\"九、\",\"十、\",\"十一、\",\"十二、\",\"十三、\",\"十四、\",\"十五、\"]\n",
    "s_title_num_char=[\"（一）\",\"（二）\",\"（三）\",\"（四）\",\"（五）\",\"（六）\",\"（七）\",\"（八）\",\"（九）\",\"（十）\",\"（十一）\",\"（十二）\",\"（十三）\",\"（十四）\",\"（十五）\"]\n",
    "s_title_num_char.extend([\"[(]一[)]\",\"[(]二[)]\",\"[(]三[)]\",\"[(]四[)]\",\"[(]五[)]\",\"[(]六[)]\",\"[(]七[)]\",\"[(]八[)]\",\"[(]九[)]\",\"[(]十[)]\",\"[(]十一[)]\",\"[(]十二[)]\",\"[(]十三[)]\",\"[(]十四[)]\",\"[(]十五[)]\"])\n",
    "\n",
    "title_pos_words=[]\n",
    "title_neg_words=[\"备查\",\"日前\",\"过去\",\"履行\",\"审批\",\"程序\",\"风险\",\"措施\",\"影响\",\"累计\",\"赎回\",\"到期\",\"截至\"]\n",
    "\n",
    "\n",
    "def get_title(text):\n",
    "    global title_num_char\n",
    "    title_list=[]\n",
    "    title_type_list=[]\n",
    "    text_start_iter_list=[]\n",
    "    text_end_iter_list=[]\n",
    "    for item in title_num_char:\n",
    "        pattern = re.compile(item+r\"[ ]*?[^ ]+?[ ]\")\n",
    "        tmp=pattern.finditer(text)\n",
    "        for i in tmp:\n",
    "            title_list.append(i.group())\n",
    "            text_start_iter_list.append(i.span(0)[0])\n",
    "            title_type_list.append(1)\n",
    "            text_end_iter_list.append(i.span(0)[1])\n",
    "    \n",
    "    # for item in title_list:\n",
    "    for item in s_title_num_char:\n",
    "        pattern = re.compile(item+r\"[ ]*?[^ ]+?[ ]\")\n",
    "        tmp=pattern.finditer(text)\n",
    "        for i in tmp:\n",
    "            title_list.append(i.group())\n",
    "            text_start_iter_list.append(i.span(0)[0])\n",
    "            title_type_list.append(2)\n",
    "            text_end_iter_list.append(i.span(0)[1])\n",
    "\n",
    "    title_list.append(\"引言\")\n",
    "    title_type_list.append(1)\n",
    "    text_start_iter_list.append(0)\n",
    "    text_end_iter_list.append(0)\n",
    "\n",
    "    result_df=pd.DataFrame([title_list,title_type_list,text_start_iter_list,text_end_iter_list]).T.sort_values(by=2).reset_index(drop=True)\n",
    "    # print(result_df)\n",
    "    return result_df\n",
    "\n",
    "def get_title_text(text,title_df):\n",
    "    # print(title_df)\n",
    "    title_1_df=title_df[title_df[1]==1]\n",
    "    text_iter_list=[]\n",
    "    text_list=[]\n",
    "    # print(title_1_df)\n",
    "    for iter1,iter2 in title_1_df[[2,3]].values:\n",
    "        # print(iter1)\n",
    "        if(len(text_iter_list)!=0):\n",
    "            text_iter_list.append(iter1)\n",
    "        text_iter_list.append(iter2)\n",
    "    # text_iter_list.append(text_iter_list[len(text_iter_list)-1])\n",
    "    text_iter_list.append(len(text))\n",
    "    for index in range(int(len(text_iter_list)/2)):\n",
    "        text_list.append(text[text_iter_list[2*index]:text_iter_list[2*index+1]])\n",
    "    \n",
    "    title_1_df[4]=text_list\n",
    "\n",
    "    return title_1_df.reset_index(drop=True)\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "def judge_title(sample_id=0,text=r\"test\\n\"):\n",
    "    # print(text)\n",
    "    text=text.replace(r\"\\n\",\"\")\n",
    "    title_df=get_title(text)\n",
    "    title_df[\"sample_id\"]=[sample_id for x in range(title_df.shape[0])]\n",
    "    # print(title_df)\n",
    "    title_1_df=get_title_text(text,title_df)[[\"sample_id\",0,1,2,3,4]]\n",
    "\n",
    "    \n",
    "\n",
    "    global val_df\n",
    "    global train_outputs\n",
    "    val_true_name=train_outputs[train_outputs[\"sample_id\"]==sample_id][\"理财产品名称\"]\n",
    "    \n",
    "    index=0\n",
    "    neg_index=[]\n",
    "    for title_des in title_1_df[0].values:\n",
    "        for item in title_neg_words:\n",
    "            if re.search(item,title_des) is not None:\n",
    "                neg_index.append(index)\n",
    "                break\n",
    "        index+=1\n",
    "\n",
    "\n",
    "    return title_1_df.drop(neg_index)\n",
    "    # print(title_list)\n",
    "\n",
    "judge_title_result=None\n",
    "\n",
    "\n",
    "for sample_id,text in tqdm(val_df[[\"sample_id\",\"text\"]].values):\n",
    "    # print(sample_id)\n",
    "    # print(text)\n",
    "    judge_title_result= judge_title(sample_id,text) if judge_title_result is None else pd.concat([judge_title_result,judge_title(sample_id,text)])\n",
    "\n",
    "# judge_title_result.to_excel(\"训练集段落标题分类结果.xlsx\",index=None)\n",
    "\n",
    "\n",
    "# is_from_text(val_df[val_df[\"sample_id\"]==930][\"sample_id\"].iloc[0],val_df[val_df[\"sample_id\"]==930][\"text\"].iloc[0])\n",
    "# is_from_text(val_df[val_df[\"sample_id\"]==125][\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabel_tmp_value=eval(val_df[val_df[\"sample_id\"]==35][\"tabel\"].iloc[0])\n",
    "# # tabel_tmp_value_list=[]\n",
    "# table_value_df=pd.DataFrame(tabel_tmp_value)\n",
    "val_df.loc[1419].shape[0]\n",
    "# tmp2=val_df.head(51).index\n",
    "# list(tmp1)\n",
    "# np.array_equal(tmp1,tmp2)\n",
    "# set(tmp2).issubset(tmp1)\n",
    "# set(tmp2).difference(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tabel_tmp_value=eval(val_df[val_df[\"sample_id\"]==4333][\"tabel\"].iloc[0])\n",
    "# tabel_tmp_value=eval(val_df[val_df[\"sample_id\"]==35][\"tabel\"].iloc[0])\n",
    "# tabel_tmp_value_list=[]\n",
    "# table_value_df=pd.DataFrame(tabel_tmp_value[0])\n",
    "# table_value_df.head(20)\n",
    "# for item in tabel_tmp_value:\n",
    "\n",
    "column_name_judge_word=[\"民币）\",\"民币)\",\"万元)\",\"元)\",\"万元）\",\"元）\",\"%)\",\"%）\",\"金额\"]\n",
    "\n",
    "\n",
    "#寻找（字段）所在的理论最后一行\n",
    "def find_amt(df):\n",
    "    global column_name_judge_word\n",
    "    max_index=-1\n",
    "    df=df.head(7)\n",
    "    for index in range(df.shape[0]):\n",
    "        for item in column_name_judge_word:\n",
    "            flag=[]\n",
    "            flag_num=[]\n",
    "            df.loc[index].map(lambda x:flag_num.append(re.match(\"[ ]*?\\d\",x)))\n",
    "            df.loc[index].map(lambda x:flag.append((item in x)))\n",
    "            for i in flag_num:\n",
    "                if(i is not None):\n",
    "                    return max_index\n",
    "            if True in flag:\n",
    "                max_index=index\n",
    "    return max_index\n",
    "\n",
    "column_name_judge_word=[\"民币）\",\"民币)\",\"万元)\",\"元)\",\"万元）\",\"元）\",\"%)\",\"%）\",\"金额\"]\n",
    "\n",
    "\n",
    "#寻找单个理财产品所在的理论最后一行\n",
    "def find_product(df):\n",
    "    global column_name_judge_word\n",
    "    max_index=-1\n",
    "    df=df.head(7)\n",
    "    for index in range(df.shape[0]):\n",
    "        for item in column_name_judge_word:\n",
    "            flag=[]\n",
    "            flag_num=[]\n",
    "            df.loc[index].map(lambda x:flag_num.append(re.match(\"[ ]*?\\d\",x)))\n",
    "            df.loc[index].map(lambda x:flag.append((item in x)))\n",
    "            for i in flag_num:\n",
    "                if(i is not None):\n",
    "                    return max_index\n",
    "            if True in flag:\n",
    "                max_index=index\n",
    "    return max_index\n",
    "\n",
    "def get_valid_columns_num(df):\n",
    "    df=df.T.dropna(axis=0).apply(lambda x:x.replace(\" \",\"\"))\n",
    "    return df[df!=\"\"].shape[0]\n",
    "\n",
    "def get_valid_columns_index(df):\n",
    "    df=df.T.dropna(axis=0).apply(lambda x:x.replace(\" \",\"\"))\n",
    "    return df[df!=\"\"].index\n",
    "\n",
    "def field_location_optimization(columns_list,df):\n",
    "    if(df.shape[1]!=columns_list.shape[0]):\n",
    "        return None\n",
    "    max_columns_num=get_valid_columns_num(columns_list)\n",
    "    if(df.shape[0]==0):\n",
    "        return df\n",
    "    valid_clomuns_index=list(get_valid_columns_index(columns_list))\n",
    "    # print(valid_clomuns_index)\n",
    "    result_df=df.head(0)\n",
    "    # print(result_df.shape)\n",
    "    # print(df)\n",
    "    for index in range(df.shape[0]):\n",
    "        tmp_row=df.loc[index]\n",
    "        tmp_index=get_valid_columns_index(tmp_row)\n",
    "        # print(list(tmp_index))\n",
    "        if set(tmp_index).issubset(valid_clomuns_index):\n",
    "            pass\n",
    "        else:\n",
    "            difference_list=list(set(tmp_index).difference(valid_clomuns_index))\n",
    "            \n",
    "            location_list=[]\n",
    "            tmp_difference_list=difference_list.copy()\n",
    "            for item in difference_list:\n",
    "                location=item\n",
    "                #前后浮动位移（可优化）\n",
    "                for i in range(1,3):\n",
    "                    location=(item-i)\n",
    "                    if location in valid_clomuns_index and location not in tmp_difference_list:\n",
    "                        location_list.append(location)\n",
    "                        tmp_difference_list.remove(item)\n",
    "                        tmp_difference_list.append(location)\n",
    "                        break\n",
    "                    location=(item+i)\n",
    "                    if location in valid_clomuns_index and location not in tmp_difference_list:\n",
    "                        location_list.append(location)\n",
    "                        tmp_difference_list.remove(item)\n",
    "                        tmp_difference_list.append(location)\n",
    "                        break\n",
    "            result_location=list(df.columns)\n",
    "            # print(location_list)\n",
    "            # print(difference_list)\n",
    "            # print(\"---------------------\")\n",
    "            for i in range(len(location_list)):\n",
    "                result_location[difference_list[i]]=location_list[i]\n",
    "                result_location[location_list[i]]=difference_list[i]\n",
    "            # print(tmp_row)\n",
    "            tmp_row=tmp_row.iloc[result_location].reset_index(drop=True)\n",
    "            # print(tmp_row)\n",
    "            # print(result_location)\n",
    "        result_df=tmp_row.to_frame().T if result_df is None else pd.concat([result_df,tmp_row.to_frame().T])\n",
    "    # print(np.array_equal(get_valid_columns_index(columns_list),get_valid_columns_index(result_df)))\n",
    "    # print(list(set(get_valid_columns_index(columns_list)).difference(result_df)))\n",
    "    # print(result_df)\n",
    "    # print(result_df.shape)\n",
    "    return result_df\n",
    "def row_combine(sample_id,pdf_table):\n",
    "# 将字符串转化成多维列表\n",
    "    pdf_table=eval(pdf_table)\n",
    "\n",
    "    table_result=[]\n",
    "    start_rows_list=[]\n",
    "    first_line_list=[]\n",
    "    product_df_list=[]\n",
    "    for item in pdf_table:\n",
    "        #考虑加入一个判定表格位置的方法（暂缺）\n",
    "\n",
    "        tmp_table_df=pd.DataFrame(item)\n",
    "        tmp_table_df=tmp_table_df.fillna(\"\").applymap(lambda x: x.replace(\"\\n\",\"\"))\n",
    "        #清除全空行\n",
    "        drop_index_list=[]\n",
    "        for index in range(tmp_table_df.shape[0]):\n",
    "            judge=[]\n",
    "            noshow=tmp_table_df.loc[index].map(lambda x:judge.append(x==\"\"))\n",
    "            if(False not in judge):\n",
    "                drop_index_list.append(index)\n",
    "        \n",
    "        tmp_table_df=tmp_table_df.drop(drop_index_list).reset_index(drop=True)\n",
    "\n",
    "        base_row=find_amt(tmp_table_df)\n",
    "        if base_row==-1:\n",
    "            if(len(table_result)>0):\n",
    "                product_df=field_location_optimization(table_result[len(table_result)-1],tmp_table_df.loc[:].reset_index(drop=True))\n",
    "                if(product_df is not None):\n",
    "                    len_product_df_list=len(product_df_list)-1\n",
    "                    product_df_list[len_product_df_list]=pd.concat([product_df_list[len_product_df_list],product_df])\n",
    "                    # print(product_df.shape)\n",
    "                else:\n",
    "                    pass\n",
    "            continue\n",
    "\n",
    "        tmp_row=None\n",
    "        valid_columns_nums=[]\n",
    "        len_df=None\n",
    "        for index in range(0,base_row+1):\n",
    "            len_list=tmp_table_df.loc[index].map(lambda x:len(x))\n",
    "            for i in range(len(len_list)):\n",
    "                if len_list[i]==0 and len_df is not None:\n",
    "                    len_list[i]=len_df.tail(1)[i]\n",
    "            len_df=pd.DataFrame(len_list).T if len_df is None else pd.concat([len_df,pd.DataFrame(len_list).T])\n",
    "            row_result=tmp_table_df.loc[index].map(lambda x:re.sub(r\"$[ ]+?\",\"\",re.sub(r\"^[ ]+?\",\"\",x)))\n",
    "            tmp_row= row_result if tmp_row is None else tmp_row+row_result\n",
    "            # valid_columns_nums.append(get_valid_columns_num(tmp_row))\n",
    "        \n",
    "        start_row=base_row+1\n",
    "\n",
    "        for index in range(base_row+1,tmp_table_df.shape[0]):\n",
    "            len_list=tmp_table_df.loc[index].map(lambda x:len(x))\n",
    "            len_judge=pd.DataFrame(len_list).T-len_df.loc[len_df.shape[0]-1]\n",
    "            len_judge=len_judge.T\n",
    "            if(len_judge[len_judge>0].shape[0]>0):\n",
    "                break\n",
    "            start_row+=1\n",
    "            for i in range(len(len_list)):\n",
    "                if len_list[i]==0 and len_df is not None:\n",
    "                    len_list[i]=len_df.tail(1)[i]\n",
    "            len_df=pd.DataFrame(len_list).T if len_df is None else pd.concat([len_df,pd.DataFrame(len_list).T])\n",
    "            row_result=tmp_table_df.loc[index].map(lambda x:re.sub(r\"$[ ]+?\",\"\",re.sub(r\"^[ ]+?\",\"\",x)))\n",
    "            tmp_row= row_result if tmp_row is None else tmp_row+row_result\n",
    "            # valid_columns_nums.append(get_valid_columns_num(tmp_row))\n",
    "        \n",
    "        columns_list=tmp_row.dropna().map(lambda x:x.replace(\" \",\"\"))\n",
    "        \n",
    "        #处理错列问题\n",
    "        product_df=field_location_optimization(columns_list,tmp_table_df.loc[start_row:].reset_index(drop=True))\n",
    "        \n",
    "        first_line=[]\n",
    "        tmp_table_df.head(2).applymap(lambda x:first_line.append(x))\n",
    "        first_line_result=[]\n",
    "        for i in first_line:\n",
    "            i=re.sub(r\"^[ ]*?\",\"\",i)\n",
    "            i=re.sub(r\"$[ ]*?\",\"\",i)\n",
    "            # i=re.sub(r\"[ ]*?\",\"\",i)\n",
    "            if i != \"\":\n",
    "                first_line_result.append(i)\n",
    "        table_result.append(columns_list)\n",
    "        first_line_list.append(first_line_result)\n",
    "        start_rows_list.append(start_row)\n",
    "        product_df_list.append(product_df)\n",
    "        # print(len(table_result))\n",
    "        # print(len(first_line_list))\n",
    "        # print(len(start_rows_list))\n",
    "    return table_result,start_rows_list,first_line_list,product_df_list\n",
    "\n",
    "\n",
    "tmp_table_column={}\n",
    "tmp_table_column[\"sample_id\"]=[]\n",
    "tmp_table_column[\"columns\"]=[]\n",
    "tmp_table_column[\"start_row\"]=[]\n",
    "tmp_table_column[\"first_line\"]=[]\n",
    "tmp_table_column[\"product_df\"]=[]\n",
    "for sample_id,tabel in tqdm(val_df[[\"sample_id\",\"tabel\"]].head(1800).values):\n",
    "    table_result,start_rows_list,first_line_list,product_df_list=row_combine(sample_id,tabel)\n",
    "    for i in range(len(start_rows_list)):\n",
    "        valid_columns_index=get_valid_columns_index(table_result[i])\n",
    "        tmp_table_column[\"sample_id\"].append(sample_id)\n",
    "        tmp_table_column[\"columns\"].append(table_result[i].iloc[valid_columns_index].T.reset_index(drop=True).T)\n",
    "        tmp_table_column[\"start_row\"].append(start_rows_list[i])\n",
    "        tmp_table_column[\"first_line\"].append(first_line_list[i])\n",
    "        tmp_table_column[\"product_df\"].append(product_df_list[i].iloc[:,valid_columns_index].T.reset_index(drop=True).T)\n",
    "tmp_table_column=pd.DataFrame(tmp_table_column)\n",
    "\n",
    "\n",
    "# tmp_table_column[[\"columns\",\"start_row\",\"first_line\"]]=val_df[\"tabel\"].head(5).apply(lambda x:row_combine(x))\n",
    "\n",
    "\n",
    "# tmp=val_df.iloc[177,:][[\"sample_id\",\"tabel\"]]\n",
    "# table_result,start_rows_list,first_line_list,product_df_list=row_combine(tmp[\"sample_id\"],tmp[\"tabel\"])\n",
    "# table_result\n",
    "# product_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tmp_table_column.loc[393][\"product_df\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#fuzz裁剪，待优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz\n",
    "# index=0\n",
    "# invalid_list=[]\n",
    "# for sample_id,first_line in tqdm(tmp_table_column[[\"sample_id\",\"first_line\"]].values):\n",
    "#     current_judge_title=judge_title_result[judge_title_result[\"sample_id\"]==sample_id]\n",
    "#     judge_flag=[]\n",
    "#     for item in first_line:\n",
    "#         judge_flag=[]\n",
    "#         for i in current_judge_title[4].values:\n",
    "#             judge_flag.append(fuzz.partial_token_sort_ratio(item,i))\n",
    "#         if(np.mean(judge_flag)>0):\n",
    "#             print(np.mean(judge_flag))\n",
    "#             invalid_list.append(index)\n",
    "#             break\n",
    "#     index+=1\n",
    "# invalid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#单个理财产品行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "a='2018年6月24日，2018年6月24日'\n",
    "t = TimeFinder()\n",
    "time_all = t.find_time(a)\n",
    "print(time_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-218-5acbc70fa00a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcolumns_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproduct_df\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp_table_column\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"product_df\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# product_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0meach_sum_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_each_product_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproduct_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meach_sum_rows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0msum_product_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meach_sum_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-218-5acbc70fa00a>\u001b[0m in \u001b[0;36mget_each_product_row\u001b[1;34m(columns_list, df)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0meach_row\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0meach_row\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0meach_row\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjudge_time_exist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meach_row\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                     \u001b[0meach_row\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                     \u001b[0msum_rows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msum_rows\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtmp_row\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-218-5acbc70fa00a>\u001b[0m in \u001b[0;36mjudge_time_exist\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mjudge_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeFinder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjudge_flag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjudge_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3968\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m         \"\"\"\n\u001b[1;32m-> 3970\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   3972\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-218-5acbc70fa00a>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mjudge_flag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTimeFinder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjudge_flag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjudge_flag\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Work\\数据挖掘\\baseline_青青草原我的家\\src\\time_extractor.py\u001b[0m in \u001b[0;36mfind_time\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch_item\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                     \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\\\'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1900\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                         \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_date\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[0;32m    567\u001b[0m     format string.\"\"\"\n\u001b[1;32m--> 568\u001b[1;33m     \u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mformat_regex\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                 \u001b[0mformat_regex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_TimeRE_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m             \u001b[1;31m# KeyError raised when a bad format is found; can be specified as\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# \\\\, in which case it was a stray % but with a space after it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, format)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;34m\"\"\"Return a compiled re object for the format string.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mre_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[0m_cache_lock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_thread_allocate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\_strptime.py\u001b[0m in \u001b[0;36mpattern\u001b[1;34m(self, format)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;31m# format directives (%m, etc.).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[0mregex_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"([\\\\.^$*+?\\(\\){}\\[\\]|])\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregex_chars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\\\\\1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0mwhitespace_replacement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\s+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[0mformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwhitespace_replacement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\\\s+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "\n",
    "\n",
    "#寻找居中列，即只占一行的列\n",
    "def find_min_columns(df):\n",
    "    count_list=[]\n",
    "    result=[]\n",
    "    df.copy().apply(lambda x:count_list.append(get_valid_columns_num(x.T)),axis=1)\n",
    "    min_count=np.min(count_list)\n",
    "    for item in count_list:\n",
    "        if item ==min_count:\n",
    "            result.append(item)\n",
    "    return result\n",
    "def judge_time_exist(df):\n",
    "    judge_flag=[]\n",
    "    t = TimeFinder()\n",
    "    df.map(lambda x:judge_flag.append(t.find_time(x) is not None))\n",
    "    if True not in judge_flag:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def get_each_product_row(columns_list,df):\n",
    "    global column_neg_words\n",
    "    df=df.applymap(lambda x:str(x).replace(\" \",\"\"))\n",
    "    max_valid_index_num=len(columns_list)\n",
    "\n",
    "    #字段个数不足，跳过\n",
    "    if(columns_list.shape[0]<=4):\n",
    "        # print(1)\n",
    "        return None\n",
    "\n",
    "    #存在敏感词，即为无效答案，跳过\n",
    "    judge_flag=[]    \n",
    "    for word in column_neg_words:\n",
    "        columns_list.map(lambda x:judge_flag.append(fuzz.partial_ratio(word,x)==100))\n",
    "    if(True in judge_flag):\n",
    "        # print(2)\n",
    "        return None\n",
    "    sum_rows=df.head(0)\n",
    "    each_row=None\n",
    "\n",
    "    #获取居中列\n",
    "    min_columns_list=find_min_columns(df)\n",
    "\n",
    "    for index in range(df.shape[0]):\n",
    "        tmp_row=df.loc[index]\n",
    "        # print(tmp_row.shape)\n",
    "        tmp_valid_num=get_valid_columns_num(tmp_row)\n",
    "\n",
    "        if tmp_valid_num==max_valid_index_num:#第一种情况，列完整\n",
    "            #判断是否存在时间\n",
    "            if(judge_time_exist(tmp_row)):\n",
    "                each_row=None\n",
    "                sum_rows=pd.concat([sum_rows,tmp_row.to_frame().T])\n",
    "                continue\n",
    "            else:#不存在时间则视为不完整,加入each_row之后判断是否存在时间，存在即完整\n",
    "                if each_row is None:\n",
    "                    each_row=tmp_row\n",
    "                else:\n",
    "                    each_row=each_row+tmp_row\n",
    "                    if(judge_time_exist(each_row)):\n",
    "                        each_row=None\n",
    "                        sum_rows=pd.concat([sum_rows,tmp_row.to_frame().T])\n",
    "                        continue\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "        elif(tmp_valid_num==1 and sum_rows.shape[0]==0):#第二种情况，列不完整,且长度为1,且sum_row无内容，认为是少数的错误字段遗留数据，舍弃\n",
    "            continue\n",
    "        elif((index+1)!=df.shape[0]):#第三种情况，列不完整，需要多行拼加\n",
    "            #寻找目前剩下的矩阵中与当前行以外存在值的最浅的居中列\n",
    "            \n",
    "            start_row=index\n",
    "            remain_df=df.iloc[start_row:,min_columns_list]\n",
    "            middle_index=index+1\n",
    "            middle_column=min_columns_list[0]\n",
    "            for i in list(remain_df.index):\n",
    "                valid_columns_index=get_valid_columns_index(remain_df.loc[i])\n",
    "                if(len(valid_columns_index))>0:\n",
    "                    middle_index=i\n",
    "                    middle_column=valid_columns_index[0]\n",
    "                    break\n",
    "            \n",
    "            start_row=index\n",
    "            end_row=middle_index*2-index if middle_index*2-index<=df.shape[0] else df.shape[0]\n",
    "            for i in range(start_row,end_row):\n",
    "                if(get_valid_columns_num(df.loc[i])==max_valid_index_num):#强制中断条件\n",
    "                    break\n",
    "                index=i\n",
    "                each_row=df.loc[i] if each_row is None else each_row+df.loc[i]\n",
    "\n",
    "                if(judge_time_exist(each_row)):\n",
    "                    each_row=None\n",
    "                    sum_rows=pd.concat([sum_rows,tmp_row.to_frame().T])\n",
    "                    continue\n",
    "                else:\n",
    "                    each_row=None\n",
    "                    continue\n",
    "    # print(sum_rows)\n",
    "    return sum_rows\n",
    "\n",
    "\n",
    "column_neg_words=[\"实际收回\",\"收回\",\"赎回\",\"实际获得\",\"实际损益\",\"收益情况\",\"投资盈亏\",\"投资收益\",\"理财盈亏\",\"理财收益\",\"盈亏\",\"收益（元\",\"收益(元\",\"收益(万元\",\"收益（万元\"]\n",
    "sum_product_df=[]\n",
    "\n",
    "index=0\n",
    "index_list=[]\n",
    "for columns_list,product_df in tqdm(tmp_table_column[[\"columns\",\"product_df\"]].values):\n",
    "    # product_df\n",
    "    each_sum_rows=get_each_product_row(columns_list,product_df.reset_index(drop=True))\n",
    "    if(each_sum_rows is not None):\n",
    "        sum_product_df.append(each_sum_rows)\n",
    "        index_list.append(index)\n",
    "    index+=1\n",
    "\n",
    "result_matrix=tmp_table_column.iloc[index_list,:]\n",
    "result_matrix[\"product_df\"]=sum_product_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.抽取的是单独的数据包含\n",
    "#### 起息日，到息日， 金额，认购日期，产品发行方，理财产品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_single={}\n",
    "temp_single['认购日期'] = []\n",
    "temp_single['产品起息日'] = []\n",
    "temp_single['产品到息日'] = []\n",
    "temp_single['产品期限'] = []\n",
    "temp_single['认购金额(万元)'] = []\n",
    "temp_single['产品发行方名称'] = []\n",
    "temp_single['理财产品名称'] = []\n",
    "\n",
    "def judge_type(columns):\n",
    "    product_name_pos_words=[]\n",
    "\n",
    "for sample_id,columns,product_df in tqdm(result_matrix[[\"sample_id\",\"columns\",\"product_df\"]].values):\n",
    "    \n",
    "    for index in range(product_df.shape[0]):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "title_num_char=[\"一、\",\"二、\",\"三、\",\"四、\",\"五、\",\"六、\",\"七、\",\"八、\",\"九、\",\"十、\",\"十一、\",\"十二、\",\"十三、\",\"十四、\",\"十五、\"]\n",
    "s_title_num_char=[\"（一）\",\"（二）\",\"（三）\",\"（四）\",\"（五）\",\"（六）\",\"（七）\",\"（八）\",\"（九）\",\"（十）\",\"（十一）\",\"（十二）\",\"（十三）\",\"（十四）\",\"（十五）\"]\n",
    "s_title_num_char.extend([\"[(]一[)]\",\"[(]二[)]\",\"[(]三[)]\",\"[(]四[)]\",\"[(]五[)]\",\"[(]六[)]\",\"[(]七[)]\",\"[(]八[)]\",\"[(]九[)]\",\"[(]十[)]\",\"[(]十一[)]\",\"[(]十二[)]\",\"[(]十三[)]\",\"[(]十四[)]\",\"[(]十五[)]\"])\n",
    "\n",
    "title_pos_words=[]\n",
    "title_neg_words=[\"备查\",\"日前\",\"过去\",\"履行\",\"审批\",\"程序\",\"风险\",\"措施\",\"影响\",\"累计\",\"赎回\",\"到期\"]\n",
    "\n",
    "\n",
    "def get_title(text):\n",
    "    global title_num_char\n",
    "    title_list=[]\n",
    "    title_type_list=[]\n",
    "    text_start_iter_list=[]\n",
    "    text_end_iter_list=[]\n",
    "    for item in title_num_char:\n",
    "        pattern = re.compile(item+r\"[ ]*?[^ ]+?[ ]\")\n",
    "        tmp=pattern.finditer(text)\n",
    "        for i in tmp:\n",
    "            title_list.append(i.group())\n",
    "            text_start_iter_list.append(i.span(0)[0])\n",
    "            title_type_list.append(1)\n",
    "            text_end_iter_list.append(i.span(0)[1])\n",
    "    \n",
    "    # for item in title_list:\n",
    "    for item in s_title_num_char:\n",
    "        pattern = re.compile(item+r\"[ ]*?[^ ]+?[ ]\")\n",
    "        tmp=pattern.finditer(text)\n",
    "        for i in tmp:\n",
    "            title_list.append(i.group())\n",
    "            text_start_iter_list.append(i.span(0)[0])\n",
    "            title_type_list.append(2)\n",
    "            text_end_iter_list.append(i.span(0)[1])\n",
    "\n",
    "    title_list.append(\"引言\")\n",
    "    title_type_list.append(1)\n",
    "    text_start_iter_list.append(0)\n",
    "    text_end_iter_list.append(0)\n",
    "\n",
    "    result_df=pd.DataFrame([title_list,title_type_list,text_start_iter_list,text_end_iter_list]).T.sort_values(by=2).reset_index(drop=True)\n",
    "    # print(result_df)\n",
    "    return result_df\n",
    "\n",
    "def get_title_text(text,title_df):\n",
    "    # print(title_df)\n",
    "    title_1_df=title_df[title_df[1]==1]\n",
    "    text_iter_list=[]\n",
    "    text_list=[]\n",
    "    # print(title_1_df)\n",
    "    for iter1,iter2 in title_1_df[[2,3]].values:\n",
    "        # print(iter1)\n",
    "        if(len(text_iter_list)!=0):\n",
    "            text_iter_list.append(iter1)\n",
    "        text_iter_list.append(iter2)\n",
    "    # text_iter_list.append(text_iter_list[len(text_iter_list)-1])\n",
    "    text_iter_list.append(len(text))\n",
    "    for index in range(int(len(text_iter_list)/2)):\n",
    "        text_list.append(text[text_iter_list[2*index]:text_iter_list[2*index+1]])\n",
    "    \n",
    "    title_1_df[4]=text_list\n",
    "\n",
    "    return title_1_df.reset_index(drop=True)\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "def judge_title(sample_id=0,text=r\"test\\n\"):\n",
    "    # print(text)\n",
    "    text=text.replace(r\"\\n\",\"\")\n",
    "    title_df=get_title(text)\n",
    "    title_df[\"sample_id\"]=[sample_id for x in range(title_df.shape[0])]\n",
    "    # print(title_df)\n",
    "    title_1_df=get_title_text(text,title_df)[[\"sample_id\",0,1,2,3,4]]\n",
    "\n",
    "    \n",
    "\n",
    "    global val_df\n",
    "    global train_outputs\n",
    "    val_true_name=train_outputs[train_outputs[\"sample_id\"]==sample_id][\"理财产品名称\"]\n",
    "    \n",
    "    # print(title_1_df)\n",
    "    title_1_df[\"匹配分数\"]=0\n",
    "    for item in val_true_name:\n",
    "        title_1_df[\"匹配分数\"]=title_1_df[\"匹配分数\"]+title_1_df[4].apply(lambda x:fuzz.partial_ratio(x,str(item)))\n",
    "    \n",
    "    max_score=np.max(title_1_df[\"匹配分数\"])\n",
    "    title_1_df[\"是否为信息来源\"]=title_1_df[\"匹配分数\"].apply(lambda x:0 if x < max_score else 1)\n",
    "    # print(title_1_df)\n",
    "\n",
    "\n",
    "    return title_1_df\n",
    "    # print(title_list)\n",
    "\n",
    "judge_title_result=None\n",
    "\n",
    "\n",
    "# for sample_id,text in tqdm(train_df[[\"sample_id\",\"text\"]].values):\n",
    "#     # print(sample_id)\n",
    "#     # print(text)\n",
    "#     judge_title_result= judge_title(sample_id,text) if judge_title_result is None else pd.concat([judge_title_result,judge_title(sample_id,text)])\n",
    "\n",
    "# judge_title_result.to_excel(\"训练集段落标题分类结果.xlsx\",index=None)\n",
    "\n",
    "\n",
    "# is_from_text(val_df[val_df[\"sample_id\"]==930][\"sample_id\"].iloc[0],val_df[val_df[\"sample_id\"]==930][\"text\"].iloc[0])\n",
    "# is_from_text(val_df[val_df[\"sample_id\"]==125][\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "judge_title(1067,train_df[train_df[\"sample_id\"]==1067][\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 直接提取时间\n",
    "# 如果出现两个时间第一个就是起息日，第二个就是到期日\n",
    "# 如果出现一个时间就是起息日\n",
    "# 出现的第一个money就是最后的金额\n",
    "# 从这里面抽取所有序列\n",
    "# 这里认为有逗号出现的就是money\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "def get_list_data(df):\n",
    "    df = list(df)\n",
    "    new_df = []\n",
    "    for i in tqdm(df):\n",
    "        temp_df = []\n",
    "        for h in i:\n",
    "            new_h = []\n",
    "            for digital in h:\n",
    "                if ',' in digital:\n",
    "                    # 这里也是为了统一数据有些是用元，有些是用万元\n",
    "                    try:\n",
    "                        ttt = float(digital.replace(',', '').replace('万元', '').replace('人民币', '').replace('元', ''))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if ttt > 20000:\n",
    "                        ttt = ttt/10000\n",
    "                    new_h.append(ttt)\n",
    "                else:\n",
    "                    continue\n",
    "            if len(new_h) == 0:\n",
    "                continue\n",
    "            temp_single = {}\n",
    "            a = '_'.join(h)\n",
    "            # 抽取时间和money\n",
    "            t = TimeFinder()\n",
    "            time_all = t.find_time(a)\n",
    "            if time_all == None:\n",
    "                continue\n",
    "            rgrq = time_all[0]\n",
    "            cpqxr = time_all[0]\n",
    "            if len(time_all) > 1:\n",
    "                try:\n",
    "                    cpdxr = time_all[1]\n",
    "                    # 相减\n",
    "                    d1 = datetime.datetime.strptime(cpqxr, '%Y-%m-%d')\n",
    "                    d2 = datetime.datetime.strptime(cpdxr, '%Y-%m-%d')\n",
    "                    d = d2 - d1\n",
    "                    cpqx = str(d.days) + '天'\n",
    "                except Exception:\n",
    "                    cpdxr = np.nan\n",
    "                    cpqx = np.nan\n",
    "            else:\n",
    "                cpdxr = np.nan\n",
    "                cpqx = np.nan\n",
    "                \n",
    "            # 筛选出除开数字与包含时间的列\n",
    "            # 末尾是\n",
    "            last_two = ['公司', '银行', '信托', '证券',  '分行', '支行', '中心', '业部', '商行', '建行']\n",
    "            mowei = np.nan\n",
    "            selected_bank_and_works = []\n",
    "            for l in h:\n",
    "                new_l = list(str(l))\n",
    "                new_l_test = ''.join(l[-2:])\n",
    "                if new_l_test in last_two:\n",
    "                    mowei = l\n",
    "                    continue\n",
    "                if '资金' in l:\n",
    "                    continue\n",
    "                if '收益' in l:\n",
    "                    continue\n",
    "                if '到期' in l:\n",
    "                    continue\n",
    "                if ',' in l:\n",
    "                    continue\n",
    "                if '.' in l:\n",
    "                    continue\n",
    "                if '/' in l:\n",
    "                    continue\n",
    "                if '年' in l:\n",
    "                    continue\n",
    "                if '-' in l:\n",
    "                    continue\n",
    "                if len(l) < 4:\n",
    "                    continue\n",
    "                if is_number(l):\n",
    "                    continue\n",
    "                selected_bank_and_works.append(l)\n",
    "            if len(selected_bank_and_works) < 1:\n",
    "                continue\n",
    "            \n",
    "            temp_single['认购日期'] = rgrq\n",
    "            temp_single['产品起息日'] = cpqxr\n",
    "            temp_single['产品到期日'] = cpdxr\n",
    "            temp_single['产品期限'] = cpqx\n",
    "            temp_single['认购金额(万元)'] = new_h[0]\n",
    "            temp_single['产品发行方名称'] = mowei\n",
    "            temp_single['理财产品名称'] = selected_bank_and_works[0]\n",
    "            temp_df.append(temp_single)\n",
    "        new_df.append(temp_df)\n",
    "    return new_df\n",
    "\n",
    "val_contain_date = get_list_data(val_df_tabel)\n",
    "# test_contain_data = get_list_data(test_df_tabel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接提取时间\n",
    "# 如果出现两个时间第一个就是起息日，第二个就是到期日\n",
    "# 如果出现一个时间就是起息日\n",
    "# 出现的第一个money就是最后的金额\n",
    "# 从这里面抽取所有序列\n",
    "# 这里认为有逗号出现的就是money\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "def get_list_data(df):\n",
    "    df = list(df)\n",
    "    new_df = []\n",
    "    for i in tqdm(df):\n",
    "        temp_df = []\n",
    "        for h in i:\n",
    "            new_h = []\n",
    "            for digital in h:\n",
    "                if ',' in digital:\n",
    "                    # 这里也是为了统一数据有些是用元，有些是用万元\n",
    "                    try:\n",
    "                        ttt = float(digital.replace(',', '').replace('万元', '').replace('人民币', '').replace('元', ''))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if ttt > 20000:\n",
    "                        ttt = ttt/10000\n",
    "                    new_h.append(ttt)\n",
    "                else:\n",
    "                    continue\n",
    "            if len(new_h) == 0:\n",
    "                continue\n",
    "            temp_single = {}\n",
    "            a = '_'.join(h)\n",
    "            # 抽取时间和money\n",
    "            t = TimeFinder()\n",
    "            time_all = t.find_time(a)\n",
    "            if time_all == None:\n",
    "                continue\n",
    "            rgrq = time_all[0]\n",
    "            cpqxr = time_all[0]\n",
    "            if len(time_all) > 1:\n",
    "                try:\n",
    "                    cpdxr = time_all[1]\n",
    "                    # 相减\n",
    "                    d1 = datetime.datetime.strptime(cpqxr, '%Y-%m-%d')\n",
    "                    d2 = datetime.datetime.strptime(cpdxr, '%Y-%m-%d')\n",
    "                    d = d2 - d1\n",
    "                    cpqx = str(d.days) + '天'\n",
    "                except Exception:\n",
    "                    cpdxr = np.nan\n",
    "                    cpqx = np.nan\n",
    "            else:\n",
    "                cpdxr = np.nan\n",
    "                cpqx = np.nan\n",
    "                \n",
    "            # 筛选出除开数字与包含时间的列\n",
    "            # 末尾是\n",
    "            last_two = ['公司', '银行', '信托', '证券',  '分行', '支行', '中心', '业部', '商行', '建行']\n",
    "            mowei = np.nan\n",
    "            selected_bank_and_works = []\n",
    "            for l in h:\n",
    "                new_l = list(str(l))\n",
    "                new_l_test = ''.join(l[-2:])\n",
    "                if new_l_test in last_two:\n",
    "                    mowei = l\n",
    "                    continue\n",
    "                if '资金' in l:\n",
    "                    continue\n",
    "                if '收益' in l:\n",
    "                    continue\n",
    "                if '到期' in l:\n",
    "                    continue\n",
    "                if ',' in l:\n",
    "                    continue\n",
    "                if '.' in l:\n",
    "                    continue\n",
    "                if '/' in l:\n",
    "                    continue\n",
    "                if '年' in l:\n",
    "                    continue\n",
    "                if '-' in l:\n",
    "                    continue\n",
    "                if len(l) < 4:\n",
    "                    continue\n",
    "                if is_number(l):\n",
    "                    continue\n",
    "                selected_bank_and_works.append(l)\n",
    "            if len(selected_bank_and_works) < 1:\n",
    "                continue\n",
    "            \n",
    "            temp_single['认购日期'] = rgrq\n",
    "            temp_single['产品起息日'] = cpqxr\n",
    "            temp_single['产品到期日'] = cpdxr\n",
    "            temp_single['产品期限'] = cpqx\n",
    "            temp_single['认购金额(万元)'] = new_h[0]\n",
    "            temp_single['产品发行方名称'] = mowei\n",
    "            temp_single['理财产品名称'] = selected_bank_and_works[0]\n",
    "            temp_df.append(temp_single)\n",
    "        new_df.append(temp_df)\n",
    "    return new_df\n",
    "\n",
    "val_contain_date = get_list_data(val_df_tabel)\n",
    "# test_contain_data = get_list_data(test_df_tabel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.汇总整理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将前面提取到的数据整理成对应格式\n",
    "sample_id_list = []\n",
    "rgrq_list = []\n",
    "lccp_list = []\n",
    "cpfxf_list = []\n",
    "rgje_list = []\n",
    "cpqxr_list = []\n",
    "cpdxr_list = []\n",
    "cpqx_list = []\n",
    "sjgmgsmc_list = []\n",
    "ggrq_list = []\n",
    "\n",
    "sample_id = list(val_df['sample_id'])\n",
    "gg = list(val_gm)\n",
    "time = list(val_time)\n",
    "for i, value in enumerate(sample_id):\n",
    "    for j in val_contain_date[i]:\n",
    "        sample_id_list.append(sample_id[i])\n",
    "        rgrq_list.append(j['认购日期'])\n",
    "        lccp_list.append(j['理财产品名称'])\n",
    "        cpfxf_list.append(j['产品发行方名称'])\n",
    "        rgje_list.append(j['认购金额(万元)'])\n",
    "        cpqxr_list.append(j['产品起息日'])\n",
    "        cpdxr_list.append(j['产品到期日'])\n",
    "        cpqx_list.append(j['产品期限'])\n",
    "        sjgmgsmc_list.append(gg[i])\n",
    "        ggrq_list.append(time[i])\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['sample_id'] = sample_id_list\n",
    "result['认购日期'] = rgrq_list\n",
    "result['理财产品名称'] = lccp_list\n",
    "result['产品发行方名称'] = cpfxf_list\n",
    "result['认购金额(万元)'] = rgje_list\n",
    "result['产品起息日'] = cpqxr_list\n",
    "result['产品到期日'] = cpdxr_list\n",
    "result['产品期限'] = cpqx_list\n",
    "result['实际购买公司名称'] = sjgmgsmc_list\n",
    "result['公告日期'] = ggrq_list\n",
    "val_result = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_id_list = []\n",
    "rgrq_list = []\n",
    "lccp_list = []\n",
    "cpfxf_list = []\n",
    "rgje_list = []\n",
    "cpqxr_list = []\n",
    "cpdxr_list = []\n",
    "cpqx_list = []\n",
    "sjgmgsmc_list = []\n",
    "ggrq_list = []\n",
    "\n",
    "sample_id = list(test_df['sample_id'])\n",
    "gg = list(test_gm)\n",
    "time = list(test_time)\n",
    "for i, value in enumerate(sample_id):\n",
    "    for j in test_contain_data[i]:\n",
    "        sample_id_list.append(sample_id[i])\n",
    "        rgrq_list.append(j['认购日期'])\n",
    "        lccp_list.append(j['理财产品名称'])\n",
    "        cpfxf_list.append(j['产品发行方名称'])\n",
    "        rgje_list.append(j['认购金额(万元)'])\n",
    "        cpqxr_list.append(j['产品起息日'])\n",
    "        cpdxr_list.append(j['产品到期日'])\n",
    "        cpqx_list.append(j['产品期限'])\n",
    "        sjgmgsmc_list.append(gg[i])\n",
    "        ggrq_list.append(time[i])\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['sample_id'] = sample_id_list\n",
    "result['认购日期'] = rgrq_list\n",
    "result['理财产品名称'] = lccp_list\n",
    "result['产品发行方名称'] = cpfxf_list\n",
    "result['认购金额(万元)'] = rgje_list\n",
    "result['产品起息日'] = cpqxr_list\n",
    "result['产品到期日'] = cpdxr_list\n",
    "result['产品期限'] = cpqx_list\n",
    "result['实际购买公司名称'] = sjgmgsmc_list\n",
    "result['公告日期'] = ggrq_list\n",
    "test_result = result\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def get_F1(val_pred, val_true):\n",
    "      val_pred = list(val_pred)\n",
    "      val_true = list(val_true)\n",
    "      curr = list(set(val_pred).intersection(set(val_true)))\n",
    "      R = len(curr)/len(val_true)\n",
    "      P = len(curr)/len(val_pred)\n",
    "      return 2*P*R/(P+R)\n",
    "\n",
    "  r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "  val_true = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) +r['产品发行方名称'].astype(str)\n",
    "\n",
    "  r = val_result\n",
    "  val_pred = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到期日'].astype(str) + r['产品期限'].astype(str) +r['产品发行方名称'].astype(str)\n",
    "\n",
    "  score = get_F1(val_pred, val_true)\n",
    "  score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_judge(result):\n",
    "    global judge_title_result\n",
    "    score_limit=30\n",
    "    drop_list=[]\n",
    "    index=0\n",
    "    for sample_id,product_name in tqdm(result[[\"sample_id\",\"理财产品名称\"]].values):\n",
    "        score_list=[]\n",
    "        for text in judge_title_result[judge_title_result[\"sample_id\"]==sample_id][4].values:\n",
    "            score_list.append(fuzz.partial_ratio(product_name,text))\n",
    "        if np.max(pd.DataFrame(score_list)[0])<=score_limit:\n",
    "            drop_list.append(index)\n",
    "        index+=1 \n",
    "    \n",
    "    return result.copy().drop(drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "  def get_F1(val_pred, val_true):\n",
    "      val_pred = list(val_pred)\n",
    "      val_true = list(val_true)\n",
    "      curr = list(set(val_pred).intersection(set(val_true)))\n",
    "      R = len(curr)/len(val_true)\n",
    "      P = len(curr)/len(val_pred)\n",
    "      return 2*P*R/(P+R)\n",
    "\n",
    "  r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "  val_true = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) +r['产品发行方名称'].astype(str)\n",
    "\n",
    "  r = drop_judge(result)\n",
    "  print(r.shape)\n",
    "  val_pred = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到期日'].astype(str) + r['产品期限'].astype(str) +r['产品发行方名称'].astype(str)\n",
    "\n",
    "  score = get_F1(val_pred, val_true)\n",
    "  score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "val_result_file=val_result.sort_values(by='sample_id').reset_index(drop=True)\n",
    "val_true_file=pd.merge(train_outputs, val_df['sample_id'], on='sample_id',how=\"right\").sort_values(by=\"sample_id\").reset_index(drop=True)\n",
    "# train_outputs\n",
    "# val_result_file\n",
    "# val_true_file\n",
    "val_id_list=val_result_file[\"sample_id\"].unique()\n",
    "val_id_count={}\n",
    "val_id_count[\"sample_id\"]=[]\n",
    "val_id_count[\"预测\"]=[]\n",
    "val_id_count[\"实际\"]=[]\n",
    "for item in val_id_list:\n",
    "    val_id_count[\"sample_id\"].append(item)\n",
    "    val_id_count[\"预测\"].append(val_result_file[val_result_file[\"sample_id\"]==item].shape[0])\n",
    "    val_id_count[\"实际\"].append(val_true_file[val_true_file[\"sample_id\"]==item].shape[0])\n",
    "\n",
    "val_id_count=pd.DataFrame(val_id_count)\n",
    "val_id_count[\"差值\"]=val_id_count[\"预测\"]-val_id_count[\"实际\"]\n",
    "print(val_id_count[val_id_count[\"差值\"]==0].shape[0]/val_id_count.shape[0])\n",
    "val_id_count.to_excel(\"验证集row数量对比.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}