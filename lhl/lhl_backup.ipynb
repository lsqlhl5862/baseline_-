{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关包\n",
    "import os\n",
    "import pathlib as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "from datetime import datetime,timedelta\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from tqdm.autonotebook import *\n",
    "import pdfplumber\n",
    "tqdm.pandas()\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "sys.path.append(\"..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF解析原始数据 \n",
    "## 加载数据并采用pdfplumber抽取PDF中的文字和表格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>认购日期</th>\n",
       "      <th>理财产品名称</th>\n",
       "      <th>产品发行方名称</th>\n",
       "      <th>理财类型</th>\n",
       "      <th>认购金额(万元)</th>\n",
       "      <th>产品起息日</th>\n",
       "      <th>产品到息日</th>\n",
       "      <th>产品期限</th>\n",
       "      <th>资金来源</th>\n",
       "      <th>实际购买公司名称</th>\n",
       "      <th>实际购买公司和上市公司关系</th>\n",
       "      <th>买卖方是否有关联关系</th>\n",
       "      <th>公告日期</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>汇聚金1号</td>\n",
       "      <td>中融国际信托有限公司</td>\n",
       "      <td>信托</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>2019-09-23</td>\n",
       "      <td>180天</td>\n",
       "      <td>自有资金</td>\n",
       "      <td>恒生电子股份有限公司</td>\n",
       "      <td>公司本身</td>\n",
       "      <td>否</td>\n",
       "      <td>2019-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>招商银行步步生金8699</td>\n",
       "      <td>招商银行</td>\n",
       "      <td>银行理财产品</td>\n",
       "      <td>200.0</td>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>自有资金</td>\n",
       "      <td>恒生电子股份有限公司</td>\n",
       "      <td>公司本身</td>\n",
       "      <td>否</td>\n",
       "      <td>2019-04-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id       认购日期        理财产品名称     产品发行方名称    理财类型  认购金额(万元)  \\\n",
       "0          1 2019-03-27         汇聚金1号  中融国际信托有限公司      信托   10000.0   \n",
       "1          1 2019-03-27  招商银行步步生金8699        招商银行  银行理财产品     200.0   \n",
       "\n",
       "       产品起息日      产品到息日  产品期限  资金来源    实际购买公司名称 实际购买公司和上市公司关系 买卖方是否有关联关系  \\\n",
       "0 2019-03-27 2019-09-23  180天  自有资金  恒生电子股份有限公司          公司本身          否   \n",
       "1 2019-03-27        NaT   NaN  自有资金  恒生电子股份有限公司          公司本身          否   \n",
       "\n",
       "        公告日期  \n",
       "0 2019-04-25  \n",
       "1 2019-04-25  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "      <th>tabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>datasets/train_data/1.PDF</td>\n",
       "      <td>['                                            ...</td>\n",
       "      <td>[[['', None, None, '', None, None, '', None, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>datasets/train_data/2.PDF</td>\n",
       "      <td>['                                            ...</td>\n",
       "      <td>[[['', None, None, '', None, None, '', None, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                  file_path  \\\n",
       "0          1  datasets/train_data/1.PDF   \n",
       "1          2  datasets/train_data/2.PDF   \n",
       "\n",
       "                                                text  \\\n",
       "0  ['                                            ...   \n",
       "1  ['                                            ...   \n",
       "\n",
       "                                               tabel  \n",
       "0  [[['', None, None, '', None, None, '', None, N...  \n",
       "1  [[['', None, None, '', None, None, '', None, N...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>file_path</th>\n",
       "      <th>text</th>\n",
       "      <th>tabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11188</td>\n",
       "      <td>datasets/test_data/11188.PDF</td>\n",
       "      <td>['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11189</td>\n",
       "      <td>datasets/test_data/11189.PDF</td>\n",
       "      <td>['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                     file_path  \\\n",
       "0      11188  datasets/test_data/11188.PDF   \n",
       "1      11189  datasets/test_data/11189.PDF   \n",
       "\n",
       "                                                text tabel  \n",
       "0  ['北京京西文化旅游股份有限公司监事会\\n \\n \\n关于使用部分闲置募集资金购买理财产品的...    []  \n",
       "1  ['北京京西文化旅游股份有限公司 \\n监事会关于使用部分自有资金购买理财产品的意见 \\n根据...    []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据准备(train_output文件中格式有点问题，需要提前用excel或者wps打开然后另存为excel文件)\n",
    "train_outputs = pd.read_excel('../datasets/train_output.xlsx')\n",
    "\n",
    "# 获取pdf中文字和表格\n",
    "def extract_pdf_content(pdf_path):\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for index_page in np.arange(0, len(pdf.pages), 1):\n",
    "            # 读取多页\n",
    "            page = pdf.pages[index_page]   # 第n页的信息\n",
    "            text = page.extract_text()\n",
    "            text_list.append(text)\n",
    "            table = page.extract_tables()\n",
    "            for t in table:\n",
    "                table_list.append(t)\n",
    "    return text_list, table_list\n",
    "\n",
    "def get_dir_file(path):\n",
    "    '''\n",
    "    输入文件夹位置，输出整理好的dataframe\n",
    "    '''\n",
    "    path_list = os.listdir(path)\n",
    "    id_list = []\n",
    "    file_path_list = []\n",
    "    text_list = []\n",
    "    table_list = []\n",
    "    for i in tqdm(path_list):\n",
    "        if '.PDF' in i:\n",
    "            file_path = path + i\n",
    "            id_list.append(int(i.split('.')[0]))\n",
    "            file_path_list.append(file_path)\n",
    "            try:\n",
    "                text_temp, table_temp = extract_pdf_content(file_path)\n",
    "            except Exception:\n",
    "                print('此pdf无法读取')\n",
    "                text_temp, table_temp = [], []\n",
    "            text_list.append(text_temp)\n",
    "            table_list.append(table_temp)\n",
    "            \n",
    "    df = pd.DataFrame()\n",
    "    df['sample_id'] = id_list\n",
    "    df['file_path'] = file_path_list\n",
    "    df['text'] = text_list\n",
    "    df['tabel'] = table_list\n",
    "    df = df.sort_values('sample_id')\n",
    "    return df\n",
    "\n",
    "# 文件处理太慢，可持续化保存文件\n",
    "train_path = '../datasets/train.csv'\n",
    "if os.path.exists(train_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "else:\n",
    "    train_df = get_dir_file('datasets/train_data/')\n",
    "    train_df.to_csv(train_path,index=False)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "\n",
    "test_path =  '../datasets/test.csv'\n",
    "if os.path.exists(test_path):\n",
    "    test_df = pd.read_csv(test_path)\n",
    "else:\n",
    "    test_df = get_dir_file('datasets/test_data/')\n",
    "    test_df.to_csv(test_path,index=False)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_outputs.head(2)\n",
    "train_df.head(2)\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集验证集\n",
    "train_df = train_df.sample(frac=1, random_state=1017)\n",
    "val_df = train_df[:1800]\n",
    "train_df = train_df[1800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理\n",
    "## 抽取整体数据（一个sampleid内此字段内容都相同）\n",
    "## 公告时间，实际购买公司"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.抽取公告时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:00<00:00, 4269.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4583333333333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:00<00:00, 4977.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 首先针对任务抽取时间（每个时间跟每个id是一一对应的）\n",
    "# 要不是取第一个时间，要不就是取最后一个时间（或者时间加一）这里可以建立一个模型预测\n",
    "# base这里面直接取最后一个时间作为发布日期\n",
    "\n",
    "CN_NUM = {\n",
    "    u'〇': 0, u'一': 1, u'二': 2, u'三': 3,\n",
    "    u'四': 4, u'五': 5, u'六': 6, u'七': 7,\n",
    "    u'八': 8, u'九': 9, u'零': 0, u'壹': 1,\n",
    "    u'贰': 2, u'叁': 3, u'肆': 4, u'伍': 5,\n",
    "    u'陆': 6, u'柒': 7, u'捌': 8, u'玖': 9,\n",
    "    u'貮': 2, u'两': 2,\n",
    "}\n",
    "\n",
    "\n",
    "def get_put_time_from_text(row):\n",
    "    row = row.replace(' ', '').replace('\\\\n', '')\n",
    "    for key in CN_NUM:\n",
    "        row = row.replace(key, str(CN_NUM[key]))   \n",
    "    r = row.replace(\"年\", \"-\").replace(\"月\", \"-\").replace(\"日\", \" \").replace(\"/\", \"-\").strip()\n",
    "    regex = \"(\\d{4}-\\d{1,2}-\\d{1,2})\"\n",
    "    r = re.findall(regex, r)\n",
    "    if len(r)==0:\n",
    "        return np.nan\n",
    "    time_str = r[-1]\n",
    "    first = time_str.split('-')[0]\n",
    "    second = time_str.split('-')[1]\n",
    "    last = time_str.split('-')[-1]\n",
    "    second = str.zfill(second, 2)\n",
    "    last = str.zfill(last, 2)\n",
    "    r = '-'.join([first, second, last])\n",
    "    return r\n",
    "\n",
    "val_result = pd.DataFrame()\n",
    "val_result['sample_id'] = val_df['sample_id']\n",
    "val_result['predict_time'] = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "test_gg = train_outputs.groupby('sample_id').apply(lambda row:list(row['公告日期'])[0]).reset_index()\n",
    "test_gg.columns = ['sample_id', 'time']\n",
    "val_result = pd.merge(val_result, test_gg, on='sample_id', how='left')\n",
    "\n",
    "# 判断验证集的准确率\n",
    "np.sum(val_result['predict_time'].astype(str) == val_result['time'].astype(str))/len(val_result)\n",
    "\n",
    "val_time = val_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)\n",
    "# test_time = test_df.progress_apply(lambda row: get_put_time_from_text(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.抽取实际购买公司"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:00<00:00, 4470.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# 抽取购买公司\n",
    "# 前几句话出现\n",
    "# 将其按照\\\\n 和空格切割\n",
    "def get_gm(row):\n",
    "    result = re.split('[\\\\\\\\n ]',row)\n",
    "    for i in result:\n",
    "        if '公司' in i:\n",
    "            return i\n",
    "\n",
    "val_gm = val_df.progress_apply(lambda row:get_gm(row['text']), axis=1)\n",
    "# test_gm = test_df.progress_apply(lambda row:get_gm(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.清洗提取出来的tabel数据，主要是清洗掉有问题的列 \n",
    "# 重写清洗方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "0  aaaa\n",
       "1     2\n",
       "2  aaaa\n",
       "3  aaaa\n",
       "4  aaaa\n",
       "5     6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tmp_table=pd.concat([val_df[\"sample_id\"],pd.DataFrame(val_df_tabel)],axis=1)\n",
    "# tmp_table[\"len_table\"]=tmp_table[\"tabel\"].apply(lambda x:[len(i) for i in x])\n",
    "# # tmp_table.to_excel(\"table矩阵检验1.xlsx\")\n",
    "\n",
    "\n",
    "a=[1,2,3,4,5,6]\n",
    "b=[0,2,3,4]\n",
    "a=pd.DataFrame(a)\n",
    "a.loc[b]=\"aaaa\"\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#大量PDF的理财购买信息不在table中，而是在text中，此段为判断理财信息是否只需要从text中提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tabel_tmp_value=eval(val_df[val_df[\"sample_id\"]==35][\"tabel\"].iloc[0])\n",
    "# # tabel_tmp_value_list=[]\n",
    "# table_value_df=pd.DataFrame(tabel_tmp_value)\n",
    "val_df.loc[1419].shape[0]\n",
    "# tmp2=val_df.head(51).index\n",
    "# list(tmp1)\n",
    "# np.array_equal(tmp1,tmp2)\n",
    "# set(tmp2).issubset(tmp1)\n",
    "# set(tmp2).difference(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [05:48<00:00,  5.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# tabel_tmp_value=eval(val_df[val_df[\"sample_id\"]==4333][\"tabel\"].iloc[0])\n",
    "# tabel_tmp_value=eval(val_df[val_df[\"sample_id\"]==35][\"tabel\"].iloc[0])\n",
    "# tabel_tmp_value_list=[]\n",
    "# table_value_df=pd.DataFrame(tabel_tmp_value[0])\n",
    "# table_value_df.head(20)\n",
    "# for item in tabel_tmp_value:\n",
    "\n",
    "column_name_judge_word=[\"民币）\",\"民币)\",\"万元)\",\"元)\",\"万元）\",\"元）\",\"%)\",\"%）\",\"金额\"]\n",
    "\n",
    "\n",
    "#寻找有有效值存在的列\n",
    "def find_valid_columns(df):\n",
    "    count_list=[]\n",
    "    result=[]\n",
    "    df.copy().T.apply(lambda x:count_list.append(get_valid_columns_num(x.T)),axis=1)\n",
    "    # df.copy().T.apply(lambda x:print(x),axis=1)\n",
    "    # print(count_list)\n",
    "    # print(count_list)\n",
    "    for i in range(len(count_list)):\n",
    "        if count_list[i]>(int(df.shape[0]/7)):\n",
    "            result.append(i)\n",
    "    # print(result)   \n",
    "    return result\n",
    "\n",
    "\n",
    "#寻找（字段）所在的理论最后一行\n",
    "def find_amt(df):\n",
    "    global column_name_judge_word\n",
    "    max_index=-1\n",
    "    df=df.head(7)\n",
    "    for index in range(df.shape[0]):\n",
    "        for item in column_name_judge_word:\n",
    "            flag=[]\n",
    "            flag_num=[]\n",
    "            df.loc[index].map(lambda x:flag_num.append(re.match(\"[ ]*?\\d\",x)))\n",
    "            df.loc[index].map(lambda x:flag.append((item in x)))\n",
    "            for i in flag_num:\n",
    "                if(i is not None):\n",
    "                    return max_index\n",
    "            if True in flag:\n",
    "                max_index=index\n",
    "    return max_index\n",
    "\n",
    "column_name_judge_word=[\"民币）\",\"民币)\",\"万元)\",\"元)\",\"万元）\",\"元）\",\"%)\",\"%）\",\"金额\"]\n",
    "\n",
    "\n",
    "#寻找单个理财产品所在的理论最后一行\n",
    "def find_product(df):\n",
    "    global column_name_judge_word\n",
    "    max_index=-1\n",
    "    df=df.head(7)\n",
    "    for index in range(df.shape[0]):\n",
    "        for item in column_name_judge_word:\n",
    "            flag=[]\n",
    "            flag_num=[]\n",
    "            df.loc[index].map(lambda x:flag_num.append(re.match(\"[ ]*?\\d\",x)))\n",
    "            df.loc[index].map(lambda x:flag.append((item in x)))\n",
    "            for i in flag_num:\n",
    "                if(i is not None):\n",
    "                    return max_index\n",
    "            if True in flag:\n",
    "                max_index=index\n",
    "    return max_index\n",
    "\n",
    "def get_valid_columns_num(df):\n",
    "    df=df.T.dropna(axis=0).apply(lambda x:x.replace(\" \",\"\")).to_frame().T.reset_index(drop=True).T\n",
    "    # print(df)\n",
    "    return df[(df[0]!=\"\")&(df[0]!=np.nan)].shape[0]\n",
    "\n",
    "def get_valid_columns_index(df):\n",
    "    df=df.T.dropna(axis=0).apply(lambda x:x.replace(\" \",\"\")).to_frame().T.reset_index(drop=True).T\n",
    "    # print(df)\n",
    "    return df[(df[0]!=\"\")&(df[0]!=np.nan)].index\n",
    "\n",
    "def field_location_optimization(columns_list,df):\n",
    "    if(df.shape[1]!=columns_list.shape[0]):\n",
    "        return columns_list.to_frame().T.head(0)\n",
    "    max_columns_num=get_valid_columns_num(columns_list)\n",
    "    if(df.shape[0]==0):\n",
    "        return df\n",
    "    valid_clomuns_index=list(get_valid_columns_index(columns_list))\n",
    "    # print(valid_clomuns_index)\n",
    "    result_df=df.head(0)\n",
    "    # print(result_df.shape)\n",
    "    # print(df)\n",
    "    for index in range(df.shape[0]):\n",
    "        tmp_row=df.loc[index]\n",
    "        tmp_index=get_valid_columns_index(tmp_row)\n",
    "        # print(list(tmp_index))\n",
    "        if set(tmp_index).issubset(valid_clomuns_index):\n",
    "            pass\n",
    "        else:\n",
    "            difference_list=list(set(tmp_index).difference(valid_clomuns_index))\n",
    "            \n",
    "            location_list=[]\n",
    "            tmp_difference_list=difference_list.copy()\n",
    "            for item in difference_list:\n",
    "                location=item\n",
    "                #前后浮动位移（可优化）\n",
    "                for i in range(1,3):\n",
    "                    location=(item-i)\n",
    "                    if location in valid_clomuns_index and location not in tmp_difference_list:\n",
    "                        location_list.append(location)\n",
    "                        tmp_difference_list.remove(item)\n",
    "                        tmp_difference_list.append(location)\n",
    "                        break\n",
    "                    location=(item+i)\n",
    "                    if location in valid_clomuns_index and location not in tmp_difference_list:\n",
    "                        location_list.append(location)\n",
    "                        tmp_difference_list.remove(item)\n",
    "                        tmp_difference_list.append(location)\n",
    "                        break\n",
    "            result_location=list(df.columns)\n",
    "            # print(location_list)\n",
    "            # print(difference_list)\n",
    "            # print(\"---------------------\")\n",
    "            for i in range(len(location_list)):\n",
    "                result_location[difference_list[i]]=location_list[i]\n",
    "                result_location[location_list[i]]=difference_list[i]\n",
    "            # print(tmp_row)\n",
    "            tmp_row=tmp_row.iloc[result_location].reset_index(drop=True)\n",
    "            # print(tmp_row)\n",
    "            # print(result_location)\n",
    "        result_df=tmp_row.to_frame().T if result_df is None else pd.concat([result_df,tmp_row.to_frame().T])\n",
    "    # print(np.array_equal(get_valid_columns_index(columns_list),get_valid_columns_index(result_df)))\n",
    "    # print(list(set(get_valid_columns_index(columns_list)).difference(result_df)))\n",
    "    # print(result_df)\n",
    "    # print(result_df.shape)\n",
    "    return result_df\n",
    "def row_combine(sample_id,pdf_table):\n",
    "# 将字符串转化成多维列表\n",
    "    pdf_table=eval(pdf_table)\n",
    "\n",
    "    table_result=[]\n",
    "    start_rows_list=[]\n",
    "    first_line_list=[]\n",
    "    product_df_list=[]\n",
    "    for item in pdf_table:\n",
    "        #考虑加入一个判定表格位置的方法（暂缺）\n",
    "\n",
    "        tmp_table_df=pd.DataFrame(item)\n",
    "        tmp_table_df=tmp_table_df.fillna(\"\").applymap(lambda x: x.replace(\"\\n\",\"\"))\n",
    "        #清除全空行\n",
    "        drop_index_list=[]\n",
    "        for index in range(tmp_table_df.shape[0]):\n",
    "            judge=[]\n",
    "            noshow=tmp_table_df.loc[index].map(lambda x:judge.append(x==\"\"))\n",
    "            if(False not in judge):\n",
    "                drop_index_list.append(index)\n",
    "        \n",
    "        tmp_table_df=tmp_table_df.drop(drop_index_list).reset_index(drop=True)\n",
    "\n",
    "        base_row=find_amt(tmp_table_df)\n",
    "\n",
    "        tmp_row=None\n",
    "\n",
    "        len_product_df_list=len(product_df_list)\n",
    "        if base_row!=-1 and len_product_df_list!=0 :\n",
    "            # print(product_df_list[len_product_df_list-1].shape)\n",
    "            # print(table_result[len_product_df_list-1])\n",
    "            if (product_df_list[len_product_df_list-1] is not None ) and (product_df_list[len_product_df_list-1].shape[0]==0):\n",
    "                # print(table_result[len_product_df_list-1])\n",
    "                tmp_row=table_result[len_product_df_list-1]\n",
    "\n",
    "        if base_row==-1:\n",
    "            if(len(table_result)>0):\n",
    "                product_df=field_location_optimization(table_result[len(table_result)-1],tmp_table_df.loc[:].reset_index(drop=True))\n",
    "                if(product_df is not None):\n",
    "                    len_product_df_list=len(product_df_list)-1\n",
    "                    product_df_list[len_product_df_list]=pd.concat([product_df_list[len_product_df_list],product_df])\n",
    "                    # print(product_df.shape)\n",
    "                else:\n",
    "                    pass\n",
    "            continue\n",
    "\n",
    "        valid_columns_nums=[]\n",
    "        len_df=None\n",
    "        for index in range(0,base_row+1):\n",
    "            len_list=tmp_table_df.loc[index].map(lambda x:len(x))\n",
    "            for i in range(len(len_list)):\n",
    "                if len_list[i]==0 and len_df is not None:\n",
    "                    len_list[i]=len_df.tail(1)[i]\n",
    "            len_df=pd.DataFrame(len_list).T if len_df is None else pd.concat([len_df,pd.DataFrame(len_list).T]).reset_index(drop=True)\n",
    "            row_result=tmp_table_df.loc[index].map(lambda x:re.sub(r\"$[ ]+?\",\"\",re.sub(r\"^[ ]+?\",\"\",x)))\n",
    "            tmp_row= row_result if tmp_row is None else tmp_row+row_result\n",
    "            # valid_columns_nums.append(get_valid_columns_num(tmp_row))\n",
    "        \n",
    "        start_row=base_row+1\n",
    "\n",
    "        for index in range(base_row+1,tmp_table_df.shape[0]):\n",
    "            len_list=tmp_table_df.loc[index].map(lambda x:len(x))\n",
    "            len_judge=(pd.DataFrame(len_list).T-len_df.loc[len_df.shape[0]-1]).reset_index(drop=True)\n",
    "            len_judge=len_judge.T\n",
    "            if(len_judge[len_judge[0]>0].shape[0]>0):\n",
    "                break\n",
    "            \n",
    "            start_row+=1\n",
    "            for i in range(len(len_list)):\n",
    "                if len_list[i]==0 and len_df is not None:\n",
    "                    len_list[i]=len_df.tail(1)[i]\n",
    "            len_df=pd.DataFrame(len_list).T if len_df is None else pd.concat([len_df,pd.DataFrame(len_list).T]).reset_index(drop=True)\n",
    "            row_result=tmp_table_df.loc[index].map(lambda x:re.sub(r\"$[ ]+?\",\"\",re.sub(r\"^[ ]+?\",\"\",x)))\n",
    "            tmp_row= row_result if tmp_row is None else tmp_row+row_result\n",
    "\n",
    "            # valid_columns_nums.append(get_valid_columns_num(tmp_row))\n",
    "        \n",
    "        columns_list=tmp_row.dropna().map(lambda x:x.replace(\" \",\"\"))\n",
    "        valid_columns=find_valid_columns(tmp_table_df.iloc[start_row:,:])\n",
    "        tmp_valid_columns=get_valid_columns_index(columns_list)\n",
    "        # print(valid_columns)\n",
    "        # print(tmp_valid_columns)\n",
    "\n",
    "        difference=list(set(valid_columns).difference(tmp_valid_columns))\n",
    "        # print(difference)\n",
    "\n",
    "\n",
    "        # print(\"-------------\")\n",
    "        if(len(difference)>2):\n",
    "            # print(\"---------\")\n",
    "            union_set=set(valid_columns).union(tmp_valid_columns)\n",
    "            tmp_row[union_set]=\"添加字段\"\n",
    "            columns_list=tmp_row.dropna().map(lambda x:x.replace(\" \",\"\"))\n",
    "\n",
    "\n",
    "        \n",
    "        #处理错列问题\n",
    "        product_df=field_location_optimization(columns_list,tmp_table_df.loc[start_row:].reset_index(drop=True))\n",
    "        \n",
    "        first_line=[]\n",
    "        tmp_table_df.head(2).applymap(lambda x:first_line.append(x))\n",
    "        first_line_result=[]\n",
    "        for i in first_line:\n",
    "            i=re.sub(r\"^[ ]*?\",\"\",i)\n",
    "            i=re.sub(r\"$[ ]*?\",\"\",i)\n",
    "            # i=re.sub(r\"[ ]*?\",\"\",i)\n",
    "            if i != \"\":\n",
    "                first_line_result.append(i)\n",
    "        table_result.append(columns_list)\n",
    "        first_line_list.append(first_line_result)\n",
    "        start_rows_list.append(start_row)\n",
    "        product_df_list.append(product_df)\n",
    "        # print(len(table_result))\n",
    "        # print(len(first_line_list))\n",
    "        # print(len(start_rows_list))\n",
    "    return table_result,start_rows_list,first_line_list,product_df_list\n",
    "\n",
    "\n",
    "tmp_table_column={}\n",
    "tmp_table_column[\"sample_id\"]=[]\n",
    "tmp_table_column[\"columns\"]=[]\n",
    "tmp_table_column[\"start_row\"]=[]\n",
    "tmp_table_column[\"first_line\"]=[]\n",
    "tmp_table_column[\"product_df\"]=[]\n",
    "for sample_id,tabel in tqdm(val_df[[\"sample_id\",\"tabel\"]].values):##修改使用的数据集\n",
    "    table_result,start_rows_list,first_line_list,product_df_list=row_combine(sample_id,tabel)\n",
    "    for i in range(len(start_rows_list)):\n",
    "        valid_columns_index=get_valid_columns_index(table_result[i])\n",
    "        tmp_table_column[\"sample_id\"].append(sample_id)\n",
    "        tmp_table_column[\"columns\"].append(table_result[i].loc[valid_columns_index].T.reset_index(drop=True).T)\n",
    "        tmp_table_column[\"start_row\"].append(start_rows_list[i])\n",
    "        tmp_table_column[\"first_line\"].append(first_line_list[i])\n",
    "        tmp_table_column[\"product_df\"].append(product_df_list[i][valid_columns_index].T.reset_index(drop=True).T)\n",
    "\n",
    "tmp_table_column=pd.DataFrame(tmp_table_column)\n",
    "\n",
    "\n",
    "# tmp_table_column[[\"columns\",\"start_row\",\"first_line\"]]=val_df[\"tabel\"].head(5).apply(lambda x:row_combine(x))\n",
    "\n",
    "\n",
    "# tmp=val_df.iloc[177,:][[\"sample_id\",\"tabel\"]]\n",
    "# table_result,start_rows_list,first_line_list,product_df_list=row_combine(tmp[\"sample_id\"],tmp[\"tabel\"])\n",
    "# table_result\n",
    "# product_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-34f8e161c07f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mproduct_df_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_columns_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mproduct_df_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_columns_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "product_df_list[i]\n",
    "len(valid_columns_index)\n",
    "product_df_list[i][valid_columns_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1800/1800 [00:14<00:00, 123.31it/s]\n"
     ]
    }
   ],
   "source": [
    "title_num_char=[\"一、\",\"二、\",\"三、\",\"四、\",\"五、\",\"六、\",\"七、\",\"八、\",\"九、\",\"十、\",\"十一、\",\"十二、\",\"十三、\",\"十四、\",\"十五、\"]\n",
    "s_title_num_char=[\"（一）\",\"（二）\",\"（三）\",\"（四）\",\"（五）\",\"（六）\",\"（七）\",\"（八）\",\"（九）\",\"（十）\",\"（十一）\",\"（十二）\",\"（十三）\",\"（十四）\",\"（十五）\"]\n",
    "s_title_num_char.extend([\"[(]一[)]\",\"[(]二[)]\",\"[(]三[)]\",\"[(]四[)]\",\"[(]五[)]\",\"[(]六[)]\",\"[(]七[)]\",\"[(]八[)]\",\"[(]九[)]\",\"[(]十[)]\",\"[(]十一[)]\",\"[(]十二[)]\",\"[(]十三[)]\",\"[(]十四[)]\",\"[(]十五[)]\"])\n",
    "\n",
    "title_pos_words=[]\n",
    "title_neg_words=[\"备查\",\"日前\",\"过去\",\"履行\",\"审批\",\"程序\",\"风险\",\"措施\",\"影响\",\"累计\",\"赎回\",\"到期\",\"截至\",\"意见\",\"十二个月内\",\"公告前\",\"报备文件\",\"前期\"]\n",
    "\n",
    "\n",
    "def get_title(text):\n",
    "    global title_num_char\n",
    "    title_list=[]\n",
    "    title_type_list=[]\n",
    "    text_start_iter_list=[]\n",
    "    text_end_iter_list=[]\n",
    "    for item in title_num_char:\n",
    "        pattern = re.compile(item+r\"[ ]*?[^ ]+?[ ]\")\n",
    "        tmp=pattern.finditer(text)\n",
    "        for i in tmp:\n",
    "            title_list.append(i.group())\n",
    "            text_start_iter_list.append(i.span(0)[0])\n",
    "            title_type_list.append(1)\n",
    "            text_end_iter_list.append(i.span(0)[1])\n",
    "    \n",
    "    # for item in title_list:\n",
    "    for item in s_title_num_char:\n",
    "        pattern = re.compile(item+r\"[ ]*?[^ ]+?[ ]\")\n",
    "        tmp=pattern.finditer(text)\n",
    "        for i in tmp:\n",
    "            title_list.append(i.group())\n",
    "            text_start_iter_list.append(i.span(0)[0])\n",
    "            title_type_list.append(2)\n",
    "            text_end_iter_list.append(i.span(0)[1])\n",
    "\n",
    "    title_list.append(\"引言\")\n",
    "    title_type_list.append(1)\n",
    "    text_start_iter_list.append(0)\n",
    "    text_end_iter_list.append(0)\n",
    "\n",
    "    result_df=pd.DataFrame([title_list,title_type_list,text_start_iter_list,text_end_iter_list]).T.sort_values(by=2).reset_index(drop=True)\n",
    "    # print(result_df)\n",
    "    return result_df\n",
    "\n",
    "def get_title_text(text,title_df):\n",
    "    # print(title_df)\n",
    "    title_1_df=title_df[title_df[1]==1]\n",
    "    text_iter_list=[]\n",
    "    text_list=[]\n",
    "    # print(title_1_df)\n",
    "    for iter1,iter2 in title_1_df[[2,3]].values:\n",
    "        # print(iter1)\n",
    "        if(len(text_iter_list)!=0):\n",
    "            text_iter_list.append(iter1)\n",
    "        text_iter_list.append(iter2)\n",
    "    # text_iter_list.append(text_iter_list[len(text_iter_list)-1])\n",
    "    text_iter_list.append(len(text))\n",
    "    for index in range(int(len(text_iter_list)/2)):\n",
    "        text_list.append(text[text_iter_list[2*index]:text_iter_list[2*index+1]])\n",
    "    \n",
    "    title_1_df[4]=text_list\n",
    "\n",
    "    return title_1_df.reset_index(drop=True)\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "def judge_title(sample_id=0,text=r\"test\\n\"):\n",
    "    # print(text)\n",
    "    text=text.replace(r\"\\n\",\"\")\n",
    "    title_df=get_title(text)\n",
    "    title_df[\"sample_id\"]=[sample_id for x in range(title_df.shape[0])]\n",
    "    # print(title_df)\n",
    "    title_1_df=get_title_text(text,title_df)[[\"sample_id\",0,1,2,3,4]]\n",
    "\n",
    "    \n",
    "\n",
    "    global val_df\n",
    "    global train_outputs\n",
    "    val_true_name=train_outputs[train_outputs[\"sample_id\"]==sample_id][\"理财产品名称\"]\n",
    "    \n",
    "    index=0\n",
    "    neg_index=[]\n",
    "    for title_des in title_1_df[0].values:\n",
    "        for item in title_neg_words:\n",
    "            if re.search(item,title_des) is not None:\n",
    "                neg_index.append(index)\n",
    "                break\n",
    "        index+=1\n",
    "\n",
    "\n",
    "    return title_1_df.drop(neg_index)\n",
    "    # print(title_list)\n",
    "\n",
    "judge_title_result=None\n",
    "\n",
    "\n",
    "for sample_id,text in tqdm(val_df[[\"sample_id\",\"text\"]].values):\n",
    "    # print(sample_id)\n",
    "    # print(text)\n",
    "    judge_title_result= judge_title(sample_id,text) if judge_title_result is None else pd.concat([judge_title_result,judge_title(sample_id,text)])\n",
    "\n",
    "# judge_title_result.to_excel(\"训练集段落标题分类结果.xlsx\",index=None)\n",
    "\n",
    "\n",
    "# is_from_text(val_df[val_df[\"sample_id\"]==930][\"sample_id\"].iloc[0],val_df[val_df[\"sample_id\"]==930][\"text\"].iloc[0])\n",
    "# is_from_text(val_df[val_df[\"sample_id\"]==125][\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#fuzz裁剪，待优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "index=0\n",
    "invalid_list=[]\n",
    "for sample_id,first_line in tqdm(tmp_table_column[[\"sample_id\",\"first_line\"]].values):\n",
    "    current_judge_title=judge_title_result[judge_title_result[\"sample_id\"]==sample_id]\n",
    "    judge_flag=[]\n",
    "    for item in first_line:\n",
    "        judge_flag=[]\n",
    "        for i in current_judge_title[4].values:\n",
    "            judge_flag.append(fuzz.partial_token_sort_ratio(item,i))\n",
    "        if(np.mean(judge_flag)>0):\n",
    "            print(np.mean(judge_flag))\n",
    "            invalid_list.append(index)\n",
    "            break\n",
    "    index+=1\n",
    "# invalid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#单个理财产品行划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2018-06-24', '2018-06-24']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'20.01'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "a='2018年6月24日，2018年6月24日'\n",
    "t = TimeFinder()\n",
    "time_all = t.find_time(a)\n",
    "print(time_all)\n",
    "\n",
    "a=\"20.01万元\"\n",
    "re.sub(\"[^0-9.]\",\"\",a)\n",
    "a=\"1\"\n",
    "a=a+\"天\"\n",
    "a=[\"1\",\"2\",\"3\"]\n",
    "b=(\"\").join(i for i in a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2162/2162 [05:27<00:00,  6.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "\n",
    "\n",
    "#寻找居中列，即只占一行的列\n",
    "def find_min_columns(df):\n",
    "    count_list=[]\n",
    "    result=[]\n",
    "    df.copy().T.apply(lambda x:count_list.append(get_valid_columns_num(x.T)),axis=1)\n",
    "    # df.copy().T.apply(lambda x:print(x),axis=1)\n",
    "    # print(count_list)\n",
    "    max_count=np.max(count_list)\n",
    "    while (0 in count_list):\n",
    "        count_list[count_list.index(0)]=max_count\n",
    "    min_count=np.min(count_list)\n",
    "    index=0\n",
    "    for item in count_list:\n",
    "        if item ==min_count:\n",
    "            result.append(index)\n",
    "        index+=1\n",
    "    # print(result)   \n",
    "    return result\n",
    "def judge_time_exist(df):\n",
    "    judge_flag=[]\n",
    "    t = TimeFinder()\n",
    "    df.map(lambda x:judge_flag.append(t.find_time(x) is not None))\n",
    "    if True not in judge_flag:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def get_each_product_row(columns_list,df):\n",
    "    global column_neg_words\n",
    "    if df.shape[0]==0:\n",
    "        return None\n",
    "    df=df.applymap(lambda x:str(x).replace(\" \",\"\"))\n",
    "    max_valid_index_num=get_valid_columns_num(columns_list)\n",
    "\n",
    "    #字段个数不足，跳过\n",
    "    if(columns_list.shape[0]<=4):\n",
    "        # print(1)\n",
    "        return None\n",
    "\n",
    "    #存在敏感词，即为无效答案，跳过\n",
    "    judge_flag=[]    \n",
    "    for word in column_neg_words:\n",
    "        columns_list.map(lambda x:judge_flag.append(fuzz.partial_ratio(word,x)==100))\n",
    "    if(True in judge_flag):\n",
    "        # print(2)\n",
    "        return None\n",
    "    sum_rows=df.head(0)\n",
    "    each_row=None\n",
    "\n",
    "    #获取居中列\n",
    "    min_columns_list=find_min_columns(df)\n",
    "    max_index=-1\n",
    "    # print(min_columns_list)\n",
    "    for index in range(df.shape[0]):\n",
    "        # print(str(index)+\":------\")\n",
    "        # print(max_index)\n",
    "        if index<=max_index:\n",
    "            continue\n",
    "        tmp_row=df.loc[index]\n",
    "        # print(tmp_row.shape)\n",
    "        tmp_valid_num=get_valid_columns_num(tmp_row)\n",
    "        # print(tmp_valid_num)\n",
    "        # print(max_valid_index_num)\n",
    "        if tmp_valid_num>=max_valid_index_num-window_size:#第一种情况，列完整\n",
    "            #判断是否存在时间\n",
    "            # print(tmp_row)\n",
    "            if(judge_time_exist(tmp_row)):\n",
    "                each_row=tmp_row\n",
    "                sum_rows=pd.concat([sum_rows,each_row.to_frame().T])\n",
    "                each_row=None\n",
    "                continue\n",
    "            else:#不存在时间则视为不完整,加入each_row之后判断是否存在时间，存在即完整\n",
    "                if each_row is None:\n",
    "                    each_row=tmp_row\n",
    "                else:\n",
    "                    each_row=each_row+tmp_row\n",
    "                if(judge_time_exist(each_row)):\n",
    "                    sum_rows=pd.concat([sum_rows,each_row.to_frame().T])\n",
    "                    each_row=None\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "        elif(tmp_valid_num==1 and sum_rows.shape[0]==0):#第二种情况，列不完整,且长度为1,且sum_row无内容，认为是少数的错误字段遗留数据，舍弃\n",
    "            continue\n",
    "        elif((index+1)!=df.shape[0]):#第三种情况，列不完整，需要多行拼加\n",
    "            #寻找目前剩下的矩阵中与当前行以外存在值的最浅的居中列\n",
    "            \n",
    "            start_row=index+1\n",
    "            remain_df=df.iloc[start_row:,min_columns_list]\n",
    "            middle_index=index+1\n",
    "            middle_column=min_columns_list[0]\n",
    "            # print(middle_column)\n",
    "            for i in list(remain_df.index):\n",
    "                valid_columns_index=get_valid_columns_index(remain_df.loc[i])\n",
    "                if(len(valid_columns_index))>0:\n",
    "                    middle_index=i\n",
    "                    middle_column=valid_columns_index[0]\n",
    "                    break\n",
    "            # print(index)\n",
    "            # print(\"middle\")\n",
    "            # print(middle_index)\n",
    "\n",
    "            len_df=None\n",
    "\n",
    "            start_row=index\n",
    "            end_row=middle_index*2-index+1 if middle_index*2-index+1<=df.shape[0] else df.shape[0]\n",
    "            for i in range(start_row,end_row):\n",
    "                # print(df.shape)\n",
    "                if(get_valid_columns_num(df.loc[i])>=max_valid_index_num-window_size):#强制中断条件\n",
    "                    break\n",
    "                max_index=i\n",
    "                len_list=df.loc[i].map(lambda x:len(re.sub(\"[A-Za-z]*?\",\"\",x)))\n",
    "                for j in range(len(len_list)):\n",
    "                    if len_list[j]==0 and len_df is not None:\n",
    "                        len_list[j]=len_df.tail(1)[j]\n",
    "                len_df=pd.DataFrame(len_list).T if len_df is None else pd.concat([len_df,pd.DataFrame(len_list).T]).reset_index(drop=True)\n",
    "                each_row=df.loc[i] if each_row is None else each_row+df.loc[i]\n",
    "            \n",
    "\n",
    "            start_row=max_index+1\n",
    "            # print(start_row)\n",
    "            # print(max_index)\n",
    "            # print(len_df)\n",
    "\n",
    "            for i in range(start_row,df.shape[0]):\n",
    "                # print(df.shape)\n",
    "                if(get_valid_columns_num(df.loc[i])>=max_valid_index_num-window_size):#强制中断条件\n",
    "                    break\n",
    "                len_list=df.loc[i].map(lambda x:len(re.sub(\"[A-Za-z]*?\",\"\",x)))\n",
    "                # print(len_df)\n",
    "                len_judge=(pd.DataFrame(len_list).T-len_df.loc[len_df.shape[0]-1]).reset_index(drop=True)\n",
    "                len_judge=len_judge.T\n",
    "                # print(len_judge)\n",
    "                if(len_judge[len_judge[0]>0].shape[0]>0):\n",
    "                    break\n",
    "                max_index=i\n",
    "                # print(\"max\")\n",
    "                # print(max_index)\n",
    "                for j in range(len(len_list)):\n",
    "                    if len_list[j]==0 and len_df is not None:\n",
    "                        len_list[j]=len_df.tail(1)[j]\n",
    "                len_df=pd.DataFrame(len_list).T if len_df is None else pd.concat([len_df,pd.DataFrame(len_list).T]).reset_index(drop=True)\n",
    "                each_row=df.loc[i] if each_row is None else each_row+df.loc[i]\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            if(judge_time_exist(each_row)):\n",
    "                sum_rows=pd.concat([sum_rows,each_row.to_frame().T])\n",
    "                each_row=None\n",
    "                continue\n",
    "            else:\n",
    "                each_row=None\n",
    "                continue\n",
    "    # print(sum_rows)\n",
    "    return sum_rows\n",
    "\n",
    "\n",
    "column_neg_words=[\"实际收回\",\"收回\",\"赎回\",\"实际获得\",\"实际损益\",\"收益情况\",\"投资盈亏\",\"投资收益\",\"理财盈亏\",\"理财收益\",\"盈亏\",\"收益（元\",\"收益(元\",\"收益(万元\",\"收益（万元\",\"到期收益\",\"到期收\",\"是否到\",\"是否已\",\"目前状\",\"到期情\",\"到息情\"]\n",
    "sum_product_df=[]\n",
    "window_size=0 ##到时浮动（0-2）选择行数最多的一次\n",
    "index=0\n",
    "index_list=[]\n",
    "for sample_id,columns_list,product_df in tqdm(tmp_table_column[[\"sample_id\",\"columns\",\"product_df\"]].values):\n",
    "    # product_df\n",
    "    if(columns_list is None or product_df is None):\n",
    "        continue\n",
    "    # sample_id\n",
    "    each_sum_rows=get_each_product_row(columns_list,product_df.reset_index(drop=True))\n",
    "    if(each_sum_rows is not None):\n",
    "        sum_product_df.append(each_sum_rows)\n",
    "        index_list.append(index)\n",
    "    index+=1\n",
    "\n",
    "result_matrix=tmp_table_column.iloc[index_list,:]\n",
    "result_matrix[\"product_df\"]=sum_product_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          序号\n",
       "1         委托方\n",
       "2         受托方\n",
       "3      理财产品名称\n",
       "4    理财金额（万元）\n",
       "5        理财期限\n",
       "6      预期收益率%\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_id=4533\n",
    "# sample_id=7123\n",
    "tabel=val_df[val_df[\"sample_id\"]==sample_id][\"tabel\"].iloc[0]\n",
    "table_result,start_rows_list,first_line_list,product_df_list=row_combine(sample_id,tabel)\n",
    "table_result[0]\n",
    "product_df_list_ele=product_df_list[0].reset_index(drop=True)\n",
    "# product_df_list_ele\n",
    "each_sum_rows=get_each_product_row(table_result[0],product_df_list_ele.reset_index(drop=True))\n",
    "\n",
    "each_sum_rows2=result_matrix[result_matrix[\"sample_id\"]==sample_id][\"product_df\"].iloc[0]\n",
    "# # each_sum_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.抽取的是单独的数据包含\n",
    "#### 起息日，到息日， 金额，认购日期，产品发行方，理财产品"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12(天'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'123(个月'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'+5（年'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'2018-03-05'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'-182天'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=\"12(天123\"\n",
    "\n",
    "b=\"123(个月123123\"\n",
    "\n",
    "c=\"+5（年123\"\n",
    "\n",
    "re.search(\"\\d+?[(]*[（]*[天]+?\",a).group()\n",
    "re.search(\"\\d+?[(]*[（]*[个]+[月]+?\",b).group()\n",
    "re.search(\"[^\\d]+\\d[(]*[（]*[年]+?\",c).group()\n",
    "\n",
    "a=\"2018.03.05 \"\n",
    "b=\"2018.09.03\"\n",
    "c=a+b\n",
    "t = TimeFinder()\n",
    "c=sorted(t.find_time(c),reverse=True)\n",
    "a=c[0]\n",
    "b=c[1]\n",
    "\n",
    "len(pd.Series(a).shape)\n",
    "d1 = datetime.datetime.strptime(a, '%Y-%m-%d')\n",
    "d2 = datetime.datetime.strptime(b, '%Y-%m-%d')\n",
    "d = d2 - d1\n",
    "\n",
    "delta = datetime.timedelta(days=d.days)\n",
    "\n",
    "datetime.datetime.strftime(d1+datetime.timedelta(days=d.days), '%Y-%m-%d')\n",
    "str(d.days) + '天'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1122/1122 [04:27<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "temp_single={}\n",
    "temp_single['认购日期'] = []\n",
    "temp_single['产品起息日'] = []\n",
    "temp_single['产品到息日'] = []\n",
    "temp_single['产品期限'] = []\n",
    "temp_single['认购金额(万元)'] = []\n",
    "temp_single['产品发行方名称'] = []\n",
    "temp_single['理财产品名称'] = []\n",
    "temp_single['sample_id'] = []\n",
    "\n",
    "            \n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def judge_type(columns):\n",
    "    type_index=[]#1:产品名,2:金额,3:发行方，4:期限\n",
    "    columns=columns.map(lambda x:x.replace(\"（\",\"(\").replace(\"）\",\")\"))\n",
    "    product_name_pos_words=[\"产品名称\",\"产品名册\",\"产品名\",\"理财产品\",\"项目名\",\"回购名\",\"回购品\",\"标的名\",\"金融产\",\"投资项\"]#\"存款种类\",\"基金类型\"#不能为空\n",
    "    # product_name_neg_words=[\"编号\",\"代码\"]\n",
    "    amt_pos_words=[\"存款金\",\"认购金\",\"投资金\",\"投入金\",\"受托金\",\"理财金\",\"金额\",\"（元\",\"(元\",\"(万元\",\"（万元\",\"(亿元\",\"（亿元\",\"人民币\",\"投资规\",\"认购规\",\"存款规\",\"投入规\",\"理财规\"]\n",
    "    counter_name_pos_words=[\"受托方\",\"银行机\",\"机构名\",\"合作方名\",\"合作银\",\"合作机\",\"受托人\",\"发行主\",\"签约方\",\"协议方\",\"受托机\",\"受托银\",\"认购银\",\"签约银\",\"签约机\",\"协议机\",\"发生主\",\"存放银\",\"存款银\",\"存款机\",\"存放机\",\"购买银\",\"购买机\",\"管理人\",\"管理银\",\"管理机\",\"银行名\",\"发行机\",\"发行主\",\"发行人\",\"对手方\",\"开户银\",\"开户行\",\"开户机\"]#可以为空\n",
    "    time_length_pos_words=[\"期限\",\"(天)\",\"持有时间\"]\n",
    "    for words in product_name_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        # columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==0))\n",
    "        if True in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==0):\n",
    "        for words in [\"种类\",\"类型\",\"类别\"]:\n",
    "            judge_flag=[]\n",
    "            columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "            # columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==0))\n",
    "            if True  in judge_flag:\n",
    "                type_index.append(judge_flag.index(True))\n",
    "                break\n",
    "        if(len(type_index)==0):\n",
    "            type_index.append(-1)\n",
    "        \n",
    "\n",
    "    for words in amt_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==1):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in counter_name_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==2):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    for words in time_length_pos_words:\n",
    "        judge_flag=[]\n",
    "        columns.map(lambda x:judge_flag.append(fuzz.partial_ratio(words,x)==100))\n",
    "        if True  in judge_flag:\n",
    "            type_index.append(judge_flag.index(True))\n",
    "            break\n",
    "    if(len(type_index)==3):\n",
    "        type_index.append(-1)\n",
    "    \n",
    "    \n",
    "    return type_index\n",
    "\n",
    "for sample_id,columns,product_df in tqdm(result_matrix[result_matrix[\"sample_id\"]!=-1][[\"sample_id\",\"columns\",\"product_df\"]].values):\n",
    "    \n",
    "    type_index=judge_type(columns)\n",
    "    # product_df\n",
    "    # columns\n",
    "    # type_index\n",
    "    for index in product_df.index:\n",
    "        tmp_df=product_df.loc[index]\n",
    "        if(len(tmp_df.shape) ==2 ):\n",
    "            tmp_df=tmp_df.reset_index(drop=True).loc[0]\n",
    "        product_name=\"\"\n",
    "        amt=\"\"\n",
    "        counter_name=\"\"\n",
    "        pur_dt=\"\"\n",
    "        val_dt=\"\"\n",
    "        coupon_dt=\"\"\n",
    "        time_limit=\"\"\n",
    "        #产品名\n",
    "        if(type_index[0]!=-1):\n",
    "            if (str(grocery.predict(tmp_df.loc[type_index[0]])) == \"理财产品\"):\n",
    "                product_name=tmp_df.loc[type_index[0]]\n",
    "        else:\n",
    "            candidate_list={}\n",
    "            candidate_list[\"理财产品\"]=[]\n",
    "            candidate_list[\"发行方\"]=[]\n",
    "            candidate_list[\"其它\"]=[]\n",
    "            for each_word in tmp_df.head(1):\n",
    "                if not (is_number(each_word)):\n",
    "                    candidate_list[str(grocery.predict(each_word))].append(each_word)\n",
    "            if(len(candidate_list[\"理财产品\"])!=0):\n",
    "                product_name=candidate_list[\"理财产品\"][0]\n",
    "        #金额\n",
    "        if(type_index[1]!=-1):\n",
    "            amt=tmp_df.loc[type_index[1]].replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\")\n",
    "            type_amt=0\n",
    "            if(\"万\" in amt or \"万\" in columns.loc[type_index[1]]):\n",
    "                type_amt=1\n",
    "            if(\"亿\" in amt or \"亿\" in columns.loc[type_index[1]]):\n",
    "                type_amt=2\n",
    "            amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "            if(is_number(amt)):\n",
    "                amt=float(amt)\n",
    "                if(type_amt==0 and amt/10000 >float(50)):\n",
    "                    amt/=10000\n",
    "                if(type_amt==2):\n",
    "                    amt*=10000\n",
    "        else:\n",
    "            candidate_list=[]\n",
    "            value_list=list(tmp_df)\n",
    "            for item in value_list:\n",
    "                tmp=str(item).replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\").replace(\"亿\",\"\").replace(\"万\",\"\")\n",
    "                if(is_number(tmp)):\n",
    "                    candidate_list.append(float(tmp))\n",
    "\n",
    "            if len(candidate_list)>0:\n",
    "                real_tmp=sorted(candidate_list,reverse=True)[0]\n",
    "\n",
    "                for item in value_list:\n",
    "                    tmp=str(item).replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\").replace(\"元\",\"\").replace(\"圆\",\"\").replace(\"亿\",\"\").replace(\"万\",\"\")\n",
    "                    if(is_number(tmp) and float(tmp)==real_tmp):\n",
    "                        amt=item\n",
    "                        type_amt=0\n",
    "                    else:\n",
    "                        continue\n",
    "                    if(\"万\" in amt ):\n",
    "                        type_amt=1\n",
    "                    if(\"亿\" in amt ):\n",
    "                        type_amt=2\n",
    "                    amt=re.sub(\"[^0-9.]\",\"\",amt)\n",
    "                    if(is_number(amt)):\n",
    "                        amt=float(amt)\n",
    "                        # print(amt)\n",
    "                        if(type_amt==0 and amt/10000 >float(50)):\n",
    "                            amt/=10000\n",
    "                        if(type_amt==2):\n",
    "                            amt*=10000\n",
    "                    if(amt !=\"\" or amt!=np.nan):\n",
    "                        break\n",
    "        #发行方\n",
    "        if(type_index[2]!=-1):\n",
    "            if (str(grocery.predict(tmp_df.loc[type_index[2]])) == \"发行方\"):\n",
    "                counter_name=tmp_df.loc[type_index[2]]\n",
    "        else:\n",
    "            candidate_list={}\n",
    "            candidate_list[\"理财产品\"]=[]\n",
    "            candidate_list[\"发行方\"]=[]\n",
    "            candidate_list[\"其它\"]=[]\n",
    "            for each_word in tmp_df.head(1):\n",
    "                if not (is_number(each_word)):\n",
    "                    candidate_list[str(grocery.predict(each_word))].append(each_word)\n",
    "            if(len(candidate_list[\"发行方\"])!=0):\n",
    "                counter_name=candidate_list[\"发行方\"][0]\n",
    "        \n",
    "        #期限\n",
    "        if(type_index[3]!=-1):\n",
    "            text=str(tmp_df.loc[type_index[3]])\n",
    "            a=re.search(\"\\d+?[天]+?\",text)\n",
    "            if (a is None):\n",
    "                a=re.search(\"\\d+?[个]+[月]+?\",text)\n",
    "            if a is None:\n",
    "                a=re.search(\"[^\\d]\\d[年]+?\",text)\n",
    "                if(a is not None):\n",
    "                    a=re.search(\"\\d[年]+?\",a.group())\n",
    "            if a is None:\n",
    "                a=re.search(\"^\\d[年]+?\",text)\n",
    "            if a is not None:\n",
    "                time_limit=a.group().replace(\"（\",\"\").replace(\"）\",\"\").replace(\"(\",\"\").replace(\"(\",\"\")\n",
    "            else:\n",
    "                if(is_number(text)):\n",
    "                    time_limit=str(time_limit)+\"天\"\n",
    "                else:\n",
    "                    time_limit=\"\"\n",
    "        #三个日期\n",
    "        # tmp_df\n",
    "        value_list=[]\n",
    "        noshow=tmp_df.map(lambda x:value_list.append(str(x)))\n",
    "        sum_value=(\" and \").join(i for i in value_list)\n",
    "        t = TimeFinder()\n",
    "        time_all = t.find_time(sum_value)\n",
    "        # print(time_all)\n",
    "        if(time_all is not None):\n",
    "            if(len(time_all)==1):\n",
    "                pur_dt=time_all[0]\n",
    "                val_dt=pur_dt\n",
    "            elif(len(time_all)>=2):\n",
    "                time_all=sorted(time_all,reverse=True)\n",
    "                pur_dt=time_all[1]\n",
    "                val_dt=pur_dt\n",
    "                coupon_dt = time_all[0]\n",
    "                try:\n",
    "                    # 相减\n",
    "                    if(type_index[3]!=-1 or time_limit==\"\"):\n",
    "                        d1 = datetime.datetime.strptime(val_dt, '%Y-%m-%d')\n",
    "                        d2 = datetime.datetime.strptime(coupon_dt, '%Y-%m-%d')\n",
    "                        d = d2 - d1\n",
    "                        time_limit = str(d.days) + '天'\n",
    "\n",
    "\n",
    "                except Exception:\n",
    "                    val_dt = \"\"\n",
    "                    time_limit = \"\"\n",
    "            \n",
    "            if pur_dt!=\"\" and coupon_dt==\"\" and time_limit!=\"\":\n",
    "                try:\n",
    "                    coupon_dt=datetime.datetime.strftime(datetime.datetime.strptime(pur_dt, '%Y-%m-%d')+datetime.timedelta(days=int(re.search(\"\\d*\",time_limit).group())), '%Y-%m-%d')\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        temp_single['认购日期'].append(pur_dt)\n",
    "        temp_single['产品起息日'].append(val_dt)\n",
    "        temp_single['产品到息日'].append(coupon_dt)\n",
    "        temp_single['产品期限'] .append(time_limit)\n",
    "        temp_single['认购金额(万元)'].append(amt)\n",
    "        temp_single['产品发行方名称'] .append(counter_name)\n",
    "        temp_single['理财产品名称'] .append(product_name)\n",
    "        temp_single['sample_id'].append(sample_id)\n",
    "    \n",
    "temp_single=pd.DataFrame(temp_single)\n",
    "temp_single.to_excel(\"result.xlsx\",index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_single.to_excel(\"result.xlsx\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "judge_title(1067,train_df[train_df[\"sample_id\"]==1067][\"text\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 直接提取时间\n",
    "# 如果出现两个时间第一个就是起息日，第二个就是到期日\n",
    "# 如果出现一个时间就是起息日\n",
    "# 出现的第一个money就是最后的金额\n",
    "# 从这里面抽取所有序列\n",
    "# 这里认为有逗号出现的就是money\n",
    "\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "from src.time_extractor import TimeFinder\n",
    "import datetime\n",
    "def get_list_data(df):\n",
    "    df = list(df)\n",
    "    new_df = []\n",
    "    for i in tqdm(df):\n",
    "        temp_df = []\n",
    "        for h in i:\n",
    "            new_h = []\n",
    "            for digital in h:\n",
    "                if ',' in digital:\n",
    "                    # 这里也是为了统一数据有些是用元，有些是用万元\n",
    "                    try:\n",
    "                        ttt = float(digital.replace(',', '').replace('万元', '').replace('人民币', '').replace('元', ''))\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    if ttt > 20000:\n",
    "                        ttt = ttt/10000\n",
    "                    new_h.append(ttt)\n",
    "                else:\n",
    "                    continue\n",
    "            if len(new_h) == 0:\n",
    "                continue\n",
    "            temp_single = {}\n",
    "            a = '_'.join(h)\n",
    "            # 抽取时间和money\n",
    "            t = TimeFinder()\n",
    "            time_all = t.find_time(a)\n",
    "            if time_all == None:\n",
    "                continue\n",
    "            rgrq = time_all[0]\n",
    "            cpqxr = time_all[0]\n",
    "            if len(time_all) > 1:\n",
    "                try:\n",
    "                    cpdxr = time_all[1]\n",
    "                    # 相减\n",
    "                    d1 = datetime.datetime.strptime(cpqxr, '%Y-%m-%d')\n",
    "                    d2 = datetime.datetime.strptime(cpdxr, '%Y-%m-%d')\n",
    "                    d = d2 - d1\n",
    "                    cpqx = str(d.days) + '天'\n",
    "                except Exception:\n",
    "                    cpdxr = np.nan\n",
    "                    cpqx = np.nan\n",
    "            else:\n",
    "                cpdxr = np.nan\n",
    "                cpqx = np.nan\n",
    "                \n",
    "            # 筛选出除开数字与包含时间的列\n",
    "            # 末尾是\n",
    "            last_two = ['公司', '银行', '信托', '证券',  '分行', '支行', '中心', '业部', '商行', '建行']\n",
    "            mowei = np.nan\n",
    "            selected_bank_and_works = []\n",
    "            for l in h:\n",
    "                new_l = list(str(l))\n",
    "                new_l_test = ''.join(l[-2:])\n",
    "                if new_l_test in last_two:\n",
    "                    mowei = l\n",
    "                    continue\n",
    "                if '资金' in l:\n",
    "                    continue\n",
    "                if '收益' in l:\n",
    "                    continue\n",
    "                if '到期' in l:\n",
    "                    continue\n",
    "                if ',' in l:\n",
    "                    continue\n",
    "                if '.' in l:\n",
    "                    continue\n",
    "                if '/' in l:\n",
    "                    continue\n",
    "                if '年' in l:\n",
    "                    continue\n",
    "                if '-' in l:\n",
    "                    continue\n",
    "                if len(l) < 4:\n",
    "                    continue\n",
    "                if is_number(l):\n",
    "                    continue\n",
    "                selected_bank_and_works.append(l)\n",
    "            if len(selected_bank_and_works) < 1:\n",
    "                continue\n",
    "            \n",
    "            temp_single['认购日期'] = rgrq\n",
    "            temp_single['产品起息日'] = cpqxr\n",
    "            temp_single['产品到期日'] = cpdxr\n",
    "            temp_single['产品期限'] = cpqx\n",
    "            temp_single['认购金额(万元)'] = new_h[0]\n",
    "            temp_single['产品发行方名称'] = mowei\n",
    "            temp_single['理财产品名称'] = selected_bank_and_works[0]\n",
    "            temp_df.append(temp_single)\n",
    "        new_df.append(temp_df)\n",
    "    return new_df\n",
    "\n",
    "val_contain_date = get_list_data(val_df_tabel)\n",
    "# test_contain_data = get_list_data(test_df_tabel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.汇总整理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 将前面提取到的数据整理成对应格式\n",
    "# sample_id_list = []\n",
    "# rgrq_list = []\n",
    "# lccp_list = []\n",
    "# cpfxf_list = []\n",
    "# rgje_list = []\n",
    "# cpqxr_list = []\n",
    "# cpdxr_list = []\n",
    "# cpqx_list = []\n",
    "# sjgmgsmc_list = []\n",
    "# ggrq_list = []\n",
    "\n",
    "# sample_id = list(val_df['sample_id'])\n",
    "# gg = list(val_gm)\n",
    "# time = list(val_time)\n",
    "# for i, value in enumerate(sample_id):\n",
    "#     for j in val_contain_date[i]:\n",
    "#         sample_id_list.append(sample_id[i])\n",
    "#         rgrq_list.append(j['认购日期'])\n",
    "#         lccp_list.append(j['理财产品名称'])\n",
    "#         cpfxf_list.append(j['产品发行方名称'])\n",
    "#         rgje_list.append(j['认购金额(万元)'])\n",
    "#         cpqxr_list.append(j['产品起息日'])\n",
    "#         cpdxr_list.append(j['产品到期日'])\n",
    "#         cpqx_list.append(j['产品期限'])\n",
    "#         sjgmgsmc_list.append(gg[i])\n",
    "#         ggrq_list.append(time[i])\n",
    "\n",
    "# result = pd.DataFrame()\n",
    "# result['sample_id'] = sample_id_list\n",
    "# result['认购日期'] = rgrq_list\n",
    "# result['理财产品名称'] = lccp_list\n",
    "# result['产品发行方名称'] = cpfxf_list\n",
    "# result['认购金额(万元)'] = rgje_list\n",
    "# result['产品起息日'] = cpqxr_list\n",
    "# result['产品到期日'] = cpdxr_list\n",
    "# result['产品期限'] = cpqx_list\n",
    "# result['实际购买公司名称'] = sjgmgsmc_list\n",
    "# result['公告日期'] = ggrq_list\n",
    "# val_result = result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_id_list = []\n",
    "rgrq_list = []\n",
    "lccp_list = []\n",
    "cpfxf_list = []\n",
    "rgje_list = []\n",
    "cpqxr_list = []\n",
    "cpdxr_list = []\n",
    "cpqx_list = []\n",
    "sjgmgsmc_list = []\n",
    "ggrq_list = []\n",
    "\n",
    "sample_id = list(test_df['sample_id'])\n",
    "gg = list(test_gm)\n",
    "time = list(test_time)\n",
    "for i, value in enumerate(sample_id):\n",
    "    for j in test_contain_data[i]:\n",
    "        sample_id_list.append(sample_id[i])\n",
    "        rgrq_list.append(j['认购日期'])\n",
    "        lccp_list.append(j['理财产品名称'])\n",
    "        cpfxf_list.append(j['产品发行方名称'])\n",
    "        rgje_list.append(j['认购金额(万元)'])\n",
    "        cpqxr_list.append(j['产品起息日'])\n",
    "        cpdxr_list.append(j['产品到期日'])\n",
    "        cpqx_list.append(j['产品期限'])\n",
    "        sjgmgsmc_list.append(gg[i])\n",
    "        ggrq_list.append(time[i])\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['sample_id'] = sample_id_list\n",
    "result['认购日期'] = rgrq_list\n",
    "result['理财产品名称'] = lccp_list\n",
    "result['产品发行方名称'] = cpfxf_list\n",
    "result['认购金额(万元)'] = rgje_list\n",
    "result['产品起息日'] = cpqxr_list\n",
    "result['产品到期日'] = cpdxr_list\n",
    "result['产品期限'] = cpqx_list\n",
    "result['实际购买公司名称'] = sjgmgsmc_list\n",
    "result['公告日期'] = ggrq_list\n",
    "test_result = result\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_judge(result,score_limit=31):\n",
    "    global judge_title_result\n",
    "    drop_list=[]\n",
    "    index=0\n",
    "    for sample_id,product_name in tqdm(result[[\"sample_id\",\"理财产品名称\"]].values):\n",
    "        # print(sample_id)\n",
    "        score_list=[]\n",
    "        for text in judge_title_result[judge_title_result[\"sample_id\"]==int(sample_id)][4].values:\n",
    "            # print(text)\n",
    "            score_list.append(fuzz.partial_token_sort_ratio(product_name,text))\n",
    "        # print(score_list)\n",
    "        if  len(score_list)>0 and np.max(pd.DataFrame(score_list)[0])<=score_limit:\n",
    "            drop_list.append(index)\n",
    "        index+=1 \n",
    "    \n",
    "    return result.copy().reset_index(drop=True).drop(drop_list)\n",
    "\n",
    "# drop_judge(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 154.01it/s][]\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>认购日期</th>\n",
       "      <th>产品起息日</th>\n",
       "      <th>产品到息日</th>\n",
       "      <th>产品期限</th>\n",
       "      <th>认购金额(万元)</th>\n",
       "      <th>产品发行方名称</th>\n",
       "      <th>理财产品名称</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>93天</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>中国银行股份有限公司厦门杏林支行</td>\n",
       "      <td>中银平稳理财计划-智荟系列</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>91天</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>厦门银行股份有限公司杏林支行</td>\n",
       "      <td>结构性存款</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-04-17</td>\n",
       "      <td>2019-04-17</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>89天</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>兴证证券资产管理有限公司</td>\n",
       "      <td>兴证资管鑫利5号集合资产管理计划</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>91天</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>兴业银行股份有限</td>\n",
       "      <td>兴业银行“金雪球-优悦”</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>2018-06-29</td>\n",
       "      <td>52天</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>中信银行股份有限公司福州分行</td>\n",
       "      <td>中信理财之共赢利率结构19980期人民币结构性理财产品</td>\n",
       "      <td>4188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6324</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6325</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6326</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6327</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6328</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            认购日期       产品起息日       产品到息日   产品期限 认购金额(万元)           产品发行方名称  \\\n",
       "15    2019-04-16  2019-04-16  2019-07-18    93天   4000.0  中国银行股份有限公司厦门杏林支行   \n",
       "16    2019-04-16  2019-04-16  2019-07-16    91天   3000.0    厦门银行股份有限公司杏林支行   \n",
       "17    2019-04-17  2019-04-17  2019-07-15    89天   3000.0      兴证证券资产管理有限公司   \n",
       "18    2019-04-16  2019-04-16  2019-07-16    91天  10000.0          兴业银行股份有限   \n",
       "19    2018-05-08  2018-05-08  2018-06-29    52天   3000.0    中信银行股份有限公司福州分行   \n",
       "...          ...         ...         ...    ...      ...               ...   \n",
       "6324  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "6325  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "6326  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "6327  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "6328  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "\n",
       "                           理财产品名称 sample_id  \n",
       "15                  中银平稳理财计划-智荟系列      3581  \n",
       "16                          结构性存款      3581  \n",
       "17               兴证资管鑫利5号集合资产管理计划      3581  \n",
       "18                   兴业银行“金雪球-优悦”      3581  \n",
       "19    中信理财之共赢利率结构19980期人民币结构性理财产品      4188  \n",
       "...                           ...       ...  \n",
       "6324         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "6325         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "6326         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "6327         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "6328         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "\n",
       "[5571 rows x 8 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5697/5697 [00:15<00:00, 357.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1427909133055169"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5122/5122 [00:15<00:00, 335.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14457831325301201"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4961/4961 [00:15<00:00, 319.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14521954001140466"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4859/4859 [00:14<00:00, 340.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14596303036107652"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4778/4778 [00:13<00:00, 343.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14745204417748498"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4659/4659 [00:14<00:00, 330.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14892987474109873"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4476/4476 [00:13<00:00, 332.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1500199760287655"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4349/4349 [00:13<00:00, 324.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15103954341622502"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4149/4149 [00:12<00:00, 328.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1523219814241486"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4027/4027 [00:12<00:00, 330.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15420314312836197"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3818/3818 [00:11<00:00, 326.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15432429535955416"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3668/3668 [00:11<00:00, 326.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1549902996335417"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3615/3615 [00:10<00:00, 339.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15500163274191794"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3524/3524 [00:10<00:00, 341.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15512031337437043"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3272/3272 [00:09<00:00, 340.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15522655899506504"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3253/3253 [00:09<00:00, 340.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15542986425339367"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3177/3177 [00:09<00:00, 346.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15465810004589262"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62%|██████▏   | 1899/3053 [00:05<00:03, 333.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-329-ec59868211a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"result_r.xlsx\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m   \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop_judge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m   \u001b[1;31m# t_p_r=r\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;31m# r.to_excel(\"result_after_drop.xlsx\",index=None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-323-2644d75dc015>\u001b[0m in \u001b[0;36mdrop_judge\u001b[1;34m(result, score_limit)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# print(sample_id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mscore_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjudge_title_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjudge_title_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"sample_id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;31m# print(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mscore_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuzz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_token_sort_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "  def get_F1(val_pred, val_true):\n",
    "      val_pred = list(val_pred)\n",
    "      val_true = list(val_true)\n",
    "      curr = list(set(val_pred).intersection(set(val_true)))\n",
    "      R = len(curr)/len(val_true)\n",
    "      P = len(curr)/len(val_pred)\n",
    "      return 2*P*R/(P+R)\n",
    "\n",
    "  r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "  val_true = r['sample_id'].astype(str) + r['理财产品名称'].astype(str) \n",
    "  # t_t_r=r\n",
    "  r = temp_single\n",
    "  noshow=r.fillna(\"\").reset_index(drop=True)\n",
    "  i=0\n",
    "  i_list=[]\n",
    "  for index in r.index:\n",
    "      if r.loc[index].dropna().shape[0]<=5 or type(r.loc[index][\"理财产品名称\"]) is float or len(r.loc[index][\"理财产品名称\"])<2:\n",
    "        i_list.append(i)\n",
    "      i+=1\n",
    "  r=r.drop(i_list)\n",
    "  r=r.fillna(\"\").applymap(lambda x:str(x).replace(\" \",\"\"))\n",
    "  r.to_excel(\"result_r.xlsx\",index=None)\n",
    "  for i in range(20,50):\n",
    "    r=drop_judge(r,i)\n",
    "    # t_p_r=r\n",
    "    # r.to_excel(\"result_after_drop.xlsx\",index=None)\n",
    "    val_pred = r['sample_id'].astype(str)+ r['理财产品名称'].astype(str) \n",
    "    score = get_F1(val_pred, val_true)\n",
    "    score\n",
    "    i\n",
    "  # val_pred = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) +r['产品发行方名称'].astype(str)\n",
    "  # curr = list(set(list(val_pred)).difference(list(val_true)))\n",
    "\n",
    "  # score = get_F1(val_pred, val_true)\n",
    "  # score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p_r=t_p_r.fillna(\"null\")\n",
    "t_t_r=t_t_r.fillna(\"null\")\n",
    "t_t_r=t_t_r.astype(str)\n",
    "columns=[\"sample_id\",\"理财产品名称\",\"产品发行方名称_x\",\"产品发行方名称_y\",\"认购金额(万元)_x\",\"认购金额(万元)_y\",\"认购日期_x\",\"认购日期_y\",\"产品起息日_x\",\"产品起息日_y\",\"产品到息日_x\",\"产品到息日_y\",\"产品期限_x\",\"产品期限_y\"]\n",
    "\n",
    "t_p_r[\"sample_id\"]=t_p_r[\"sample_id\"].astype(str)\n",
    "t_t_r[\"sample_id\"]=t_t_r[\"sample_id\"].astype(str)\n",
    "p_vs_t=pd.merge(t_p_r,t_t_r,on=[\"sample_id\",\"理财产品名称\"])[columns]\n",
    "p_vs_t.to_excel(\"result_p_vs_t.xlsx\",index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>认购日期</th>\n",
       "      <th>产品起息日</th>\n",
       "      <th>产品到息日</th>\n",
       "      <th>产品期限</th>\n",
       "      <th>认购金额(万元)</th>\n",
       "      <th>产品发行方名称</th>\n",
       "      <th>理财产品名称</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-07-18</td>\n",
       "      <td>93天</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>中国银行股份有限公司厦门杏林支行</td>\n",
       "      <td>中银平稳理财计划-智荟系列</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>91天</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>厦门银行股份有限公司杏林支行</td>\n",
       "      <td>结构性存款</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-17</td>\n",
       "      <td>2019-04-17</td>\n",
       "      <td>2019-07-15</td>\n",
       "      <td>89天</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>兴证证券资产管理有限公司</td>\n",
       "      <td>兴证资管鑫利5号集合资产管理计划</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-04-16</td>\n",
       "      <td>2019-07-16</td>\n",
       "      <td>91天</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>兴业银行股份有限</td>\n",
       "      <td>兴业银行“金雪球-优悦”</td>\n",
       "      <td>3581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>2018-05-08</td>\n",
       "      <td>2018-06-29</td>\n",
       "      <td>52天</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>中信银行股份有限公司福州分行</td>\n",
       "      <td>中信理财之共赢利率结构19980期人民币结构性理财产品</td>\n",
       "      <td>4188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>2018-05-23</td>\n",
       "      <td>2018-05-23</td>\n",
       "      <td></td>\n",
       "      <td>周期赎回天</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>建设银行</td>\n",
       "      <td>乾元-周周利开放式保本理财产品“乾元-日</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3185 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            认购日期       产品起息日       产品到息日   产品期限 认购金额(万元)           产品发行方名称  \\\n",
       "0     2019-04-16  2019-04-16  2019-07-18    93天   4000.0  中国银行股份有限公司厦门杏林支行   \n",
       "1     2019-04-16  2019-04-16  2019-07-16    91天   3000.0    厦门银行股份有限公司杏林支行   \n",
       "2     2019-04-17  2019-04-17  2019-07-15    89天   3000.0      兴证证券资产管理有限公司   \n",
       "3     2019-04-16  2019-04-16  2019-07-16    91天  10000.0          兴业银行股份有限   \n",
       "4     2018-05-08  2018-05-08  2018-06-29    52天   3000.0    中信银行股份有限公司福州分行   \n",
       "...          ...         ...         ...    ...      ...               ...   \n",
       "5566  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "5567  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "5568  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "5569  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "5570  2018-05-23  2018-05-23              周期赎回天   6000.0              建设银行   \n",
       "\n",
       "                           理财产品名称 sample_id  \n",
       "0                   中银平稳理财计划-智荟系列      3581  \n",
       "1                           结构性存款      3581  \n",
       "2                兴证资管鑫利5号集合资产管理计划      3581  \n",
       "3                    兴业银行“金雪球-优悦”      3581  \n",
       "4     中信理财之共赢利率结构19980期人民币结构性理财产品      4188  \n",
       "...                           ...       ...  \n",
       "5566         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "5567         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "5568         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "5569         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "5570         乾元-周周利开放式保本理财产品“乾元-日       193  \n",
       "\n",
       "[3185 rows x 8 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "  def get_F1(val_pred, val_true):\n",
    "      val_pred = list(val_pred)\n",
    "      val_true = list(val_true)\n",
    "      curr = list(set(val_pred).intersection(set(val_true)))\n",
    "      R = len(curr)/len(val_true)\n",
    "      P = len(curr)/len(val_pred)\n",
    "      return 2*P*R/(P+R)\n",
    "\n",
    "  r = pd.merge(val_df[['sample_id']], train_outputs, on='sample_id', how='left')\n",
    "  val_true = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到息日'].astype(str) + r['产品期限'].astype(str) +r['产品发行方名称'].astype(str)\n",
    "\n",
    "  r = drop_judge(result)\n",
    "  print(r.shape)\n",
    "  val_pred = r['sample_id'].astype(str) + r['认购日期'].astype(str) + r['理财产品名称'].astype(str) + r['认购金额(万元)'].astype(str) + r['产品起息日'].astype(str)+ r['产品到期日'].astype(str) + r['产品期限'].astype(str) +r['产品发行方名称'].astype(str)\n",
    "\n",
    "  score = get_F1(val_pred, val_true)\n",
    "  score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "val_result_file=val_result.sort_values(by='sample_id').reset_index(drop=True)\n",
    "val_true_file=pd.merge(train_outputs, val_df['sample_id'], on='sample_id',how=\"right\").sort_values(by=\"sample_id\").reset_index(drop=True)\n",
    "# train_outputs\n",
    "# val_result_file\n",
    "# val_true_file\n",
    "val_id_list=val_result_file[\"sample_id\"].unique()\n",
    "val_id_count={}\n",
    "val_id_count[\"sample_id\"]=[]\n",
    "val_id_count[\"预测\"]=[]\n",
    "val_id_count[\"实际\"]=[]\n",
    "for item in val_id_list:\n",
    "    val_id_count[\"sample_id\"].append(item)\n",
    "    val_id_count[\"预测\"].append(val_result_file[val_result_file[\"sample_id\"]==item].shape[0])\n",
    "    val_id_count[\"实际\"].append(val_true_file[val_true_file[\"sample_id\"]==item].shape[0])\n",
    "\n",
    "val_id_count=pd.DataFrame(val_id_count)\n",
    "val_id_count[\"差值\"]=val_id_count[\"预测\"]-val_id_count[\"实际\"]\n",
    "print(val_id_count[val_id_count[\"差值\"]==0].shape[0]/val_id_count.shape[0])\n",
    "val_id_count.to_excel(\"验证集row数量对比.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.采用LSTM网络用提取好的部分跟pdf中的text做交互预测\n",
    "理财类型、资金来源、实际购买公司和上市公司关系、买卖方是否有关联关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把result_matrix的表格全变成文本\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1122/1122 [00:02<00:00, 388.37it/s]\n"
     ]
    }
   ],
   "source": [
    "train_table_dict={}\n",
    "for sample_id,product_df in tqdm(result_matrix[[\"sample_id\",\"product_df\"]].values):\n",
    "    if sample_id not in train_table_dict:\n",
    "        train_table_dict[sample_id]=[]\n",
    "    a= product_df.applymap(lambda x:train_table_dict[sample_id].append(str(x)))\n",
    "\n",
    "# train_table_df={}\n",
    "# train_table_df[\"sample_id\"]=[]\n",
    "# train_table_df[\"text\"]=[]\n",
    "# for item in train_table_dict:\n",
    "#     tmp_text=(\"\").join(a for a in train_table_dict[item])\n",
    "#     if(len(tmp_text)!=0):\n",
    "#         train_table_df[\"sample_id\"].append(item)\n",
    "#         train_table_df[\"text\"].append(tmp_text)\n",
    "\n",
    "# train_table_df=pd.DataFrame(train_table_df)\n",
    "# \"123\" in train_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中银保本理财-人民币按期开放理财产品</td>\n",
       "      <td>理财产品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>中银保本理财-人民币按期开放理财产品</td>\n",
       "      <td>理财产品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>与利率挂钩的结构性产品</td>\n",
       "      <td>理财产品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>广发银行“薪加薪”16号XJXCKJ2578</td>\n",
       "      <td>理财产品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>兴业银行“金雪球-优悦”保本开放式人民币理财产品(2M)</td>\n",
       "      <td>理财产品</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185903</th>\n",
       "      <td>控股参股公司</td>\n",
       "      <td>其它</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185904</th>\n",
       "      <td>控股参股公司</td>\n",
       "      <td>其它</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185905</th>\n",
       "      <td>控股参股公司</td>\n",
       "      <td>其它</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185906</th>\n",
       "      <td>控股参股公司</td>\n",
       "      <td>其它</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185907</th>\n",
       "      <td>控股参股公司</td>\n",
       "      <td>其它</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185908 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              text_1 label_1\n",
       "0                 中银保本理财-人民币按期开放理财产品    理财产品\n",
       "1                 中银保本理财-人民币按期开放理财产品    理财产品\n",
       "2                        与利率挂钩的结构性产品    理财产品\n",
       "3             广发银行“薪加薪”16号XJXCKJ2578    理财产品\n",
       "4       兴业银行“金雪球-优悦”保本开放式人民币理财产品(2M)    理财产品\n",
       "...                              ...     ...\n",
       "185903                        控股参股公司      其它\n",
       "185904                        控股参股公司      其它\n",
       "185905                        控股参股公司      其它\n",
       "185906                        控股参股公司      其它\n",
       "185907                        控股参股公司      其它\n",
       "\n",
       "[185908 rows x 2 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后一部分字段采用预测好的部分，跟提取的text做交互采用双输入lstm在dense层做交互预测最后几个字段\n",
    "\n",
    "# train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "# result_matrix\n",
    "train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "\n",
    "train_lstm_input = train_lstm_input.fillna('否')\n",
    "\n",
    "# label_1理财类型-10  label_2资金来源-3 label_3实际购买公司和上市公司关系-3 label_4买卖方是否有关联关系-2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_1 = LabelEncoder()\n",
    "# label_2 = LabelEncoder()\n",
    "# label_3 = LabelEncoder()\n",
    "# label_4 = LabelEncoder()\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "tmp=pd.DataFrame()\n",
    "train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) \n",
    "\n",
    "# train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) + '_' + train_lstm_input['产品发行方名称'].astype(str)\n",
    "\n",
    "# train_data['text_2'] = train_lstm_input['text'].astype(str)\n",
    "\n",
    "# train_lstm_input[\"文本类别\"]=\"理财产品\"\n",
    "\n",
    "train_data['label_1'] = \"理财产品\"\n",
    "\n",
    "\n",
    "train_data2=train_lstm_input[train_lstm_input[\"产品发行方名称\"]!=\"否\"].reset_index(drop=True)\n",
    "\n",
    "# train_data2[\"文本类别\"]=\"发行方\"\n",
    "\n",
    "tmp['text_1']=train_data2[\"产品发行方名称\"].astype(str)\n",
    "\n",
    "# tmp['text_2']= train_data2[\"text\"].astype(str)\n",
    "\n",
    "tmp['label_1']=\"发行方\"\n",
    "\n",
    "train_data = pd.concat([train_data,tmp]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "other_columns_list=[\"认购金额(万元)\",\"认购日期\",\"资金来源\",\"实际购买公司名称\",\"实际购买公司和上市公司关系\"]\n",
    "\n",
    "for item in other_columns_list:\n",
    "\n",
    "    train_data2=train_lstm_input[train_lstm_input[item]!=\"否\"].reset_index(drop=True)\n",
    "\n",
    "    # train_data2[\"文本类别\"]=item\n",
    "\n",
    "    tmp['text_1']=train_data2[item].astype(str)\n",
    "\n",
    "    # tmp['text_2']= train_data2[\"text\"].astype(str)\n",
    "\n",
    "    tmp['label_1']=\"其它\"\n",
    "\n",
    "    \n",
    "    train_data = pd.concat([train_data,tmp]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_data['label_2'] = label_2.fit_transform(train_lstm_input['资金来源'])\n",
    "# train_data['label_3'] = label_3.fit_transform(train_lstm_input['实际购买公司和上市公司关系'])\n",
    "# train_data['label_4'] = label_4.fit_transform(train_lstm_input['买卖方是否有关联关系'])\n",
    "train_data\n",
    "\n",
    "train_src=[]\n",
    "for text,label in train_data[[\"text_1\",\"label_1\"]].values:\n",
    "    train_src.append([label,text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tgrocery.Grocery at 0x1ca1139aee0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tgrocery import Grocery\n",
    "\n",
    "grocery=Grocery(\"sample\")\n",
    "\n",
    "\n",
    "grocery.train(train_src)\n",
    "\n",
    "grocery.save()\n",
    "\n",
    "\n",
    "\n",
    "# text_list=[]\n",
    "# count=0\n",
    "# sample_id_count=0\n",
    "# for sample_id,product_name in tqdm(train_outputs[[\"sample_id\",\"理财产品名称\"]].values):\n",
    "#     if sample_id in train_table_dict:\n",
    "#         sample_id_count+=1\n",
    "#         if(product_name in train_table_dict[sample_id]):\n",
    "#             count+=1\n",
    "# count\n",
    "# sample_id_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "理财产品\n",
      "理财产品\n",
      "发行方\n",
      "其它\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The argument should be plain text",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-bab3705aa8e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mtrain_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mgrocery\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_src\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tgrocery\\__init__.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, single_text)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_load_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mGroceryNotTrainException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_src\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tgrocery\\classifier.py\u001b[0m in \u001b[0;36mpredict_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'This model is not usable because svm model is not given'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The argument should be plain text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;31m# process unicode type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The argument should be plain text"
     ]
    }
   ],
   "source": [
    "print(grocery.predict(\"中信理财之共赢利率结构20062期人民币结构性理财产品\"))\n",
    "\n",
    "print(grocery.predict(\"利多多对公结构性存款固定持有期产品（1101168902）公司固定持有期JG902期\"))\n",
    "\n",
    "print(grocery.predict(\"中国光大银行中国光大银行上海浦东发展银行股份有限公司衡水分行本次自有资金委托理财金额总计\"))\n",
    "\n",
    "print(str(grocery.predict(\"2019-12-31江苏沙钢集团淮钢特钢股份有限公司\")))\n",
    "\n",
    "train_lstm_input = pd.merge(val_df, train_outputs, on='sample_id', how='left')\n",
    "noshow=train_lstm_input.fillna(\"否\")\n",
    "# train_data = pd.DataFrame()\n",
    "# train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) \n",
    "# train_data['label_1'] = \"理财产品\"\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "train_data['text_1'] = train_lstm_input[train_lstm_input[\"产品发行方名称\"]!=\"否\"].reset_index(drop=True)[\"理财产品名称\"].astype(str)\n",
    "train_data['label_1'] = \"理财产品\"\n",
    "\n",
    "train_src=[]\n",
    "for text,label in train_data[[\"text_1\",\"label_1\"]].values:\n",
    "    train_src.append([label,text])\n",
    "\n",
    "str(grocery.test(train_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"123\" in train_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32818/32818 [00:00<00:00, 631778.17it/s]\n",
      "100%|██████████| 32818/32818 [00:00<00:00, 864491.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2017',\n",
       " '中银',\n",
       " '产品',\n",
       " '人民币',\n",
       " '保本',\n",
       " '利多',\n",
       " '存款',\n",
       " '对公',\n",
       " '开放式',\n",
       " '收益',\n",
       " '日增',\n",
       " '理财',\n",
       " '理财产品',\n",
       " '结构性',\n",
       " '蕴通',\n",
       " '计划',\n",
       " '财富',\n",
       " '雪球'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "text_list=[]\n",
    "for product_name in tqdm(train_outputs[\"产品发行方名称\"].values):\n",
    "    text_list.append(str(product_name))\n",
    "\n",
    "# text_list\n",
    "\n",
    "a_list=[]\n",
    "\n",
    "for x in jieba.analyse.extract_tags((\",\").join(i for i in text_list)):#可以再添加一个参数指定输出个数\n",
    "    a_list.append(x)#直接输出关键词和词频\n",
    "\n",
    "\n",
    "text_list=[]\n",
    "for product_name in tqdm(train_outputs[\"理财产品名称\"].values):\n",
    "    text_list.append(str(product_name))\n",
    "\n",
    "b_list=[]\n",
    "\n",
    "for x in jieba.analyse.extract_tags((\",\").join(i for i in text_list)):#可以再添加一个参数指定输出个数\n",
    "    b_list.append(x)\n",
    "\n",
    "set(b_list).difference(a_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后一部分字段采用预测好的部分，跟提取的text做交互采用双输入lstm在dense层做交互预测最后几个字段\n",
    "\n",
    "# train_lstm_input = pd.merge(train_df, train_outputs, on='sample_id', how='left')\n",
    "result_matrix\n",
    "train_lstm_input = pd.merge(train_table_df, train_outputs, on='sample_id', how='left')\n",
    "\n",
    "train_lstm_input = train_lstm_input.fillna('否')\n",
    "\n",
    "# label_1理财类型-10  label_2资金来源-3 label_3实际购买公司和上市公司关系-3 label_4买卖方是否有关联关系-2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_1 = LabelEncoder()\n",
    "# label_2 = LabelEncoder()\n",
    "# label_3 = LabelEncoder()\n",
    "# label_4 = LabelEncoder()\n",
    "\n",
    "train_data = pd.DataFrame()\n",
    "tmp=pd.DataFrame()\n",
    "train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) \n",
    "\n",
    "# train_data['text_1'] = train_lstm_input['理财产品名称'].astype(str) + '_' + train_lstm_input['产品发行方名称'].astype(str)\n",
    "\n",
    "train_data['text_2'] = train_lstm_input['text'].astype(str)\n",
    "\n",
    "train_lstm_input[\"文本类别\"]=\"理财产品\"\n",
    "\n",
    "train_data['label_1'] = label_1.fit_transform(train_lstm_input[\"文本类别\"])\n",
    "\n",
    "\n",
    "train_data2=train_lstm_input[train_lstm_input[\"产品发行方名称\"]!=\"无\"].reset_index(drop=True)\n",
    "\n",
    "train_data2[\"文本类别\"]=\"发行方\"\n",
    "\n",
    "tmp['text_1']=train_data2[\"产品发行方名称\"].astype(str)\n",
    "\n",
    "tmp['text_2']= train_data2[\"text\"].astype(str)\n",
    "\n",
    "tmp['label_1']=label_1.fit_transform(train_data2[\"文本类别\"])\n",
    "\n",
    "train_data = pd.concat([train_data,tmp]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "other_columns_list=[\"认购金额(万元)\",\"认购日期\"]\n",
    "\n",
    "for item in other_columns_list:\n",
    "\n",
    "    train_data2=train_lstm_input[train_lstm_input[item]!=\"无\"].reset_index(drop=True)\n",
    "\n",
    "    train_data2[\"文本类别\"]=\"其他\"\n",
    "\n",
    "    tmp['text_1']=train_data2[item].astype(str)\n",
    "\n",
    "    tmp['text_2']= train_data2[\"text\"].astype(str)\n",
    "\n",
    "    tmp['label_1']=label_1.fit_transform(train_data2[\"文本类别\"])\n",
    "\n",
    "    \n",
    "    train_data = pd.concat([train_data,tmp]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "# train_data['label_2'] = label_2.fit_transform(train_lstm_input['资金来源'])\n",
    "# train_data['label_3'] = label_3.fit_transform(train_lstm_input['实际购买公司和上市公司关系'])\n",
    "# train_data['label_4'] = label_4.fit_transform(train_lstm_input['买卖方是否有关联关系'])\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import *\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from gensim.models import FastText, Word2Vec\n",
    "import re\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import *\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "import keras.backend as K\n",
    "from keras.optimizers import *\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import gc\n",
    "import logging\n",
    "import gensim\n",
    "import jieba\n",
    "tqdm.pandas()\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "# 显卡使用（如没显卡需要注释掉）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "np.random.seed(1024)\n",
    "rn.seed(1024)\n",
    "tf.random.set_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33576/33576 [00:01<00:00, 24915.15it/s]\n",
      "100%|██████████| 33576/33576 [02:29<00:00, 224.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>结构性 存款</td>\n",
       "      <td>联动 联动 银行 保证 收益 10 , 0004.35% 91 天 2017 / 8 / 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>单位 大额 存单</td>\n",
       "      <td>兴业银行 股份 有限公司 兴业银行 股份 有限公司 兴业银行 股份 有限公司 广东 华兴 银...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“ 乾元 - 福顺盈 ” 开放式 资产 组合型 理财产品</td>\n",
       "      <td>兴业银行 股份 有限公司 兴业银行 股份 有限公司 兴业银行 股份 有限公司 广东 华兴 银...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>兴业银行 企业 金融 结构性 存款 ( 封闭式 )</td>\n",
       "      <td>兴业银行 股份 有限公司 兴业银行 股份 有限公司 兴业银行 股份 有限公司 广东 华兴 银...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>兴证资 管鑫利 5 号 集合 资产 管理 计划</td>\n",
       "      <td>12341234 华懋 科技 华懋 科技 华懋 科技 华懋 科技 中国银行 股份 有限公司 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text_1  \\\n",
       "0                        结构性 存款   \n",
       "1                      单位 大额 存单   \n",
       "2  “ 乾元 - 福顺盈 ” 开放式 资产 组合型 理财产品   \n",
       "3     兴业银行 企业 金融 结构性 存款 ( 封闭式 )   \n",
       "4       兴证资 管鑫利 5 号 集合 资产 管理 计划   \n",
       "\n",
       "                                              text_2  label_1  \n",
       "0  联动 联动 银行 保证 收益 10 , 0004.35% 91 天 2017 / 8 / 2...        0  \n",
       "1  兴业银行 股份 有限公司 兴业银行 股份 有限公司 兴业银行 股份 有限公司 广东 华兴 银...        0  \n",
       "2  兴业银行 股份 有限公司 兴业银行 股份 有限公司 兴业银行 股份 有限公司 广东 华兴 银...        0  \n",
       "3  兴业银行 股份 有限公司 兴业银行 股份 有限公司 兴业银行 股份 有限公司 广东 华兴 银...        0  \n",
       "4  12341234 华懋 科技 华懋 科技 华懋 科技 华懋 科技 中国银行 股份 有限公司 ...        0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text_1'] = train_data['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data['text_2'] = train_data['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizer 序列化文本\n",
    "def set_tokenizer(docs, split_char=' ', max_len=100):\n",
    "    '''\n",
    "    输入\n",
    "    docs:文本列表\n",
    "    split_char:按什么字符切割\n",
    "    max_len:截取的最大长度\n",
    "    \n",
    "    输出\n",
    "    X:序列化后的数据\n",
    "    word_index:文本和数字对应的索引\n",
    "    '''\n",
    "    tokenizer = Tokenizer(lower=False, char_level=False, split=split_char)\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    X = tokenizer.texts_to_sequences(docs)\n",
    "    maxlen = max_len\n",
    "    X = pad_sequences(X, maxlen=maxlen, value=0)\n",
    "    word_index=tokenizer.word_index\n",
    "    return X, word_index, tokenizer\n",
    "\n",
    "### 做embedding 这里采用word2vec 可以换成其他例如（glove词向量）\n",
    "def trian_save_word2vec(docs, embed_size=300, save_name='w2v.txt', split_char=' '):\n",
    "    '''\n",
    "    输入\n",
    "    docs:输入的文本列表\n",
    "    embed_size:embed长度\n",
    "    save_name:保存的word2vec位置\n",
    "    \n",
    "    输出\n",
    "    w2v:返回的模型\n",
    "    '''\n",
    "    input_docs = []\n",
    "    for i in docs:\n",
    "        input_docs.append(i.split(split_char))\n",
    "    logging.basicConfig(\n",
    "    format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "    w2v = Word2Vec(input_docs, size=embed_size, sg=1, window=8, seed=1017, workers=24, min_count=1, iter=10)\n",
    "    w2v.save(save_name)\n",
    "    print(\"w2v model done\")\n",
    "    return w2v\n",
    "\n",
    "# 得到embedding矩阵\n",
    "def get_embedding_matrix(word_index, embed_size=300, Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = Word2Vec.load(Emed_path)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector  \n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "# 得到fasttext矩阵\n",
    "def load_fasttext(word_index, path):  \n",
    "    count=0\n",
    "    null_list=[]\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(path, encoding='utf-8') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words =  len(word_index)+1\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            null_list.append(word)\n",
    "            count+=1\n",
    "    print(\"null cnt:\",count)\n",
    "    return embedding_matrix\n",
    "\n",
    "def get_embedding_matrix_txt(word_index,embed_size=200,Emed_path=\"w2v_300.txt\"):\n",
    "    embeddings_index = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        Emed_path, binary=False)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    count = 0\n",
    "    for word, i in tqdm(word_index.items()):\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = np.zeros(embed_size)\n",
    "            count += 1\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(\"null cnt\",count)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始序列化\n",
      "序列化完成\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-05 17:19:12,808:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:12,810:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:12,812:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:12,815:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:12,819:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:12,820:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:12,822:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:12,824:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:12,824:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:12,825:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:12,826:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:12,827:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:12,828:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:12,828:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:12,829:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:12,843:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:12,862:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:12,863:INFO:EPOCH - 2 : training on 31818 raw words (17300 effective words) took 0.1s, 220676 effective words/s\n",
      "2020-09-05 17:19:12,884:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:12,892:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:12,896:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:12,899:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:12,900:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:12,901:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:12,903:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:12,905:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:12,906:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:12,907:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:12,908:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:12,909:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:12,910:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:12,911:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:12,913:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:12,914:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:12,915:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:12,916:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:12,917:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:12,918:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:12,920:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:12,921:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:12,942:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:12,965:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:12,966:INFO:EPOCH - 3 : training on 31818 raw words (17319 effective words) took 0.1s, 201132 effective words/s\n",
      "2020-09-05 17:19:12,986:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:12,994:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:12,996:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:12,999:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:13,000:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:13,002:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:13,004:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:13,008:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:13,009:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:13,010:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:13,012:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:13,013:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:13,014:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:13,015:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:13,015:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:13,016:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:13,017:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:13,018:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:13,019:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:13,020:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:13,021:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:13,022:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:13,036:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:13,058:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:13,059:INFO:EPOCH - 4 : training on 31818 raw words (17201 effective words) took 0.1s, 224203 effective words/s\n",
      "2020-09-05 17:19:13,079:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:13,087:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:13,090:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:13,092:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:13,093:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:13,095:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:13,096:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:13,098:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:13,099:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:13,100:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:13,101:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:13,102:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:13,104:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:13,105:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:13,106:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:13,107:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:13,107:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:13,108:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:13,109:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:13,110:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:13,110:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:13,111:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:13,125:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:13,144:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:13,146:INFO:EPOCH - 5 : training on 31818 raw words (17302 effective words) took 0.1s, 249985 effective words/s\n",
      "2020-09-05 17:19:13,164:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:13,173:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:13,174:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:13,175:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:13,176:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:13,177:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:13,178:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:13,179:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:13,180:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:13,181:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:13,182:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:13,183:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:13,184:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:13,185:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:13,185:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:13,187:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:13,188:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:13,189:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:13,190:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:13,191:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:13,192:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:13,193:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:13,213:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:13,232:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:13,233:INFO:EPOCH - 6 : training on 31818 raw words (17219 effective words) took 0.1s, 237296 effective words/s\n",
      "2020-09-05 17:19:13,257:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:13,266:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:13,267:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:13,268:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:13,270:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:13,272:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:13,274:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:13,275:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:13,276:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:13,277:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:13,278:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:13,280:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:13,281:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:13,282:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:13,283:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:13,284:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:13,285:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:13,285:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:13,286:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:13,287:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:13,288:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:13,289:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:13,303:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:13,326:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:13,328:INFO:EPOCH - 7 : training on 31818 raw words (17322 effective words) took 0.1s, 236553 effective words/s\n",
      "2020-09-05 17:19:13,348:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:13,358:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:13,360:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:13,361:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:13,362:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:13,363:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:13,364:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:13,365:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:13,367:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:13,369:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:13,370:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:13,371:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:13,371:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:13,372:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:13,373:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:13,374:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:13,375:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:13,376:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:13,377:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:13,378:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:13,379:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:13,381:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:13,398:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:13,414:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:13,415:INFO:EPOCH - 8 : training on 31818 raw words (17354 effective words) took 0.1s, 247204 effective words/s\n",
      "2020-09-05 17:19:13,438:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:13,444:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:13,446:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:13,447:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:13,448:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:13,450:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:13,452:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:13,454:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:13,455:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:13,456:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:13,456:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:13,457:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:13,458:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:13,459:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:13,460:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:13,460:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:13,461:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:13,462:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:13,463:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:13,463:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:13,464:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:13,465:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:13,480:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:13,500:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:13,501:INFO:EPOCH - 9 : training on 31818 raw words (17311 effective words) took 0.1s, 251189 effective words/s\n",
      "2020-09-05 17:19:13,520:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:13,531:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:13,533:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:13,536:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:13,538:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:13,539:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:13,541:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:13,542:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:13,544:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:13,546:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:13,547:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:13,548:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:13,549:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:13,550:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:13,551:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:13,552:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:13,553:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:13,553:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:13,554:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:13,555:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:13,557:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:13,558:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:13,572:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:13,587:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:13,588:INFO:EPOCH - 10 : training on 31818 raw words (17302 effective words) took 0.1s, 239547 effective words/s\n",
      "2020-09-05 17:19:13,589:INFO:training on a 318180 raw words (172893 effective words) took 0.9s, 189432 effective words/s\n",
      "2020-09-05 17:19:13,590:WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2020-09-05 17:19:13,591:INFO:saving Word2Vec object under ../models/w2v_300_1.txt, separately None\n",
      "2020-09-05 17:19:13,591:INFO:not storing attribute vectors_norm\n",
      "2020-09-05 17:19:13,592:INFO:not storing attribute cum_table\n",
      "2020-09-05 17:19:13,664:INFO:saved ../models/w2v_300_1.txt\n",
      "w2v model done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1ca6a1cf730>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " threads\n",
      "2020-09-05 17:19:17,405:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:17,407:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:17,415:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:17,417:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:17,419:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:17,421:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:17,423:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:17,426:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:17,429:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:17,439:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:17,440:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:17,446:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:17,448:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:17,449:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:17,451:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:17,453:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:17,461:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:17,468:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:17,469:INFO:EPOCH - 2 : training on 236649 raw words (150286 effective words) took 0.6s, 266700 effective words/s\n",
      "2020-09-05 17:19:17,872:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:17,884:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:17,908:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:17,922:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:17,938:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:17,949:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:17,952:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:17,959:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:17,969:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:17,971:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:17,972:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:17,982:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:17,990:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:17,993:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:18,002:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:18,012:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:18,016:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:18,020:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:18,028:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:18,035:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:18,037:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:18,052:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:18,056:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:18,057:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:18,058:INFO:EPOCH - 3 : training on 236649 raw words (150421 effective words) took 0.6s, 261281 effective words/s\n",
      "2020-09-05 17:19:18,499:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:18,515:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:18,532:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:18,567:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:18,578:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:18,580:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:18,583:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:18,590:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:18,593:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:18,595:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:18,605:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:18,607:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:18,615:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:18,619:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:18,626:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:18,631:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:18,634:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:18,636:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:18,638:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:18,639:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:18,644:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:18,662:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:18,669:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:18,672:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:18,673:INFO:EPOCH - 4 : training on 236649 raw words (150380 effective words) took 0.6s, 249970 effective words/s\n",
      "2020-09-05 17:19:19,119:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:19,128:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:19,152:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:19,162:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:19,173:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:19,185:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:19,194:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:19,195:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:19,211:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:19,213:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:19,219:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:19,229:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:19,233:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:19,235:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:19,237:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:19,241:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:19,244:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:19,246:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:19,249:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:19,251:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:19,252:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:19,254:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:19,261:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:19,275:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:19,277:INFO:EPOCH - 5 : training on 236649 raw words (150096 effective words) took 0.6s, 255146 effective words/s\n",
      "2020-09-05 17:19:19,708:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:19,723:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:19,727:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:19,744:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:19,756:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:19,772:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:19,776:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:19,779:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:19,784:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:19,786:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:19,788:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:19,801:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:19,804:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:19,805:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:19,807:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:19,840:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:19,841:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:19,843:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:19,851:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:19,854:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:19,856:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:19,858:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:19,868:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:19,875:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:19,876:INFO:EPOCH - 6 : training on 236649 raw words (150354 effective words) took 0.6s, 256195 effective words/s\n",
      "2020-09-05 17:19:20,291:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:20,354:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:20,356:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:20,357:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:20,366:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:20,368:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:20,377:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:20,382:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:20,388:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:20,391:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:20,393:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:20,401:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:20,402:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:20,405:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:20,407:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:20,416:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:20,420:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:20,422:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:20,426:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:20,429:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:20,434:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:20,443:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:20,448:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:20,450:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:20,451:INFO:EPOCH - 7 : training on 236649 raw words (150579 effective words) took 0.6s, 268555 effective words/s\n",
      "2020-09-05 17:19:20,857:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:20,864:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:20,873:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:20,895:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:20,940:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:20,942:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:20,943:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:20,946:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:20,951:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:20,953:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:20,957:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:20,963:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:20,968:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:20,971:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:20,976:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:20,979:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:20,982:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:20,986:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:20,987:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:20,989:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:20,992:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:20,995:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:21,002:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:21,005:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:21,006:INFO:EPOCH - 8 : training on 236649 raw words (150463 effective words) took 0.5s, 276657 effective words/s\n",
      "2020-09-05 17:19:21,372:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:21,407:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:21,489:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:21,492:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:21,494:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:21,498:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:21,508:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:21,525:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:21,529:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:21,539:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:21,541:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:21,543:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:21,545:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:21,558:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:21,560:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:21,561:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:21,567:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:21,569:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:21,570:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:21,571:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:21,572:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:21,585:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:21,587:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:21,587:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:21,588:INFO:EPOCH - 9 : training on 236649 raw words (150294 effective words) took 0.6s, 263625 effective words/s\n",
      "2020-09-05 17:19:21,971:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-09-05 17:19:22,033:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-09-05 17:19:22,040:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-09-05 17:19:22,049:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-09-05 17:19:22,051:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-09-05 17:19:22,052:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-09-05 17:19:22,058:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-09-05 17:19:22,065:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-09-05 17:19:22,067:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-09-05 17:19:22,080:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-09-05 17:19:22,092:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-09-05 17:19:22,101:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-09-05 17:19:22,103:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-09-05 17:19:22,105:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-09-05 17:19:22,125:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-09-05 17:19:22,129:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-09-05 17:19:22,132:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-09-05 17:19:22,136:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-09-05 17:19:22,144:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-09-05 17:19:22,152:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-09-05 17:19:22,154:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-09-05 17:19:22,160:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-09-05 17:19:22,164:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-09-05 17:19:22,175:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-09-05 17:19:22,176:INFO:EPOCH - 10 : training on 236649 raw words (150393 effective words) took 0.6s, 260851 effective words/s\n",
      "2020-09-05 17:19:22,178:INFO:training on a 2366490 raw words (1503386 effective words) took 5.9s, 256481 effective words/s\n",
      "2020-09-05 17:19:22,179:INFO:saving Word2Vec object under ../models/w2v_300_3.txt, separately None\n",
      "2020-09-05 17:19:22,180:INFO:not storing attribute vectors_norm\n",
      "2020-09-05 17:19:22,183:INFO:not storing attribute cum_table\n",
      "2020-09-05 17:19:22,407:INFO:saved ../models/w2v_300_3.txt\n",
      "w2v model done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1ca0fcd1190>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-05 17:19:22,753:INFO:loading Word2Vec object from ../models/w2v_300_1.txt\n",
      "2020-09-05 17:19:22,818:INFO:loading wv recursively from ../models/w2v_300_1.txt.wv.* with mmap=None\n",
      "2020-09-05 17:19:22,819:INFO:setting ignored attribute vectors_norm to None\n",
      "2020-09-05 17:19:22,819:INFO:loading vocabulary recursively from ../models/w2v_300_1.txt.vocabulary.* with mmap=None\n",
      "2020-09-05 17:19:22,820:INFO:loading trainables recursively from ../models/w2v_300_1.txt.trainables.* with mmap=None\n",
      "2020-09-05 17:19:22,820:INFO:setting ignored attribute cum_table to None\n",
      "2020-09-05 17:19:22,821:INFO:loaded ../models/w2v_300_1.txt\n",
      "100%|██████████| 2886/2886 [00:00<00:00, 74076.01it/s]\n",
      "2020-09-05 17:19:22,871:INFO:loading Word2Vec object from ../models/w2v_300_3.txt\n",
      "null cnt 347\n",
      "2020-09-05 17:19:23,056:INFO:loading wv recursively from ../models/w2v_300_3.txt.wv.* with mmap=None\n",
      "2020-09-05 17:19:23,057:INFO:setting ignored attribute vectors_norm to None\n",
      "2020-09-05 17:19:23,058:INFO:loading vocabulary recursively from ../models/w2v_300_3.txt.vocabulary.* with mmap=None\n",
      "2020-09-05 17:19:23,059:INFO:loading trainables recursively from ../models/w2v_300_3.txt.trainables.* with mmap=None\n",
      "2020-09-05 17:19:23,060:INFO:setting ignored attribute cum_table to None\n",
      "2020-09-05 17:19:23,060:INFO:loaded ../models/w2v_300_3.txt\n",
      "100%|██████████| 5843/5843 [00:00<00:00, 76960.79it/s]\n",
      "null cnt 1007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1_list = np.unique(train_data['text_1'])\n",
    "text_3_list = np.unique(train_data['text_2'])\n",
    "\n",
    "print('开始序列化')\n",
    "x1, index_1, token_1 = set_tokenizer(train_data['text_1'], split_char=' ', max_len=30)\n",
    "x3, index_3, token_3 = set_tokenizer(train_data['text_2'], split_char=' ', max_len=600)\n",
    "print('序列化完成')\n",
    "gc.collect()\n",
    "\n",
    "trian_save_word2vec(text_1_list, save_name='../models/w2v_300_1.txt', split_char=' ')\n",
    "gc.collect()\n",
    "trian_save_word2vec(text_3_list, save_name='../models/w2v_300_3.txt', split_char=' ')\n",
    "gc.collect()\n",
    "\n",
    "# 得到emb矩阵\n",
    "emb1 = get_embedding_matrix(index_1, Emed_path='../models/w2v_300_1.txt')\n",
    "emb3 = get_embedding_matrix(index_3, Emed_path='../models/w2v_300_3.txt')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81006"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.initializers import *\n",
    "\n",
    "def model_conv(emb1, emb3):\n",
    "    '''\n",
    "    注意这个inputs\n",
    "    seq1、seq2分别是两个输入\n",
    "    是否做emb可选可不选，\n",
    "    这个就是我们之前训练已经得到的用于embedding的（embedding_matrix1， embedding_matrix2）\n",
    "    '''\n",
    "    K.clear_session()\n",
    "\n",
    "    emb_layer_1 = Embedding(\n",
    "        input_dim=emb1.shape[0],\n",
    "        output_dim=emb1.shape[1],\n",
    "        weights=[emb1],\n",
    "        input_length=30,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    emb_layer_3 = Embedding(\n",
    "        input_dim=emb3.shape[0],\n",
    "        output_dim=emb3.shape[1],\n",
    "        weights=[emb3],\n",
    "        input_length=600,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    seq1 = Input(shape=(30,))\n",
    "    seq3 = Input(shape=(600,))    \n",
    "    \n",
    "    x1 = emb_layer_1(seq1)\n",
    "    x3 = emb_layer_3(seq3)\n",
    "    \n",
    "    sdrop=SpatialDropout1D(rate=0.2)\n",
    "\n",
    "    x1 = sdrop(x1)\n",
    "    x3 = sdrop(x3)\n",
    "     \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x1))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_1 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    x = Dropout(0.2)(Bidirectional(GRU(128, return_sequences=True))(x3))\n",
    "    semantic = TimeDistributed(Dense(100, activation=\"tanh\"))(x)\n",
    "    merged_3 = Lambda(lambda x: K.max(x, axis=1), output_shape=(100,))(semantic)\n",
    "    \n",
    "    \n",
    "    x = Multiply()([merged_1, merged_3])\n",
    "    \n",
    "    x = Dropout(0.2)(Activation(activation=\"relu\")(BatchNormalization()(Dense(1000)(x))))\n",
    "    x = Activation(activation=\"relu\")(BatchNormalization()(Dense(500)(x)))\n",
    "    pred_1 = Dense(3, activation='softmax')(x)\n",
    "    # pred_2 = Dense(3, activation='softmax')(x)\n",
    "    # pred_3 = Dense(3, activation='softmax')(x)\n",
    "    # pred_4 = Dense(2, activation='softmax')(x)\n",
    "    model = Model(inputs=[seq1, seq3], outputs=[pred_1])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=0.0001),metrics=[\"accuracy\"])\n",
    "    return model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 600)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 300)      866100      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 600, 300)     1753200     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo multiple             0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 30, 256)      330240      spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 600, 256)     330240      spatial_dropout1d[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 30, 256)      0           bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600, 256)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 30, 100)      25700       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 600, 100)     25700       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 100)          0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100)          0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 100)          0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1000)         101000      multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1000)         4000        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1000)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000)         0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 500)          500500      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 500)          2000        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 500)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 3)            1503        activation_1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,940,183\n",
      "Trainable params: 1,317,883\n",
      "Non-trainable params: 2,622,300\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:759 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:388 update_state\n        self.build(y_pred, y_true)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:318 build\n        self._metrics = nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1135 map_structure_up_to\n        return map_structure_with_tuple_paths_up_to(\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1234 map_structure_with_tuple_paths_up_to\n        results = [func(*args, **kwargs) for args in zip(flat_path_list,\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1234 <listcomp>\n        results = [func(*args, **kwargs) for args in zip(flat_path_list,\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1137 <lambda>\n        lambda _, *values: func(*values),  # Discards the path arg.\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:419 _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:419 <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:440 _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    AttributeError: 'tuple' object has no attribute 'shape'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-86ae51bbcd44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# l3 = to_categorical(train_data['label_3'], 3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# l4 = to_categorical(train_data['label_4'], 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:806 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:789 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:759 train_step\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:388 update_state\n        self.build(y_pred, y_true)\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:318 build\n        self._metrics = nest.map_structure_up_to(y_pred, self._get_metric_objects,\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1135 map_structure_up_to\n        return map_structure_with_tuple_paths_up_to(\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1234 map_structure_with_tuple_paths_up_to\n        results = [func(*args, **kwargs) for args in zip(flat_path_list,\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1234 <listcomp>\n        results = [func(*args, **kwargs) for args in zip(flat_path_list,\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py:1137 <lambda>\n        lambda _, *values: func(*values),  # Discards the path arg.\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:419 _get_metric_objects\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:419 <listcomp>\n        return [self._get_metric_object(m, y_t, y_p) for m in metrics]\n    C:\\Users\\lsqlh\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\compile_utils.py:440 _get_metric_object\n        y_t_rank = len(y_t.shape.as_list())\n\n    AttributeError: 'tuple' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "model = model_conv(emb1, emb3)\n",
    "model.summary()\n",
    "l1 = to_categorical(train_data['label_1'], 3)\n",
    "# l2 = to_categorical(train_data['label_2'], 3)\n",
    "# l3 = to_categorical(train_data['label_3'], 3)\n",
    "# l4 = to_categorical(train_data['label_4'], 2)\n",
    "model.fit([x1, x3],[l1], batch_size=256, epochs=8, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存权重\n",
    "model.save_weights('models/lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测验证集\n",
    "val_result_for_pred = pd.merge(val_result, val_df, on='sample_id', how='left')\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['理财产品名称'].astype(str) + '_' + val_result_for_pred['产品发行方名称'].astype(str)\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text'].astype(str)\n",
    "\n",
    "val_result_for_pred['text_1'] = val_result_for_pred['text_1'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "val_result_for_pred['text_2'] = val_result_for_pred['text_2'].progress_apply(lambda row:' '.join(jieba.lcut(str(row))))\n",
    "\n",
    "x1 = token_1.texts_to_sequences(val_result_for_pred['text_1'])\n",
    "x1 = pad_sequences(x1, maxlen=30, value=0)\n",
    "x3 = token_3.texts_to_sequences(val_result_for_pred['text_2'])\n",
    "x3 = pad_sequences(x3, maxlen=600, value=0)\n",
    "pred_result = model.predict([x1, x3], batch_size=1024, verbose=1)\n",
    "pred_1 = label_1.inverse_transform(np.argmax(pred_result[0], axis=1))\n",
    "pred_2 = label_2.inverse_transform(np.argmax(pred_result[1], axis=1))\n",
    "pred_3 = label_3.inverse_transform(np.argmax(pred_result[2], axis=1))\n",
    "pred_4 = label_4.inverse_transform(np.argmax(pred_result[3], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_result['理财类型'] = pred_1\n",
    "val_result['资金来源'] = pred_2\n",
    "val_result['实际购买公司和上市公司关系'] = pred_3\n",
    "val_result['买卖方是否有关联关系'] = pred_4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
